{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36423615",
   "metadata": {},
   "source": [
    "# DSCI591 Data Collection\n",
    "## Covid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a15f223a-abf5-443d-8e4e-bf6b8cfec183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95724345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "import os\n",
    "from os import path\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import gamma\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e041f08-1ad3-443b-a09c-2df97f93de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b2ada",
   "metadata": {},
   "source": [
    "### Helper Methods from the C3.ai Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ce547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_json(typename, api, body):\n",
    "    \"\"\"\n",
    "    read_data_json directly accesses the C3.ai COVID-19 Data Lake APIs using the requests library, \n",
    "    and returns the response as a JSON, raising an error if the call fails for any reason.\n",
    "    ------\n",
    "    typename: The type you want to access, i.e. 'OutbreakLocation', 'LineListRecord', 'BiblioEntry', etc.\n",
    "    api: The API you want to access, either 'fetch' or 'evalmetrics'.\n",
    "    body: The spec you want to pass. For examples, see the API documentation.\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        \"https://api.c3.ai/covid/api/1/\" + typename + \"/\" + api, \n",
    "        json = body, \n",
    "        headers = {\n",
    "            'Accept' : 'application/json', \n",
    "            'Content-Type' : 'application/json'\n",
    "        }\n",
    "    )\n",
    "    if not response.ok:\n",
    "        print(response.text)\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def fetch(typename, body, get_all = False, remove_meta = True):\n",
    "    \"\"\"\n",
    "    fetch accesses the Data Lake using read_data_json, and converts the response into a Pandas dataframe. \n",
    "    fetch is used for all non-timeseries data in the Data Lake, and will call read_data as many times \n",
    "    as required to access all of the relevant data for a given typename and body.\n",
    "    ------\n",
    "    typename: The type you want to access, i.e. 'OutbreakLocation', 'LineListRecord', 'BiblioEntry', etc.\n",
    "    body: The spec you want to pass. For examples, see the API documentation.\n",
    "    get_all: If True, get all records and ignore any limit argument passed in the body. If False, use the limit argument passed in the body. The default is False.\n",
    "    remove_meta: If True, remove metadata about each record. If False, include it. The default is True.\n",
    "    \"\"\"\n",
    "    if get_all:\n",
    "        has_more = True\n",
    "        offset = 0\n",
    "        limit = 2000\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        while has_more:\n",
    "            body['spec'].update(limit = limit, offset = offset)\n",
    "            response_json = read_data_json(typename, 'fetch', body)\n",
    "            new_df = pd.json_normalize(response_json['objs'])\n",
    "            df = df.append(new_df)\n",
    "            has_more = response_json['hasMore']\n",
    "            offset += limit\n",
    "            \n",
    "    else:\n",
    "        response_json = read_data_json(typename, 'fetch', body)\n",
    "        df = pd.json_normalize(response_json['objs'])\n",
    "        \n",
    "    if remove_meta:\n",
    "        df = df.drop(columns = [c for c in df.columns if ('meta' in c) | ('version' in c)])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def evalmetrics(typename, body, remove_meta = True):\n",
    "    \"\"\"\n",
    "    evalmetrics accesses the Data Lake using read_data_json, and converts the response into a Pandas dataframe.\n",
    "    evalmetrics is used for all timeseries data in the Data Lake.\n",
    "    ------\n",
    "    typename: The type you want to access, i.e. 'OutbreakLocation', 'LineListRecord', 'BiblioEntry', etc.\n",
    "    body: The spec you want to pass. For examples, see the API documentation.\n",
    "    remove_meta: If True, remove metadata about each record. If False, include it. The default is True.\n",
    "    \"\"\"\n",
    "    response_json = read_data_json(typename, 'evalmetrics', body)\n",
    "    df = pd.json_normalize(response_json['result'])\n",
    "    \n",
    "    # get the useful data out\n",
    "    df = df.apply(pd.Series.explode)\n",
    "    if remove_meta:\n",
    "        df = df.filter(regex = 'dates|data|missing')\n",
    "    \n",
    "    # only keep one date column\n",
    "    date_cols = [col for col in df.columns if 'dates' in col]\n",
    "    keep_cols =  date_cols[:1] + [col for col in df.columns if 'dates' not in col]\n",
    "    df = df.filter(items = keep_cols).rename(columns = {date_cols[0] : \"dates\"})\n",
    "    df[\"dates\"] = pd.to_datetime(df[\"dates\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfc48c2",
   "metadata": {},
   "source": [
    "#### Streamlined request for single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459e53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_one(typename: str, body: dict, objs_only=True) -> dict:\n",
    "    \"\"\"\n",
    "    Returns JSON output from single API call\n",
    "    \n",
    "    Args:\n",
    "        typename: the C3.ai type name\n",
    "        body: the body of the request\n",
    "        objs_only: if True, remove the metadata and just returns the objects\n",
    "        \n",
    "    Returns:\n",
    "        JSON response as dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    response = read_data_json(typename, 'fetch', body)\n",
    "    if objs_only:\n",
    "        for r in response['objs']:\n",
    "            if 'meta' in r.keys():\n",
    "                del(r['meta'])\n",
    "                \n",
    "        return response['objs']\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526b1725",
   "metadata": {},
   "source": [
    "### Load the location codes for the US into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8d7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_us_locations(file_name='./config/C3-ai-Location-IDs.xlsx'): \n",
    "    \"\"\" Loads all US counties from C3 ai spreadsheet \n",
    "    \n",
    "    Args:\n",
    "        file_name: the name of the spreadsheet\n",
    "        \n",
    "    Returns:\n",
    "        Pandas dataframe with the results\n",
    "    \n",
    "    \"\"\"\n",
    "                     \n",
    "    locations = pd.read_excel(path.join('.', file_name), sheet_name='County IDs', header=2)\n",
    "    us_locations = locations[locations.Country=='United States']\n",
    "    \n",
    "    return us_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b61c89",
   "metadata": {},
   "source": [
    "### Get the basic population data for each of the 3429 counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfc0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_outbreaklocation_body(county_id: str) -> dict:\n",
    "    \"\"\" Forms the request body for a count for the outbreak location API \n",
    "    \n",
    "    Args:\n",
    "        count_id: the ID for the County\n",
    "    \n",
    "    Returns:\n",
    "        The request body\n",
    "    \n",
    "    \"\"\"\n",
    "    return {\n",
    "              \"spec\": {\n",
    "                \"filter\": f\"id == '{county_id}'\"\n",
    "              }\n",
    "}\n",
    "\n",
    "# fetch_one('outbreaklocation', make_outbreaklocation_body('Autauga_Alabama_UnitedStates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "921d9d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_population_data(file_name='./config/counties.json'):\n",
    "    \"\"\" Loads all population data for US counties and stores in a file called counties.json\"\"\"\n",
    "\n",
    "    us_locations = get_us_locations()\n",
    "    keep_going = True\n",
    "    tries = 0\n",
    "    while keep_going:\n",
    "        try:\n",
    "            with open(file_name) as file:\n",
    "                county_data = json.load(file)\n",
    "        except:    \n",
    "            county_data = {}\n",
    "        i = 0\n",
    "        for county in us_locations['County id']:\n",
    "            if county not in county_data.keys():\n",
    "                try:\n",
    "                    data = fetch_one('outbreaklocation',  make_outbreaklocation_body(county))\n",
    "                    county_data[county] = data[0]\n",
    "                    i += 1\n",
    "                    if i % 100 == 0:\n",
    "                        print(f'Saving: {i}')\n",
    "                        with open(file_name, 'w') as file:\n",
    "                            json.dump(county_data, file)\n",
    "                except Exception as e:\n",
    "                    county_data[county] = None\n",
    "                    print(f'Problem with {county}: {e}')\n",
    "                sleep(1)\n",
    "        with open('counties.json', 'w') as file:\n",
    "            json.dump(county_data, file)\n",
    "        if len(county_data) >= len(us_locations) or tries >= 5:\n",
    "            keep_going = False\n",
    "        else:\n",
    "            tries += 1\n",
    "        \n",
    "def get_counties_df(file_name='./config/counties.json'):\n",
    "    with open(file_name) as file:\n",
    "                county_data = json.load(file)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(county_data)\n",
    "    \n",
    "    data = [df[col] for col in df.columns]    \n",
    "    \n",
    "    # pivot\n",
    "    return pd.DataFrame(data,columns=df.index, index=df.columns)\n",
    "\n",
    "# load_population_data()\n",
    "# get_counties_df().head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59249002",
   "metadata": {},
   "source": [
    "### Get the County Stats from the Census Bureau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f07e47a",
   "metadata": {},
   "source": [
    "### New source of county date\n",
    "#### co-est2019-annres.xlsx\n",
    "#### from US Census Bureau at https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0264666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_stats_df(file_name='./config/county_stats.csv'):\n",
    "    \"\"\" Get county land area (LND110210) by FIPS code \"\"\"\n",
    "    return pd.read_csv(path.join('.',file_name))[['fips','LND110210']]\n",
    "\n",
    "# get_county_stats_df()\n",
    "\n",
    "# Equivalent data https://www.kaggle.com/benhamner/2016-us-election"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff08df3",
   "metadata": {},
   "source": [
    "### Functions to Download the Evalmetrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6274fc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_file_date(county: str) -> str:\n",
    "    \"\"\" Retrieves the last date through which data was loaded for the specified county\n",
    "    \n",
    "    Args:\n",
    "            county: The county requested\n",
    "            \n",
    "    Returns:\n",
    "            A string representing the last date processed.  If the county has never been processed, a\n",
    "            default date of 2020-01-01 is returned\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    max_date = '2020-01-01'\n",
    "    files = glob(path.join('.', 'data', f'{county}*.psv'))\n",
    "    if not files:\n",
    "        return max_date\n",
    "    for file in files:\n",
    "        match = re.search(r'(\\d\\d\\d\\d-\\d\\d-\\d\\d).psv', file)\n",
    "        if match:\n",
    "            max_date = max(max_date, match.group(1))\n",
    "    return max_date\n",
    "            \n",
    "    \n",
    "def download_evalmetrics_data():\n",
    "    \"\"\" Downloads the evalmetrics data from the last download \n",
    "    through current \"\"\"\n",
    "    \n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    # Get the list of counties\n",
    "    counties_file = path.join('.', 'config', 'counties.json')\n",
    "    with open(counties_file) as file:\n",
    "        counties = json.load(file)\n",
    "\n",
    "    # Iterate through counties saving the time series data\n",
    "    for counter, (county, details) in enumerate(counties.items()):\n",
    "        print('.', end='')\n",
    "        if counter and not counter % 120:\n",
    "            print()\n",
    "\n",
    "        # Skip if missing county details\n",
    "        if not details:\n",
    "            continue\n",
    "\n",
    "        # Get the last date we processed\n",
    "        last_date = get_last_file_date(county)\n",
    "\n",
    "        if last_date == today:\n",
    "            continue\n",
    "            \n",
    "        expressions = [    \"JHU_ConfirmedCases\", \n",
    "                           \"JHU_ConfirmedDeaths\", \n",
    "                           \"JHU_ConfirmedRecoveries\",\n",
    "                           \"NYT_ConfirmedCases\",\n",
    "                           \"NYT_ConfirmedDeaths\",\n",
    "                           \"NYT_AllCausesDeathsWeekly_Deaths_AllCauses\",\n",
    "                           \"NYT_AllCausesDeathsWeekly_Excess_Deaths\",\n",
    "                           \"NYT_AllCausesDeathsWeekly_Expected_Deaths_AllCauses\",\n",
    "                           \"NYT_AllCausesDeathsMonthly_Deaths_AllCauses\",\n",
    "                           \"NYT_AllCausesDeathsMonthly_Excess_Deaths\",\n",
    "                           \"NYT_AllCausesDeathsMonthly_Expected_Deaths_AllCauses\",\n",
    "                           # \"CDS_Active\",\n",
    "                           # \"CDS_Cases\",\n",
    "                           # \"CDS_Deaths\",\n",
    "                           # \"CDS_Discharged\",\n",
    "                           # \"CDS_GrowthFactor\",\n",
    "                           # \"CDS_Hospitalized\",\n",
    "                           # \"CDS_Hospitalized_Current\",\n",
    "                           # \"CDS_ICU\",\n",
    "                           # \"CDS_ICU_Current\",\n",
    "                           # \"CDS_Recovered\",\n",
    "                           # \"CDS_Tested\",\n",
    "                           \"TotalPopulation\",\n",
    "                           \"Male_Total_Population\",\n",
    "                           \"Female_Total_Population\",\n",
    "                           \"MaleAndFemale_Under18_Population\",\n",
    "                           \"MaleAndFemale_AtLeast65_Population\",\n",
    "                           \"BLS_LaborForcePopulation\",\n",
    "                           \"BLS_EmployedPopulation\",\n",
    "                           \"BLS_UnemployedPopulation\",\n",
    "                           \"BLS_UnemploymentRate\",\n",
    "                           \"AverageDailyTemperature\",\n",
    "                           \"AverageDewPoint\",\n",
    "                           \"AverageRelativeHumidity\",\n",
    "                           \"AverageSurfaceAirPressure\",\n",
    "                           \"AveragePrecipitation\",\n",
    "                           \"AverageWindSpeed\",\n",
    "                           \"AverageWindDirection\",\n",
    "                           \"AveragePrecipitationTotal\",]\n",
    "                                     \n",
    "        # Get the data for the county from the last date processed\n",
    "        for i in range(math.ceil(len(expressions)//4)): \n",
    "            body = {\"spec\" : {\n",
    "                                \"ids\" : [county],\n",
    "                                \"expressions\": expressions[i*4:(i+1)*4] , \n",
    "                                \"start\" : last_date,\n",
    "                                \"end\" : today,\n",
    "                                \"interval\" : \"DAY\",\n",
    "                            }\n",
    "                    }\n",
    "\n",
    "            try:\n",
    "                df = evalmetrics(\"outbreaklocation\", body)\n",
    "                file_name = path.join('.', 'data', f'{county}-part-{i}-{last_date}-{today}.psv')\n",
    "                df.to_csv(file_name, sep='|')\n",
    "            except Exception as e:\n",
    "                print(f'Error processing {county}: {e}')\n",
    "        \n",
    "            sleep(1)\n",
    "  \n",
    "# download_evalmetrics_data()           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82babb",
   "metadata": {},
   "source": [
    "### Load all the downloaded data and produce raw DataFrame (OLD - not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29ec3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def append_to_dataframe(files: list) -> pd.DataFrame:\n",
    "#     \"\"\" Appends all files passed as an argument into a single dataframe\n",
    "    \n",
    "#     Args:\n",
    "#         files: list of file names\n",
    "        \n",
    "#     Returns:\n",
    "#         A data frame with all the data from the files populated\n",
    "    \n",
    "#     \"\"\"\n",
    "#     name_pattern = re.compile('([\\w_]+)')\n",
    "#     county_df = None\n",
    "\n",
    "#     for file in files:\n",
    "#         base: str = path.basename(file)\n",
    "#         if base.startswith('Unassigned') or base.startswith('Outof') or base.startswith('.'):\n",
    "#             continue\n",
    "#         try:\n",
    "#             county = name_pattern.match(base).group(1)\n",
    "#             records = []\n",
    "#             with open(file) as fd:\n",
    "#                 reader = csv.reader(fd, delimiter='|')\n",
    "#                 # advance past the header\n",
    "#                 next(iter(reader))\n",
    "#                 records = [[county, row[1], row[2], row[4], row[6]] for row in reader]\n",
    "            \n",
    "#             temp_df = pd.DataFrame(records, columns=['county', 'date', \n",
    "#                                                      'confirmed_cases_running', \n",
    "#                                                      'confirmed_deaths_running', \n",
    "#                                                      'confirmed_recoveries_running'])\n",
    "            \n",
    "#             if float(temp_df.confirmed_deaths_running[-1:].values) < 2:\n",
    "#                 continue\n",
    "            \n",
    "#             # turn running amounts into daily amounts\n",
    "#             temp_df['prev_day_cases'] = ['0.0'] + list(temp_df.confirmed_cases_running[:-1])\n",
    "#             temp_df['confirmed_cases'] = (temp_df.confirmed_cases_running.astype('float') - \n",
    "#                                           temp_df.prev_day_cases.astype('float'))\n",
    "            \n",
    "#             temp_df['prev_day_deaths'] = ['0.0'] + list(temp_df.confirmed_deaths_running[:-1])\n",
    "#             temp_df['confirmed_deaths'] = (temp_df.confirmed_deaths_running.astype('float') - \n",
    "#                                            temp_df.prev_day_deaths.astype('float'))\n",
    "            \n",
    "#             temp_df['prev_day_recoveries'] = ['0.0'] + list(temp_df.confirmed_recoveries_running[:-1])\n",
    "#             temp_df['confirmed_recoveries'] = (temp_df.confirmed_recoveries_running.astype('float') - \n",
    "#                                                temp_df.prev_day_recoveries.astype('float'))\n",
    "            \n",
    "            \n",
    "#             temp_df = temp_df.drop(columns=['confirmed_cases_running', 'prev_day_cases',\n",
    "#                                             'confirmed_deaths_running', 'prev_day_deaths',\n",
    "#                                             'confirmed_recoveries_running', \n",
    "#                                             'prev_day_recoveries',])\n",
    "#             if county_df is None:\n",
    "#                 county_df = temp_df\n",
    "#             else:\n",
    "#                 county_df = pd.concat([county_df, temp_df], ignore_index=True)\n",
    "#         except Exception as e:\n",
    "#             print(f'Failure on file {base} with error {e}')\n",
    "        \n",
    "#     return county_df\n",
    "\n",
    "# def join_evalmetrics_to_county(evalmetrics_df, counties_df, county_stats_df):\n",
    "#     df = evalmetrics_df.merge(counties_df, how='left', left_on='county', right_index=True, sort=True)\n",
    "#     # df.fips = [int(fips['id']) for fips in df.fips]\n",
    "#     fips = []\n",
    "#     for f in df.fips:\n",
    "#         if isinstance(f, dict):\n",
    "#             fips.append(int(f['id']))\n",
    "#         else:\n",
    "#             fips.append(f)\n",
    "#     df.fips = fips\n",
    "#     df = df.merge(county_stats_df, how='left', left_on='fips', right_on='fips')\n",
    "#     df = df.drop(columns=['location', 'id', 'name', 'version', 'typeIdent'])\n",
    "    \n",
    "#     return df.sort_values(by=['county', 'date'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c6261",
   "metadata": {},
   "source": [
    "### Save the raw dataframe to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d10466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_df(df: pd.DataFrame):\n",
    "    df.to_pickle(path.join('.', 'raw_evalmetrics_df.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f54d0a",
   "metadata": {},
   "source": [
    "### Runners\n",
    "#### The following runners can take a significant amount of time to process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d020869",
   "metadata": {},
   "source": [
    "##### Download Evalmetrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87d4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_evalmetrics_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec2070",
   "metadata": {},
   "source": [
    "#### Convert raw files to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf173f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize_df(filename):\n",
    "    df = pd.read_csv(path.join('.', 'data', filename), delimiter='|', index_col=1)\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    columns = {c: '.'.join(c.split('.')[-2:]) for c in df.columns}\n",
    "    df.rename(columns=columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "def merge_county_parts_to_dataframe(filenames):\n",
    "    df = load_and_normalize_df(filenames[0])\n",
    "    for filename in filenames[1:]:\n",
    "        df2 = load_and_normalize_df(filename)\n",
    "        df = df.join(df2)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_county_merged_parts_df(county, df):\n",
    "    df.to_pickle(path.join('.', 'processed_data', 'county_merged_parts', \n",
    "                           f'{county}.pkl'))\n",
    "    \n",
    "\n",
    "def get_counties_from_files():\n",
    "    return list({f.split('-part')[0] for f in os.listdir(path.join('.', 'data'))})\n",
    "\n",
    "\n",
    "def get_dates_for_county(county):\n",
    "    files = glob(f'./data/{county}*' )\n",
    "    dates = {re.findall('\\d\\d\\d\\d-\\d\\d-\\d\\d-\\d\\d\\d\\d-\\d\\d-\\d\\d', f)[0] for f in files}\n",
    "    return list(dates)\n",
    "    \n",
    "def get_county_files_for_date(county, dt):\n",
    "    files = glob(f'./data/{county}-part-*-{dt}.psv')\n",
    "    return [f[f.index(county):]  for f in files]\n",
    "                    \n",
    "def process_county(county, county_population_stats):\n",
    "    df = None\n",
    "    dates = get_dates_for_county(county)\n",
    "    for dt in dates:\n",
    "        files = get_county_files_for_date(county, dt)\n",
    "        if df is not None:\n",
    "            df.append(merge_county_parts_to_dataframe(files))\n",
    "        else:\n",
    "            df = merge_county_parts_to_dataframe(files)\n",
    "    \n",
    "    for k,v in county_population_stats.iteritems():\n",
    "        df[k] = v\n",
    "    save_county_merged_parts_df(county, df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_county_population_stats():\n",
    "    county_population = get_counties_df()  \n",
    "    county_stats = get_county_stats_df()\n",
    "    fips = []\n",
    "    for county, population in county_population.iterrows():\n",
    "        if isinstance(population.fips, dict):\n",
    "            fips.append(int(population.fips['id']))\n",
    "        else:\n",
    "            fips.append(population.fips)\n",
    "    county_population.fips = fips\n",
    "    county_population = county_population.merge(county_stats, how='left', left_on='fips', right_on='fips')\n",
    "    county_population.set_index('id', inplace=True)\n",
    "    county_population = county_population.drop(columns=['location', 'name', 'version', 'typeIdent'])\n",
    "    return county_population\n",
    "\n",
    "    \n",
    "def process_counties():\n",
    "    counties = get_counties_from_files()\n",
    "    county_population_stats = get_county_population_stats()  \n",
    "    \n",
    "    for county in counties:\n",
    "        # print(county)\n",
    "        process_county(county, county_population_stats.loc[county])\n",
    "\n",
    "        \n",
    "# get_county_population_stats()\n",
    "# process_counties()        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d4ea050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "\n",
    "# county = get_counties_from_files()[0]\n",
    "# county_population_stats = get_county_population_stats()  \n",
    "# print('county->',county)\n",
    "# df = process_county(county, county_population_stats.loc[county])\n",
    "# df[100:110]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7c82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "044f6210",
   "metadata": {},
   "source": [
    "### Collect Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c80f1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_outbreaklocation_body_with_includes(county_id: str, includes: str = None) -> dict:\n",
    "    \"\"\" Forms the request body for a count for the outbreak location API \n",
    "    \n",
    "    Args:\n",
    "        count_id: the ID for the County\n",
    "    \n",
    "    Returns:\n",
    "        The request body\n",
    "    \n",
    "    \"\"\"\n",
    "    spec =   {\"spec\": {\"filter\": f\"id == '{county_id}'\"}}\n",
    "              \n",
    "    if includes:\n",
    "        spec[\"spec\"][\"include\"] = includes\n",
    "\n",
    "    return spec\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "                     \n",
    "# d = fetch_one_population('Autauga_Alabama_UnitedStates')\n",
    "# d_backup = d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859186e",
   "metadata": {},
   "source": [
    "#### Population Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b569da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_one_population(county_id: str) -> List[dict]:\n",
    "    population_data = fetch_one('outbreaklocation', make_outbreaklocation_body_with_includes(county_id, 'populationData'))\n",
    "    population_data = population_data[0]['populationData']\n",
    "    last_year = max(e['year'] for e in population_data)\n",
    "    population_data = [e for e in population_data if e['year'] == last_year]\n",
    "    return population_data\n",
    "\n",
    "\n",
    "def unique_gender(population_data: list): \n",
    "    return list(set(e['gender'] for e in population_data))\n",
    "\n",
    "\n",
    "def unique_race(population_data: list): \n",
    "    return list(set(e.get('race') for e in population_data))\n",
    "\n",
    "\n",
    "def unique_ethnicity(population_data: list): \n",
    "    return list(set(e.get('ethnicity') for e in population_data))\n",
    "\n",
    "\n",
    "def unique_population_age(population_data: list): \n",
    "    return list(set(e.get('populationAge') for e in population_data))\n",
    "\n",
    "def to_dict(data, col_name):\n",
    "    result = Counter()\n",
    "    for e in data:\n",
    "        try:\n",
    "            result[e[col_name]] += e['value']\n",
    "        except:\n",
    "            pass\n",
    "    return result\n",
    "        \n",
    "\n",
    "def to_all_counties_dict(col_names):\n",
    "    counties = get_counties_from_files()\n",
    "    results = defaultdict(defaultdict)\n",
    "    for county in counties:\n",
    "        try:\n",
    "            pop = fetch_one_population(county)\n",
    "            for col_name in col_names:\n",
    "                pop_dict = to_dict(pop, col_name)\n",
    "                results[county][col_name] = pop_dict\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return results\n",
    "\n",
    "def to_df(all_counties_dict, col_name):\n",
    "    results = dict(dict())\n",
    "    for k, v in all_counties_dict.items():\n",
    "        results[k] = v[col_name]\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "def to_dfs(all_counties_dict, col_names):\n",
    "    results = {}\n",
    "    for col_name in col_names:\n",
    "        results[col_name] = to_df(all_counties_dict, col_name)\n",
    "    return results\n",
    "\n",
    "def save_dfs(dfs):\n",
    "    for k, v in dfs.items():\n",
    "        v.to_pickle(path.join('.', 'processed_data', 'demographics', \n",
    "                           f'{k}.pkl'))\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "# pop = fetch_one_population('Autauga_Alabama_UnitedStates')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a28cba",
   "metadata": {},
   "source": [
    "##### Execute and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cbd0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_names = ['gender', 'race', 'ethnicity', 'populationAge']\n",
    "# acd = to_all_counties_dict(col_names)\n",
    "# dfs = to_dfs(acd, col_names)\n",
    "# save_dfs(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3e76e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('./processed_data/demographics/populationAge.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29198705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "064ab7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = d_backup\n",
    "# d = d[0]\n",
    "# d = d['populationData']\n",
    "# last_year = max(e['year'] for e in d)\n",
    "# print(len(d))\n",
    "# d = [e for e in d if e['year'] == 2019]\n",
    "# print(len(d))\n",
    "# d[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78257f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = []\n",
    "\n",
    "# for e in d:\n",
    "#     x = (e['gender'], e.get('race'), e.get('ethnicity'), e['value'])\n",
    "#     l.append(x)\n",
    "# print(len(l))\n",
    "# print(len(set(l)))\n",
    "# l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bbc4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_gender = set(e['gender'] for e in d)\n",
    "# unique_race = set(e.get('race') for e in d)\n",
    "# unique_ethnicity = set(e.get('ethnicity') for e in d)\n",
    "# unique_population_age = set(e.get('populationAge') for e in d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccd6ec8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.unique_population_age(population_data: list)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_population_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "908a9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  x = (e['gender'], e.get('race'), e.get('ethnicity'), e['value'])\n",
    "# populationAge\n",
    "\n",
    "# from pprint import pprint\n",
    "\n",
    "# for e in d:\n",
    "#     if e['gender'] == l[0][0] and e.get('race') == l[0][1] and e.get('ethnicity') == l[0][2]:\n",
    "#         pprint(e)\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a775abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2982f514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f069f-2a03-47d3-92b1-da8bc4309f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235024c-1ca8-4f91-a816-151d6361b5de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
