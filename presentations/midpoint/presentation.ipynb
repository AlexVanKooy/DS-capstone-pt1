{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ae3118-92ff-40fc-af15-f61dd34c6d30",
   "metadata": {},
   "source": [
    "# Neural Networks for Predicting Covid-19 Morbidity\n",
    "\n",
    "## Status Update\n",
    "## February 16, 2022\n",
    "\n",
    "#### Evan Falkowski, Noah Krieger, Richard Strouss-Rooney, Alex Van Kooy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a1002-1b09-4e81-8dec-3b981f08024c",
   "metadata": {},
   "source": [
    "$\\rule{10in}{0.4pt}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef14fb-4a21-46cf-b3e3-abd62810a565",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "* The Dataset: Nearly two years of Covid-19 data for the US by county, including demographics, pollution, and weather\n",
    "* The Task: Predict the next one day and next five days from the 30 prior days\n",
    "* The Approach: Design an optimal neural network using the Tensorflow HParams Dashboard\n",
    "* The Goal: Compare the performance of the neural network to other statistical and classic machine learning approaches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de5f450-86ed-4eec-9b13-d7439dfb3eb9",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "* Golden data set of 1,879,589 observations\n",
    "* Cleaned up in DSCI591\n",
    "* Removed textual data (county names, state names)\n",
    "* Standardized data\n",
    "\n",
    "### Geo-Encoding of Counties\n",
    "* Converted FIPS codes to latitude and longitude of the centroid of each county\n",
    "* Used binning technique to place each county in a bin\n",
    "    * Tf.feature_column.bucketized_column\n",
    "    * Tf.feature_column.crossed_column\n",
    "    * Tf.feature_column.embedding_column\n",
    "    * Tf.keras.layers.DenseFeatures\n",
    "* Encoded into a 9 dimensional vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ff0bb-cc79-4fc8-a7c9-8e8a6ff3a8de",
   "metadata": {},
   "source": [
    "    lat_buckets = list(np.linspace(df.latitude.min(), df.latitude.max(),100))\n",
    "    long_buckets = list(np.linspace(df.longitude.min(), df.longitude.max(),100))\n",
    "\n",
    "    #make feature columns\n",
    "    lat_fc = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'),lat_buckets)\n",
    "    long_fc= tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'),long_buckets)\n",
    "\n",
    "    # crossed columns tell the model how the features relate\n",
    "    crossed_latlong = tf.feature_column.crossed_column(keys=[lat_fc, long_fc], hash_bucket_size=1000) # No precise rule, maybe 1000 buckets will be good?\n",
    "\n",
    "    embedded_latlong = tf.feature_column.embedding_column(crossed_latlong,9)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(embedded_latlong)\n",
    "\n",
    "    df[['geo0', 'geo1', 'geo2','geo3', 'geo4','geo5','geo6','geo7','geo8']] = feature_layer({'latitude': df.latitude, 'longitude': df.longitude})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434dc8b-305b-486d-8439-fb74d13ce548",
   "metadata": {},
   "source": [
    "### Time Encoding\n",
    "\n",
    "#### Goal: represent both the cyclical (seasons) and continuous nature of time\n",
    "\n",
    "Let $t_i$ represent day $i$ of the Covid-19 pandemic, where $t_0$ = March 11, 2020 \n",
    "\n",
    "$$\n",
    "\\text{Cyclical}_i = sin\\left(\\frac{2i\\pi}{365}\\right) \\oplus cos\\left(\\frac{2i\\pi}{365}\\right)\n",
    "$$\n",
    "$$\n",
    "\\text{Continuous}_i = sin\\left(\\frac{2i\\pi}{3650}\\right) \\oplus cos\\left(\\frac{2i\\pi}{3650}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a9531-13ae-41d1-9977-a00bea886436",
   "metadata": {},
   "source": [
    "    df.dates = pd.to_datetime(df.dates, format='%Y-%m-%d')\n",
    "    min_date = min(df.dates)\n",
    "    max_date = max(df.dates)\n",
    "    min_date, max_date, df.dates.dtype\n",
    "\n",
    "    df['day'] =(df.dates - min_date).dt.days\n",
    "    df.drop(['dates'], axis=1, inplace=True)\n",
    "\n",
    "    cyclical_interval = 365\n",
    "    continuous_interval = 3650\n",
    "    df['cyclical_sin'] = np.sin((df.day * 2 * np.pi)/cyclical_interval)\n",
    "    df['cyclical_cos'] = np.cos((df.day * 2 * np.pi)/cyclical_interval)\n",
    "    df['continuous_sin'] = np.sin((df.day * 2 * np.pi)/continuous_interval)\n",
    "    df['continuous_cos'] = np.cos((df.day * 2 * np.pi)/continuous_interval)\n",
    "    df.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb18dc7-829f-4bc4-9742-f793b77d3768",
   "metadata": {},
   "source": [
    "## The Task\n",
    "\n",
    "* Started with predicting 1 day from the prior 30 days\n",
    "* For each county\n",
    "    * Create 31 day samples, $t_0, t_1, \\cdots, t_{30}; t_1, t_2, \\cdots, t_{31}; \\cdots; t_{T-31}, t_{T-30}, \\cdots, t_T$, where $T$ is the last observation\n",
    "* Process county by county, saving each 200 observations to a file\n",
    "* Randomly shuffle the resulting files\n",
    "    * Move 70% to the train directory\n",
    "    * Move 15% to the eval directory\n",
    "    * Move 15% to the test directory\n",
    "* Build a generator to\n",
    "    * Shuffle the files\n",
    "    * Pull files 5 at a time\n",
    "    * Get the observations for all 5 files (1000 observations)\n",
    "    * Shuffle the results\n",
    "    * Yield\n",
    "* Build a split function to separate the 30 X values from the label \n",
    "* Construct the train, validate, and test datasets as Tensorflow Dataset objects\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c6653-b4cd-4499-86e7-b419a82d9e70",
   "metadata": {},
   "source": [
    "    def create_generator(files, cycle_length=5):\n",
    "        set_seed()\n",
    "        random.shuffle(files)\n",
    "        for i in range(0, len(files), cycle_length):\n",
    "            subset = files[i:i+cycle_length]\n",
    "            np_arrays = [np.load(s) for s in subset]\n",
    "            np_array = np.concatenate(np_arrays, axis=0)\n",
    "            np.random.shuffle(np_array)\n",
    "            yield np_array\n",
    "\n",
    "\n",
    "    def split_xy(np_array):\n",
    "        X = np_array[:,:-1,:]\n",
    "        y = np_array[:,-1:,:1]\n",
    "        return X,y\n",
    "\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_generator(lambda: create_generator(train_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "    train_ds = train_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "    val_ds = tf.data.Dataset.from_generator(lambda: create_generator(eval_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "    val_ds = val_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "    test_ds = tf.data.Dataset.from_generator(lambda: create_generator(test_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "    test_ds = test_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "\n",
    "## The Approach\n",
    "\n",
    "Design an optimal neural network using the Tensorflow HParams Dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5e6c70-4dff-46c7-abc3-890200225a59",
   "metadata": {},
   "source": [
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([1, 2, 3, 4, 5, 6]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([32, 64, 128, 256, 512]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.0, 0.10, 0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-2, 1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )\n",
    "    \n",
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,101)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "        output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)\n",
    "        \n",
    "\n",
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b79971-ad41-45cb-b07c-563afb138aa1",
   "metadata": {},
   "source": [
    "![image.png](tensorboard_scalar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f83d8-865c-48f3-a901-eccf1820a98d",
   "metadata": {},
   "source": [
    "![image.png](tensorboard_hparams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ace43b-757d-41b6-8e38-b214b4e0643b",
   "metadata": {},
   "source": [
    "![image.png](parallel_coordinate_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c1722-74d2-4ca7-bf31-c9797cae5c11",
   "metadata": {},
   "source": [
    "![image.png](tensorboard_scatter_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841c08c-1cd6-4727-81f5-1bf8c2d6ccab",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "* Ran iteratively, starting with smaller number of epochs to eliminate parts of the search path\n",
    "* Concluded that LSTM and GRU had similar performance.  LSTM slightly outperformed (6 of top 10), so eliminated GRU\n",
    "* Slower learning rate was generally better, so eliminated 0.01\n",
    "* Dropoff didn't have much impact, so settled on 20%\n",
    "* After many rounds of testing, settled on a 3 layer LSTM with 256 Units\n",
    "* 4 - 6 layers did not outperform (getting worse)\n",
    "* 512 units may have been slightly better, but took much longer to train.  Deemed not worth it.\n",
    "* MAE of just over 11 cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf4ab4-fc86-4c55-9c4a-4b1b79942ebf",
   "metadata": {},
   "source": [
    "### Current Architecture\n",
    "\n",
    "<pre>\n",
    "Model: \"Covid-Prediction-30-1\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " Input (InputLayer)          [(None, 30, 101)]         0         \n",
    "\n",
    " LSTM-1 (LSTM)               (None, 30, 256)           366592    \n",
    "\n",
    " Dropout-1 (Dropout)         (None, 30, 256)           0         \n",
    "\n",
    " LSTM-2 (LSTM)               (None, 30, 256)           525312    \n",
    "\n",
    " Dropout-2 (Dropout)         (None, 30, 256)           0         \n",
    "\n",
    " LSTM-3 (LSTM)               (None, 30, 256)           525312    \n",
    "\n",
    " Output (Dense)              (None, 30, 1)             257       \n",
    "\n",
    "=================================================================\n",
    "Total params: 1,417,473\n",
    "Trainable params: <b>1,417,473</b>\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4079eec2-4e32-4ff0-bb69-d5db46067dd1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![image.png](model_3_256.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de956297-fa63-4128-aad5-5baba280ed61",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![image.png](model_3_256_detailed.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdcf8e8-1663-4b76-80bb-3b9a5a260e55",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "* With and without time encoding\n",
    "* With and without geolocation/lat-long coordinates\n",
    "* With and without pre-clustering (need pollution metric)\n",
    "* Optimizers other than Adam (SGD, RMSprop, etc.)\n",
    "* Other loss functions (esp MSE)\n",
    "* Additional layers/more units (I'm already running these)\n",
    "* Batch size\n",
    "* Additional dense layers\n",
    "* Extend the window from 30+1\n",
    "* Vary activation functions on LSTM and dense layers\n",
    "* Batch normalization\n",
    "* Vary y (i.e., predict COVID deaths)\n",
    "* Calculate baselines (guessing zero, guessing last observation, guessing mean of 30 day observations, guessing mean of county, linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525347d-126c-4cf1-bd85-62dd4ad5e868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
