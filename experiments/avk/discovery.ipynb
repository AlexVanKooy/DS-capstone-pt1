{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d0698-a89c-4f60-8092-ad842f06b879",
   "metadata": {},
   "source": [
    "# Experiments Notebook 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8519c3-fa4d-4404-a8c7-9486a7e14861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f293244-628b-4bee-a316-cf96b7b89bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import bz2\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5615b1-c083-4e1c-acbf-d56ee6caa346",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_DIR = pathlib.Path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92d5bdbf-ae0b-453f-b529-7bf3223aeac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('F:/projects/Drexel/DS-capstone-pt1/experiments/avk')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce25521f-bad6-4705-b439-eda844e32615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183fcd9-48c1-4e6d-a197-4c39853316c7",
   "metadata": {},
   "source": [
    "### Load the golden data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c511402-262c-45a0-a9ae-148b6802fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows version\n",
    "golden_dataset_file_name = os.path.join('..', '..', 'data', 'golden', 'feeFiFoFum.pbz2')\n",
    "\n",
    "# data = bz2.BZ2File(golden_dataset_file_name,'rb')\n",
    "with bz2.BZ2File(golden_dataset_file_name,'rb') as data:\n",
    "    df = pd.read_pickle(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fde0d-13e5-4198-9f8e-5689f1130301",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "#### Drop non-numeric and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e761798-4dba-4c6c-96db-04ba85fb6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['NYT_ConfirmedCases.data','NYT_ConfirmedDeaths.data','NYT_ConfirmedDeaths.missing','county','LND110210','countyStateName','stateFip','countyFip']\n",
    "\n",
    "df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf9d9-f105-4376-80ce-65a15ad0bca0",
   "metadata": {},
   "source": [
    "#### Temporarily, replace FIPS codes with latitude and longitude of the centroid of each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128e20f9-d1ca-4bfa-a76a-547209f8bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = pd.read_csv('../noah/2021_Gaz_counties_national.txt', delimiter='\\t')\n",
    "counties.rename(columns={'INTPTLONG                                                                                                               ': 'INTPTLONG'}, inplace=True)\n",
    "\n",
    "counties = counties[['GEOID', 'INTPTLAT', 'INTPTLONG' ]]\n",
    "df.fips = df.fips.astype('int64')\n",
    "\n",
    "df = df.merge(counties, how='left', left_on='fips', right_on='GEOID')\n",
    "df.drop(['GEOID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324af6e-050d-4195-b5d9-4c9d51dac540",
   "metadata": {},
   "source": [
    "#### Replace dates with monotonically increasing integers starting with the minimum date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d464b3-5904-4671-9ef8-97b2450bfc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-03-11 00:00:00'),\n",
       " Timestamp('2022-01-16 00:00:00'),\n",
       " dtype('<M8[ns]'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dates = pd.to_datetime(df.dates, format='%Y-%m-%d')\n",
    "min_date = min(df.dates)\n",
    "max_date = max(df.dates)\n",
    "min_date, max_date, df.dates.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a52c51-f7f0-4835-a57e-8ec4298e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] =(df.dates - min_date).dt.days\n",
    "df.drop(['dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0581ee-2583-4748-8a3c-e4ccadc6a3ba",
   "metadata": {},
   "source": [
    "#### Replace the integer representation of date with sin and cosine encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4517112d-13a1-4685-9f96-22e5ece969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclical_interval = 365\n",
    "continuous_interval = 3650\n",
    "df['cyclical_sin'] = np.sin((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['cyclical_cos'] = np.cos((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['continuous_sin'] = np.sin((df.day * 2 * np.pi)/continuous_interval)\n",
    "df['continuous_cos'] = np.cos((df.day * 2 * np.pi)/continuous_interval)\n",
    "df.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f842b-c049-404a-bb19-55f26e8efa25",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "513da66f-15f7-4907-bf2d-f136a335f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = [\n",
    "       'TotalPopulation.data', 'MaleAndFemale_AtLeast65_Population.data',\n",
    "       'Male_Total_Population.data', 'Female_Total_Population.data',\n",
    "       'MaleAndFemale_Under18_Population.data', 'BLS_EmployedPopulation.data',\n",
    "       'BLS_EmployedPopulation.missing', 'BLS_UnemployedPopulation.data',\n",
    "       'BLS_UnemployedPopulation.missing', 'BLS_UnemploymentRate.data',\n",
    "       'BLS_UnemploymentRate.missing', 'BLS_LaborForcePopulation.data',\n",
    "       'BLS_LaborForcePopulation.missing', 'AverageDailyTemperature.data',\n",
    "       'AverageDailyTemperature.missing', 'AverageDewPoint.data',\n",
    "       'AverageDewPoint.missing', 'AverageRelativeHumidity.data',\n",
    "       'AverageRelativeHumidity.missing', 'AverageSurfaceAirPressure.data',\n",
    "       'AverageSurfaceAirPressure.missing', 'AveragePrecipitationTotal.data',\n",
    "       'AveragePrecipitationTotal.missing', 'AveragePrecipitation.data',\n",
    "       'AveragePrecipitation.missing', 'AverageWindDirection.data',\n",
    "       'AverageWindDirection.missing', 'AverageWindSpeed.data',\n",
    "       'AverageWindSpeed.missing', 'hospitalIcuBeds', 'hospitalStaffedBeds',\n",
    "       'hospitalLicensedBeds', 'latestTotalPopulation', 'jhu_daily_death',\n",
    "       'jhu_daily_cases', 'jhu_daily_new_cases', \n",
    "    'jhu_daily_death_rolling_7',\n",
    "       'jhu_daily_cases_rolling_7', 'jhu_daily_new_cases_rolling_7',\n",
    "       'jhu_daily_death_rolling_30', 'jhu_daily_cases_rolling_30',\n",
    "       'jhu_daily_new_cases_rolling_30', 'jhu_death_rate', 'jhu_case_rate',\n",
    "       'jhu_new_case_rate', 'density', 'icu_beds_per_person',\n",
    "       'staffed_beds_per_person', 'licensed_beds_per_person', 'cold_days',\n",
    "       'hot_days', 'moderate_days', 'gte_65_percent', 'lt_18_percent',\n",
    "       'employed_percent', 'unemployed_percent', 'totalMoved',\n",
    "       'movedWithinState', 'movedWithoutState', 'movedFromAbroad',\n",
    "       'publicTrans', 'totalTrans', 'householdsTotal', 'houseWith65',\n",
    "       'house2+with65', 'houseFamily65', 'houseNonfam65', 'houseNo65',\n",
    "       'house2+No65', 'houseFamilyNo65', 'houseNonfamNo65',\n",
    "       'householdStructuresTotal', 'householdIncomeMedian', 'gini',\n",
    "       'hoursWorkedMean', 'unitsInStructure', 'healthInsTotal',\n",
    "       'healthInsNativeWith', 'healthInsForeignNatWith',\n",
    "       'healthInsForeignNoncitWith', 'healthInsForeignNatNo',\n",
    "       'healthInsForeignNoncitNo', 'healthInsNativeNo', 'pm25', 'INTPTLAT',\n",
    "       'INTPTLONG']\n",
    "cols_raw = ['fips','JHU_ConfirmedCases.data', 'JHU_ConfirmedDeaths.data', 'cyclical_sin', 'cyclical_cos', 'continuous_sin',\n",
    "       'continuous_cos']\n",
    "df_normalized = df[cols_to_normalize]\n",
    "df_normalized = (df_normalized - df_normalized.mean())/df_normalized.std()\n",
    "df_raw = df[cols_raw]\n",
    "df = pd.concat([df_raw, df_normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bf3ef-a6d5-4f22-b336-f46f2798a363",
   "metadata": {},
   "source": [
    "#### Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7731216-cef5-4d0a-a9d2-51dfbe00172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_history = 30\n",
    "days_to_predict = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b3c1f6-ec25-4420-8f70-b8d7f0cef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = df.fips.unique()\n",
    "\n",
    "def x_generator(data, days_of_history=30, days_to_predict=1):\n",
    "    for j, fip in enumerate(fips):\n",
    "        if not j % 100: print(j, end=' ')\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days_of_history, len(county) - days_to_predict):\n",
    "            data_matrix = data.iloc[i - days_of_history: i, 1:].to_numpy()\n",
    "            yield data_matrix\n",
    "            \n",
    "def y_generator(data, days_of_history=30, days_to_predict=1):\n",
    "    for fip in fips:\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days_of_history, len(county) - days_to_predict):\n",
    "            data_matrix = data.iloc[i: i + days_to_predict, 1:3].to_numpy()\n",
    "            yield data_matrix\n",
    "    \n",
    "def xy_generator(data, days=31):\n",
    "    for j, fip in enumerate(fips):\n",
    "        if not j % 100: print(j, end=' ')\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days, len(county) + 1):\n",
    "            data_matrix = data.iloc[i - days: i, 1:].to_numpy()\n",
    "            yield data_matrix\n",
    "            \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac163c8-45f9-4a5c-972d-babb0d65816a",
   "metadata": {},
   "source": [
    "##### Save the raw X and Y to files of 50,000 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32add75b-d9ef-432b-a607-6fe85948c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = []\n",
    "j = 0\n",
    "\n",
    "N_SAMPLES = 2\n",
    "\n",
    "for i, x in enumerate(xy_generator(df)):\n",
    "    Xi.append(x)\n",
    "    if i and not i % (N_SAMPLES - 1):\n",
    "        X = np.asarray(Xi)\n",
    "        np.savez(os.path.join('.','data', f'x_{j}.np'), X)\n",
    "        j += 1\n",
    "        Xi = []\n",
    "if Xi:\n",
    "    X = np.asarray(Xi)\n",
    "    np.savez(os.path.join('.','data', f'x_{j}.npz'), X)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664610a-0086-4410-b237-4ae731bce0d7",
   "metadata": {},
   "source": [
    "##### Split into train, test, eval directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ea094f1-f70d-4c1e-8c90-293d8f460e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata_dir = pathlib.Path.cwd().joinpath(\"..\",\"noah\",\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee865b7d-1f2b-4c1e-b24b-ee56a59ca907",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_files = glob('../noah/data/x_*.npz')\n",
    "random.shuffle(x_files)\n",
    "n_files = len(x_files)\n",
    "print(n_files)\n",
    "n_train = int(n_files * 0.70)\n",
    "print(n_train)\n",
    "n_eval = int(n_files * 0.15)\n",
    "print(n_eval)\n",
    "n_test = n_files - n_train - n_eval\n",
    "print(n_test)\n",
    "train_files = x_files[:n_train]\n",
    "# print(len(train_files))\n",
    "eval_files = x_files[n_train:n_train+n_test]\n",
    "# print(len(eval_files))\n",
    "test_files = x_files[n_train+n_test:]\n",
    "assert n_files == len(train_files) + len(eval_files) + len(test_files)\n",
    "for (subdir, lst) in [['train', train_files], ['eval', eval_files], ['test', test_files]]:\n",
    "    for file in lst:\n",
    "        shutil.move(file, os.path.join('.', 'data', subdir))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72038235-dbcf-47e3-8a70-7b8021b68f7f",
   "metadata": {},
   "source": [
    "##### Create the Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "72c917f4-f604-48cf-a8dd-d07a1fffed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob('../noah/data/train/x_*.npz')\n",
    "eval_files = glob('.../noah/data/eval/x_*.npz')\n",
    "test_files = glob('../noah/data/test/x_*.npz')\n",
    "\n",
    "n_readers = 5\n",
    "n_parse_threads = 5\n",
    "len_array = 995\n",
    "\n",
    "def create_generator(files, cycle_length=5):\n",
    "    random.shuffle(files)\n",
    "    for i in range(0, len(files), cycle_length):\n",
    "        subset = files[i:i+cycle_length]\n",
    "        np_arrays = [np.load(s)['arr_0'] for s in subset]\n",
    "        np_array = np.concatenate(np_arrays, axis=0)\n",
    "        if np_array.shape[0] != len_array:\n",
    "            continue\n",
    "        np.random.shuffle(np_array)\n",
    "        yield np_array        \n",
    "            \n",
    "\n",
    "def split_xy(np_array):\n",
    "    # seq_len = np_array.shape[1]\n",
    "    X = np_array[:,:-1,:]\n",
    "    y = np_array[:,-1:,:1]\n",
    "    return X,y\n",
    "        \n",
    "    \n",
    "def get_dataset_size():\n",
    "    size = 0\n",
    "    for file in x_files:\n",
    "        x = np.load(file)['arr_0']\n",
    "        if x.shape[0] != 199:\n",
    "            continue\n",
    "        # print(x.shape)\n",
    "        # break\n",
    "        size += x.shape[0]\n",
    "    return size\n",
    "\n",
    "\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000, seed=None):\n",
    "    \"\"\" Adapted from https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\"\"\"\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=seed)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    print('Train size:', train_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    print('Val size:', val_size)\n",
    "    print('Test size:', ds_size - train_size - val_size)\n",
    "    \n",
    "    BATCH_SIZE = 4096\n",
    "    train_ds = ds.take(train_size).prefetch(1)    \n",
    "    val_ds = ds.skip(train_size).take(val_size).prefetch(1)\n",
    "    test_ds = ds.skip(train_size).skip(val_size).prefetch(1)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds    \n",
    "    \n",
    "    \n",
    "train_ds = tf.data.Dataset.from_generator(lambda: create_generator(train_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "train_ds = train_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(lambda: create_generator(eval_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "val_ds = val_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(lambda: create_generator(test_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "test_ds = test_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "# print('Split dataset')\n",
    "# dataset_size = get_dataset_size()\n",
    "# print(f'Size is {dataset_size}')\n",
    "# train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset, dataset_size, shuffle=False)\n",
    "\n",
    "\n",
    "# print('size:', get_dataset_size())\n",
    "# for line in dataset.take(1):\n",
    "#     X, y = line\n",
    "#     print('X shape:', X.shape)\n",
    "#     print('X:', X)\n",
    "#     print('y shape:', y.shape)\n",
    "#     print('y:', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b955af17-9a38-4b4b-a73c-861a69b44881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fda3df5d-10a1-4ae9-8eeb-1465561d9e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_generator(lambda: create_generator(x_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "\n",
    "i = 0\n",
    "for x in val_ds:\n",
    "    print(x.shape)\n",
    "    break\n",
    "    i += 1\n",
    "    if not i % 100_000:\n",
    "        print(i, end=' ')\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31cd2bff-51a3-494a-bc11-4bcd6c8fd1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ccec1-2529-45bd-8d4c-49e2ca0a27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help( dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb7116b0-4c68-4b70-8971-c6990256730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in val_ds.take(100):\n",
    "    X, y = line\n",
    "    print('X shape:', X.shape)\n",
    "    \n",
    "    # print('X:', X)\n",
    "    print('y shape:', y.shape)\n",
    "    # print('y:', y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaea1d7-a885-4ca5-9f52-83846c1bd2df",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f591b3b-a116-4223-9a66-b549b75a1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(512, return_sequences=True)(input)\n",
    "dropout1 = keras.layers.Dropout(0.20)(lstm1)\n",
    "lstm2 = keras.layers.LSTM(512, return_sequences=True)(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20)(lstm2)\n",
    "lstm3 = keras.layers.LSTM(512, return_sequences=True)(dropout2)\n",
    "dropout3 = keras.layers.Dropout(0.20)(lstm3)\n",
    "lstm4 = keras.layers.LSTM(512, return_sequences=True)(dropout3)\n",
    "dropout4 = keras.layers.Dropout(0.20)(lstm4)\n",
    "lstm5 = keras.layers.LSTM(512)(dropout4)\n",
    "output = keras.layers.Dense(1)(lstm5)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d996daba-5af0-40cc-a2b2-faa36f00f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for quick testing\n",
    "\n",
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(32, return_sequences=True)(input)\n",
    "output = keras.layers.Dense(1)(lstm1)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dd42f8d-55a5-4ef6-962f-bf2cdfe25af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 30, 92)]          0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 30, 32)            16000     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30, 1)             33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,033\n",
      "Trainable params: 16,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3280bb06-dae2-4115-9eaa-2c24336ea76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras.utils.plot_model(model, to_file='multilayer_perceptron_graph.png')\n",
    "keras.utils.plot_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a598a9-f8a2-48d6-8338-cd527f056f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51695cb4-1dde-47ed-8b0e-2bc35befe767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "   1256/Unknown - 144s 110ms/step - loss: 1714.7162 - mse: 4667308.5000 - mae: 1714.7292WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "1256/1256 [==============================] - 144s 110ms/step - loss: 1714.7162 - mse: 4667308.5000 - mae: 1714.7292\n",
      "Epoch 2/32\n",
      " 237/1256 [====>.........................] - ETA: 1:07 - loss: 1675.7246 - mse: 4532346.5000 - mae: 1675.7258"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [71]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m     11\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\engine\\training.py:1389\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1387\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1389\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1391\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\callbacks.py:1106\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1106\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\utils\\tf_utils.py:563\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    561\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    910\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    911\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 914\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    915\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\utils\\tf_utils.py:557\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    555\u001b[0m   \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 557\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m   \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1223\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \n\u001b[0;32m   1202\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1223\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1189\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1188\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq_len = days_of_history\n",
    "future_predict = days_to_predict\n",
    "NAME = f\"{seq_len}-SEQ-{future_predict}-PRED-{int(time.time())}\"#unique name for each model\n",
    "\n",
    "# setting the optimizer parameters\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "model.compile(optimizer = opt,  loss='mae',  metrics=['mse', 'mae'])\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm.h5', save_best_only=True)\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=10, restore_best_weights=True)\n",
    "history = model.fit(train_ds, epochs=32, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[tensorboard, checkpoint_cb, early_stopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb196d17-bb25-49e6-9085-81e580259d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('F:/projects/Drexel/DS-capstone-pt1/experiments/avk')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f692656d-ff4e-43ce-b6b5-93301c71c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m checkpoint_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/model/covid_lstm.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\keras\\engine\\training.py:1395\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1393\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1396\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(Empty logs). Please use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1397\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1398\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1399\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1400\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1401\u001b[0m epoch_logs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(logs)\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;66;03m# Run validation.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=10, restore_best_weights=True)\n",
    "history = model.fit(train_ds, epochs=32, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a52820-5929-47a6-b787-9ab60dd994e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbb93a-133a-4770-8573-bd4d6d30d9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d3f08-9917-4700-aa20-875fcedfa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added this to the data preprocessing, so not required\n",
    "\n",
    "class TimeEncoding(keras.layers.Layer):\n",
    "    \"\"\" Layer to encode cyclical and continuous time.  \n",
    "    Input should an n x 1 array or vector of integers.  \n",
    "    Integers represent number of time units (i.e., days) from the starting point\"\"\"\n",
    "\n",
    "    def __init__(self, cyclical_interval=365, continuous_interval=3650 , **kwargs):\n",
    "        self.cyclical_interval = cyclical_interval\n",
    "        self.continuous_interval = continuous_interval\n",
    "        super(TimeEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        cyclical_sin = np.sin((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        cyclical_cos = np.cos((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        continuous_sin = np.sin((x * 2 * np.pi)/self.continuous_interval)\n",
    "        continuous_cos = np.cos((x * 2 * np.pi)/self.continuous_interval)\n",
    "        \n",
    "        return keras.layers.concatenate([cyclical_sin, cyclical_cos, continuous_sin, continuous_cos], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42a910-0e7c-4bad-b21a-fc9b8a939924",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_encoding = TimeEncoding()\n",
    "x = np.arange(30)/1.0\n",
    "x = x[:, tf.newaxis]\n",
    "time_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68d65a-5b39-40d0-bc3f-b234a22de38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d016b9-bb92-4212-b14f-2ff53eb074a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capDL",
   "language": "python",
   "name": "capdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
