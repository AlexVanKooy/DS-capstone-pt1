{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88d40da-c25a-4cfd-87ff-ec5f996b3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19ef38-1301-4a0b-903d-207966c84bef",
   "metadata": {},
   "source": [
    "## Feature column crossing \n",
    "\n",
    "process taken from https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21835a4-c408-41d5-bf34-9c76e95f1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = pathlib.Path.cwd().parent.parent.resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31555452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max latitude = 69.449343\n",
      "max longitude = 178.338813\n"
     ]
    }
   ],
   "source": [
    "counties = pd.read_csv(top_dir.joinpath('experiments','noah','2021_Gaz_counties_national.txt'), delimiter='\\t')\n",
    "\n",
    "counties.columns = counties.columns.str.replace(\" \",\"\")\n",
    "\n",
    "cdf = counties[['GEOID', 'INTPTLAT','INTPTLONG']].copy()\n",
    "cdf.rename(columns={'INTPTLAT':'latitude', 'INTPTLONG':'longitude'},inplace=True)\n",
    "\n",
    "print(f\"max latitude = {cdf.latitude.max()}\")\n",
    "print(f\"max longitude = {cdf.longitude.max()}\")\n",
    "                       \n",
    "feature_columns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732e6f8-6701-4580-93cf-3b14d4caf75a",
   "metadata": {},
   "source": [
    "# Dropping Alaska and Hawaii\n",
    "\n",
    "If we want to keep it then just remove this and set noak = cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d4a010-ae0f-44f0-a7d7-78566643058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "noak=cdf.drop(cdf[(cdf['GEOID'] > 2000) & (cdf['GEOID'] < 3000)].index)\n",
    "noak.drop(noak[(noak['GEOID']> 15000) & (noak['GEOID'] < 16000)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d771cb80-3070-4503-8d16-31e2b02775a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long max = -65.307769, long min = -124.210929\n"
     ]
    }
   ],
   "source": [
    "print(f\"long max = {noak.longitude.max()}, long min = {noak.longitude.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aeb1ffb-6bd6-4318-8679-328f8d4b6fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lat max = 48.842653, lat min = 17.948052\n"
     ]
    }
   ],
   "source": [
    "print(f\"lat max = {noak.latitude.max()}, lat min = {noak.latitude.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1769148-a380-4c7e-a9ec-c5b3f9f888f2",
   "metadata": {},
   "source": [
    "For the continental US \n",
    "\n",
    "Latitude is in the range (17, 50) degrees.\n",
    "\n",
    "Longitude is in the range (-124,-65) degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaaacf4f-53a3-4397-ac52-474ac639bfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latitude diff = 30.894600999999998\n",
      "longitude diff = 58.90316\n"
     ]
    }
   ],
   "source": [
    "print(f'latitude diff = {noak.latitude.max()-noak.latitude.min()}')\n",
    "print(f'longitude diff = {noak.longitude.max()-noak.longitude.min()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479042e",
   "metadata": {},
   "source": [
    "#### How many partititions  do we need using average distance between county centroids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb89c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321.2296677905454 318.4777667803588\n",
      "This might be too many. I still used it. For hash bucket size, I multiplied these values and divided by ten.Might still be too granular.\n"
     ]
    }
   ],
   "source": [
    "longList = noak.longitude.sort_values()\n",
    "meanLong = longList.diff(10).mean()\n",
    "latList = noak.latitude.sort_values()\n",
    "meanLat = latList.diff(10).mean()\n",
    "\n",
    "longSize = noak.longitude.max()-noak.longitude.min()\n",
    "latSize = noak.latitude.max()-noak.latitude.min()\n",
    "print(longSize/meanLong, latSize/meanLat)\n",
    "print(\"This might be too many. I still used it. For hash bucket size, I multiplied these values and divided by ten.\" \n",
    "      \"Might still be too granular.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f93708",
   "metadata": {},
   "source": [
    "### Bucketizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c2ee15-c020-470c-a4e4-67106494e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lat_buckets = list(np.linspace(noak.latitude.min(),noak.latitude.max(),322))\n",
    "long_buckets = list(np.linspace(noak.longitude.min(),noak.longitude.max(),319))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ee7f88e-80c1-47fd-8361-7ab13773fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_fc = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'),lat_buckets)\n",
    "long_fc= tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'),long_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d90f3fc7-55b9-4693-8f30-82c871281ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_coordinate_fc = tf.feature_column.crossed_column(keys=[lat_fc, long_fc], hash_bucket_size=10272) # No precise rule, maybe 1000 buckets will be good?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7ada3",
   "metadata": {},
   "source": [
    "### Turning the feature column into a dense layer to be added to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a821a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_feature = tf.feature_column.indicator_column(cross_coordinate_fc)\n",
    "feature_columns.append(crossed_feature)\n",
    "\n",
    "geo_crossed_layer = keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e97ddf-7972-436a-b9f2-f1c8593a28df",
   "metadata": {},
   "source": [
    "# Final notes\n",
    "\n",
    "The article says it's best to feed the lat_fc and long_fc into the network along with the crossed column "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
