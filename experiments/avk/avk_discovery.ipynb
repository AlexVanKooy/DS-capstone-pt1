{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d0698-a89c-4f60-8092-ad842f06b879",
   "metadata": {},
   "source": [
    "# Experiments Notebook 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8519c3-fa4d-4404-a8c7-9486a7e14861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in f:\\projects\\venvs\\capdl\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in f:\\projects\\venvs\\capdl\\lib\\site-packages (from pydot) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f293244-628b-4bee-a316-cf96b7b89bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import bz2\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8ecb2e-2eb5-42c2-87a3-d2550cbf7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92d5bdbf-ae0b-453f-b529-7bf3223aeac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce25521f-bad6-4705-b439-eda844e32615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183fcd9-48c1-4e6d-a197-4c39853316c7",
   "metadata": {},
   "source": [
    "### Load the golden data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c511402-262c-45a0-a9ae-148b6802fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows version\n",
    "golden_dataset_file_name = os.path.join('..', '..', 'data', 'golden', 'feeFiFoFum.pbz2')\n",
    "\n",
    "# data = bz2.BZ2File(golden_dataset_file_name,'rb')\n",
    "with bz2.BZ2File(golden_dataset_file_name,'rb') as data:\n",
    "    df = pd.read_pickle(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fde0d-13e5-4198-9f8e-5689f1130301",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "#### Drop non-numeric and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e761798-4dba-4c6c-96db-04ba85fb6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['NYT_ConfirmedCases.data','NYT_ConfirmedDeaths.data','NYT_ConfirmedDeaths.missing','county','LND110210','countyStateName','stateFip','countyFip']\n",
    "\n",
    "df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf9d9-f105-4376-80ce-65a15ad0bca0",
   "metadata": {},
   "source": [
    "#### Temporarily, replace FIPS codes with latitude and longitude of the centroid of each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "128e20f9-d1ca-4bfa-a76a-547209f8bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = pd.read_csv('../noah/2021_Gaz_counties_national.txt', delimiter='\\t')\n",
    "counties.rename(columns={'INTPTLONG                                                                                                               ': 'INTPTLONG'}, inplace=True)\n",
    "\n",
    "counties = counties[['GEOID', 'INTPTLAT', 'INTPTLONG' ]]\n",
    "df.fips = df.fips.astype('int64')\n",
    "\n",
    "df = df.merge(counties, how='left', left_on='fips', right_on='GEOID')\n",
    "df.drop(['GEOID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca9f5f97-bb66-4992-8e9f-99e8e4098c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossed_latlong = pp.get_latlong_fc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2505af45-5cd8-43a3-9400-c9d6d1456818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 9 here is the dimensions parameter in the embedding_column function. It appears to be another hyperparameter.\n",
    "embedded_latlong = tf.feature_column.embedding_column(crossed_latlong,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90343dd4-8e19-4a3a-bdad-f4eafc75219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(embedded_latlong)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324af6e-050d-4195-b5d9-4c9d51dac540",
   "metadata": {},
   "source": [
    "#### Replace dates with monotonically increasing integers starting with the minimum date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d464b3-5904-4671-9ef8-97b2450bfc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-03-11 00:00:00'),\n",
       " Timestamp('2022-01-16 00:00:00'),\n",
       " dtype('<M8[ns]'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dates = pd.to_datetime(df.dates, format='%Y-%m-%d')\n",
    "min_date = min(df.dates)\n",
    "max_date = max(df.dates)\n",
    "min_date, max_date, df.dates.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82a52c51-f7f0-4835-a57e-8ec4298e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] =(df.dates - min_date).dt.days\n",
    "df.drop(['dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0581ee-2583-4748-8a3c-e4ccadc6a3ba",
   "metadata": {},
   "source": [
    "#### Replace the integer representation of date with sin and cosine encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4517112d-13a1-4685-9f96-22e5ece969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclical_interval = 365\n",
    "continuous_interval = 3650\n",
    "df['cyclical_sin'] = np.sin((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['cyclical_cos'] = np.cos((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['continuous_sin'] = np.sin((df.day * 2 * np.pi)/continuous_interval)\n",
    "df['continuous_cos'] = np.cos((df.day * 2 * np.pi)/continuous_interval)\n",
    "df.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f842b-c049-404a-bb19-55f26e8efa25",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "513da66f-15f7-4907-bf2d-f136a335f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = [\n",
    "       'TotalPopulation.data', 'MaleAndFemale_AtLeast65_Population.data',\n",
    "       'Male_Total_Population.data', 'Female_Total_Population.data',\n",
    "       'MaleAndFemale_Under18_Population.data', 'BLS_EmployedPopulation.data',\n",
    "       'BLS_EmployedPopulation.missing', 'BLS_UnemployedPopulation.data',\n",
    "       'BLS_UnemployedPopulation.missing', 'BLS_UnemploymentRate.data',\n",
    "       'BLS_UnemploymentRate.missing', 'BLS_LaborForcePopulation.data',\n",
    "       'BLS_LaborForcePopulation.missing', 'AverageDailyTemperature.data',\n",
    "       'AverageDailyTemperature.missing', 'AverageDewPoint.data',\n",
    "       'AverageDewPoint.missing', 'AverageRelativeHumidity.data',\n",
    "       'AverageRelativeHumidity.missing', 'AverageSurfaceAirPressure.data',\n",
    "       'AverageSurfaceAirPressure.missing', 'AveragePrecipitationTotal.data',\n",
    "       'AveragePrecipitationTotal.missing', 'AveragePrecipitation.data',\n",
    "       'AveragePrecipitation.missing', 'AverageWindDirection.data',\n",
    "       'AverageWindDirection.missing', 'AverageWindSpeed.data',\n",
    "       'AverageWindSpeed.missing', 'hospitalIcuBeds', 'hospitalStaffedBeds',\n",
    "       'hospitalLicensedBeds', 'latestTotalPopulation', 'jhu_daily_death',\n",
    "       'jhu_daily_cases', 'jhu_daily_new_cases', \n",
    "    'jhu_daily_death_rolling_7',\n",
    "       'jhu_daily_cases_rolling_7', 'jhu_daily_new_cases_rolling_7',\n",
    "       'jhu_daily_death_rolling_30', 'jhu_daily_cases_rolling_30',\n",
    "       'jhu_daily_new_cases_rolling_30', 'jhu_death_rate', 'jhu_case_rate',\n",
    "       'jhu_new_case_rate', 'density', 'icu_beds_per_person',\n",
    "       'staffed_beds_per_person', 'licensed_beds_per_person', 'cold_days',\n",
    "       'hot_days', 'moderate_days', 'gte_65_percent', 'lt_18_percent',\n",
    "       'employed_percent', 'unemployed_percent', 'totalMoved',\n",
    "       'movedWithinState', 'movedWithoutState', 'movedFromAbroad',\n",
    "       'publicTrans', 'totalTrans', 'householdsTotal', 'houseWith65',\n",
    "       'house2+with65', 'houseFamily65', 'houseNonfam65', 'houseNo65',\n",
    "       'house2+No65', 'houseFamilyNo65', 'houseNonfamNo65',\n",
    "       'householdStructuresTotal', 'householdIncomeMedian', 'gini',\n",
    "       'hoursWorkedMean', 'unitsInStructure', 'healthInsTotal',\n",
    "       'healthInsNativeWith', 'healthInsForeignNatWith',\n",
    "       'healthInsForeignNoncitWith', 'healthInsForeignNatNo',\n",
    "       'healthInsForeignNoncitNo', 'healthInsNativeNo', 'pm25']\n",
    "cols_raw = ['fips','JHU_ConfirmedCases.data', 'JHU_ConfirmedDeaths.data', 'cyclical_sin', 'cyclical_cos', 'continuous_sin',\n",
    "       'continuous_cos', 'latitude','longitude']\n",
    "df_normalized = df[cols_to_normalize]\n",
    "df_normalized = (df_normalized - df_normalized.mean())/df_normalized.std()\n",
    "df_raw = df[cols_raw]\n",
    "df = pd.concat([df_raw, df_normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bf3ef-a6d5-4f22-b336-f46f2798a363",
   "metadata": {},
   "source": [
    "#### Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7731216-cef5-4d0a-a9d2-51dfbe00172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_history = 30\n",
    "days_to_predict = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14b3c1f6-ec25-4420-8f70-b8d7f0cef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = df.fips.unique()\n",
    "\n",
    "def x_generator(data, days_of_history=30, days_to_predict=1):\n",
    "    for j, fip in enumerate(fips):\n",
    "        if not j % 100: print(j, end=' ')\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days_of_history, len(county) - days_to_predict):\n",
    "            data_matrix = data.iloc[i - days_of_history: i, 1:].to_numpy()\n",
    "            yield data_matrix\n",
    "            \n",
    "def y_generator(data, days_of_history=30, days_to_predict=1):\n",
    "    for fip in fips:\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days_of_history, len(county) - days_to_predict):\n",
    "            data_matrix = data.iloc[i: i + days_to_predict, 1:3].to_numpy()\n",
    "            yield data_matrix\n",
    "    \n",
    "def xy_generator(data, days=31):\n",
    "    for j, fip in enumerate(fips):\n",
    "        if not j % 100: print(j, end=' ')\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days, len(county) + 1):\n",
    "            data_matrix = data.iloc[i - days: i, 1:].to_numpy()\n",
    "            yield data_matrix\n",
    "            \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac163c8-45f9-4a5c-972d-babb0d65816a",
   "metadata": {},
   "source": [
    "##### Save the raw X and Y to files of 50,000 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32add75b-d9ef-432b-a607-6fe85948c224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3150e322d84c78b568f2e42b4e1c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 "
     ]
    }
   ],
   "source": [
    "Xi = []\n",
    "j = 0\n",
    "\n",
    "N_SAMPLES = 200\n",
    "\n",
    "for i, x in tqdm(enumerate(xy_generator(df))):\n",
    "    Xi.append(x)\n",
    "    if i and not i % (N_SAMPLES - 1):\n",
    "        X = np.asarray(Xi)\n",
    "        np.savez(os.path.join('.','data', f'x_{j}.npz'), X)\n",
    "        j += 1\n",
    "        Xi = []\n",
    "    \n",
    "if Xi:\n",
    "    X = np.asarray(Xi)\n",
    "    np.savez(os.path.join('.','data', f'x_{j}.npz'), X)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664610a-0086-4410-b237-4ae731bce0d7",
   "metadata": {},
   "source": [
    "##### Split into train, test, eval directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ca2183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('F:/projects/Drexel/DS-capstone-pt1/experiments/avk')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea094f1-f70d-4c1e-8c90-293d8f460e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdata_dir = pathlib.Path.cwd().joinpath(\"..\",\"noah\",\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee865b7d-1f2b-4c1e-b24b-ee56a59ca907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8983\n",
      "6288\n",
      "1347\n",
      "1348\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: './data\\\\x_11685.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Programs\\Python\\python39\\lib\\shutil.py:806\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 806\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_dst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: './data\\\\x_11685.npz' -> '.\\\\data\\\\train\\\\x_11685.npz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (subdir, lst) \u001b[38;5;129;01min\u001b[39;00m [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, train_files], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_files], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, test_files]]:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m lst:\n\u001b[1;32m---> 19\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubdir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programs\\Python\\python39\\lib\\shutil.py:827\u001b[0m, in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    826\u001b[0m         copy_function(src, real_dst)\n\u001b[1;32m--> 827\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m real_dst\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: './data\\\\x_11685.npz'"
     ]
    }
   ],
   "source": [
    "x_files = glob('./data/x_*.npz')\n",
    "random.shuffle(x_files)\n",
    "n_files = len(x_files)\n",
    "print(n_files)\n",
    "n_train = int(n_files * 0.70)\n",
    "print(n_train)\n",
    "n_eval = int(n_files * 0.15)\n",
    "print(n_eval)\n",
    "n_test = n_files - n_train - n_eval\n",
    "print(n_test)\n",
    "train_files = x_files[:n_train]\n",
    "# print(len(train_files))\n",
    "eval_files = x_files[n_train:n_train+n_test]\n",
    "# print(len(eval_files))\n",
    "test_files = x_files[n_train+n_test:]\n",
    "assert n_files == len(train_files) + len(eval_files) + len(test_files)\n",
    "for (subdir, lst) in [['train', train_files], ['eval', eval_files], ['test', test_files]]:\n",
    "    for file in lst:\n",
    "        shutil.move(file, os.path.join('.', 'data', subdir))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72038235-dbcf-47e3-8a70-7b8021b68f7f",
   "metadata": {},
   "source": [
    "##### Create the Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c917f4-f604-48cf-a8dd-d07a1fffed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_files = glob('./data/train/x_*.npz')\n",
    "eval_files = glob('./data/eval/x_*.npz')\n",
    "test_files = glob('./data/test/x_*.npz')\n",
    "\n",
    "n_readers = 5\n",
    "n_parse_threads = 5\n",
    "len_array = 995\n",
    "\n",
    "def create_generator(files, cycle_length=5):\n",
    "    random.shuffle(files)\n",
    "    for i in range(0, len(files), cycle_length):\n",
    "        subset = files[i:i+cycle_length]\n",
    "        np_arrays = [np.load(s)['arr_0'] for s in subset]\n",
    "        np_array = np.concatenate(np_arrays, axis=0)\n",
    "        if np_array.shape[0] != len_array:\n",
    "            continue\n",
    "        np.random.shuffle(np_array)\n",
    "        yield np_array        \n",
    "            \n",
    "\n",
    "def split_xy(np_array):\n",
    "    # seq_len = np_array.shape[1]\n",
    "    X = np_array[:,:-1,:]\n",
    "    y = np_array[:,-1:,:1]\n",
    "    return X,y\n",
    "        \n",
    "    \n",
    "def get_dataset_size():\n",
    "    size = 0\n",
    "    for file in x_files:\n",
    "        x = np.load(file)['arr_0']\n",
    "        if x.shape[0] != 199:\n",
    "            continue\n",
    "        # print(x.shape)\n",
    "        # break\n",
    "        size += x.shape[0]\n",
    "    return size\n",
    "\n",
    "\n",
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000, seed=None):\n",
    "    \"\"\" Adapted from https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\"\"\"\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    if shuffle:\n",
    "        # Specify seed to always have the same split distribution between runs\n",
    "        ds = ds.shuffle(shuffle_size, seed=seed)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    print('Train size:', train_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    print('Val size:', val_size)\n",
    "    print('Test size:', ds_size - train_size - val_size)\n",
    "    \n",
    "    BATCH_SIZE = 4096\n",
    "    train_ds = ds.take(train_size).prefetch(1)    \n",
    "    val_ds = ds.skip(train_size).take(val_size).prefetch(1)\n",
    "    test_ds = ds.skip(train_size).skip(val_size).prefetch(1)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds    \n",
    "    \n",
    "    \n",
    "train_ds = tf.data.Dataset.from_generator(lambda: create_generator(train_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "train_ds = train_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(lambda: create_generator(eval_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "val_ds = val_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(lambda: create_generator(test_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "test_ds = test_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b955af17-9a38-4b4b-a73c-861a69b44881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda3df5d-10a1-4ae9-8eeb-1465561d9e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m val_ds:\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_generator(lambda: create_generator(x_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "\n",
    "i = 0\n",
    "for x in val_ds:\n",
    "    print(x.shape)\n",
    "    break\n",
    "    i += 1\n",
    "    if not i % 100_000:\n",
    "        print(i, end=' ')\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd2bff-51a3-494a-bc11-4bcd6c8fd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ccec1-2529-45bd-8d4c-49e2ca0a27d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help( dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb7116b0-4c68-4b70-8971-c6990256730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n",
      "X shape: (995, 30, 92)\n",
      "y shape: (995, 1, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m val_ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      2\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m line\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX shape:\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:836\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    835\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:819\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 819\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    824\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mF:\\projects\\venvs\\capDL\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:2917\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   2916\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2917\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2918\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2919\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   2921\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for line in val_ds.take(100):\n",
    "    X, y = line\n",
    "    print('X shape:', X.shape)\n",
    "    \n",
    "    # print('X:', X)\n",
    "    print('y shape:', y.shape)\n",
    "    # print('y:', y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaea1d7-a885-4ca5-9f52-83846c1bd2df",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91080b49-5239-474c-8004-fc4897d14b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f591b3b-a116-4223-9a66-b549b75a1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(512, return_sequences=True)(input)\n",
    "dropout1 = keras.layers.Dropout(0.20)(lstm1)\n",
    "lstm2 = keras.layers.LSTM(512, return_sequences=True)(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20)(lstm2)\n",
    "lstm3 = keras.layers.LSTM(512, return_sequences=True)(dropout2)\n",
    "dropout3 = keras.layers.Dropout(0.20)(lstm3)\n",
    "lstm4 = keras.layers.LSTM(512, return_sequences=True)(dropout3)\n",
    "dropout4 = keras.layers.Dropout(0.20)(lstm4)\n",
    "lstm5 = keras.layers.LSTM(512)(dropout4)\n",
    "output = keras.layers.Dense(1)(lstm5)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996daba-5af0-40cc-a2b2-faa36f00f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for quick testing\n",
    "\n",
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(32, return_sequences=True)(input)\n",
    "output = keras.layers.Dense(1)(lstm1)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd42f8d-55a5-4ef6-962f-bf2cdfe25af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280bb06-dae2-4115-9eaa-2c24336ea76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, to_file='multilayer_perceptron_graph.png')\n",
    "keras.utils.plot_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a598a9-f8a2-48d6-8338-cd527f056f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e5dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51695cb4-1dde-47ed-8b0e-2bc35befe767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len = days_of_history\n",
    "future_predict = days_to_predict\n",
    "NAME = f\"{seq_len}-SEQ-{future_predict}-PRED-{int(time.time())}\"#unique name for each model\n",
    "\n",
    "# setting the optimizer parameters\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "model.compile(optimizer = opt,  loss='mae',  metrics=['mse', 'mae'])\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm.h5', save_best_only=True)\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=10, restore_best_weights=True)\n",
    "history = model.fit(train_ds, epochs=32, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[tensorboard, checkpoint_cb, early_stopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb196d17-bb25-49e6-9085-81e580259d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692656d-ff4e-43ce-b6b5-93301c71c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=10, restore_best_weights=True)\n",
    "history = model.fit(train_ds, epochs=32, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a52820-5929-47a6-b787-9ab60dd994e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbb93a-133a-4770-8573-bd4d6d30d9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d3f08-9917-4700-aa20-875fcedfa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added this to the data preprocessing, so not required\n",
    "\n",
    "class TimeEncoding(keras.layers.Layer):\n",
    "    \"\"\" Layer to encode cyclical and continuous time.  \n",
    "    Input should an n x 1 array or vector of integers.  \n",
    "    Integers represent number of time units (i.e., days) from the starting point\"\"\"\n",
    "\n",
    "    def __init__(self, cyclical_interval=365, continuous_interval=3650 , **kwargs):\n",
    "        self.cyclical_interval = cyclical_interval\n",
    "        self.continuous_interval = continuous_interval\n",
    "        super(TimeEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        cyclical_sin = np.sin((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        cyclical_cos = np.cos((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        continuous_sin = np.sin((x * 2 * np.pi)/self.continuous_interval)\n",
    "        continuous_cos = np.cos((x * 2 * np.pi)/self.continuous_interval)\n",
    "        \n",
    "        return keras.layers.concatenate([cyclical_sin, cyclical_cos, continuous_sin, continuous_cos], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42a910-0e7c-4bad-b21a-fc9b8a939924",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_encoding = TimeEncoding()\n",
    "x = np.arange(30)/1.0\n",
    "x = x[:, tf.newaxis]\n",
    "time_encoding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68d65a-5b39-40d0-bc3f-b234a22de38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d016b9-bb92-4212-b14f-2ff53eb074a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capDL",
   "language": "python",
   "name": "capdl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
