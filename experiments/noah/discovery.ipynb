{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d0698-a89c-4f60-8092-ad842f06b879",
   "metadata": {},
   "source": [
    "# Experiments Notebook 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8519c3-fa4d-4404-a8c7-9486a7e14861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d96a9b-5d5d-4b5d-b461-746bcc80384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-hypetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f293244-628b-4bee-a316-cf96b7b89bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import bz2\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kerashypetune import KerasGridSearch\n",
    "\n",
    "\n",
    "import preprocessing as pp\n",
    "import dsci592.model as dsci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce25521f-bad6-4705-b439-eda844e32615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183fcd9-48c1-4e6d-a197-4c39853316c7",
   "metadata": {},
   "source": [
    "### Load the golden data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c511402-262c-45a0-a9ae-148b6802fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows version\n",
    "golden_dataset_file_name = os.path.join('..', '..', 'data', 'golden', 'feeFiFoFum.pbz2')\n",
    "\n",
    "# data = bz2.BZ2File(golden_dataset_file_name,'rb')\n",
    "with bz2.BZ2File(golden_dataset_file_name,'rb') as data:\n",
    "    df = pd.read_pickle(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a67ff56-fe24-41c7-8942-56c66eb1451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>JHU_ConfirmedCases.data</th>\n",
       "      <th>NYT_ConfirmedCases.data</th>\n",
       "      <th>JHU_ConfirmedDeaths.data</th>\n",
       "      <th>NYT_ConfirmedDeaths.data</th>\n",
       "      <th>NYT_ConfirmedDeaths.missing</th>\n",
       "      <th>TotalPopulation.data</th>\n",
       "      <th>MaleAndFemale_AtLeast65_Population.data</th>\n",
       "      <th>Male_Total_Population.data</th>\n",
       "      <th>Female_Total_Population.data</th>\n",
       "      <th>MaleAndFemale_Under18_Population.data</th>\n",
       "      <th>BLS_EmployedPopulation.data</th>\n",
       "      <th>BLS_EmployedPopulation.missing</th>\n",
       "      <th>BLS_UnemployedPopulation.data</th>\n",
       "      <th>BLS_UnemployedPopulation.missing</th>\n",
       "      <th>BLS_UnemploymentRate.data</th>\n",
       "      <th>BLS_UnemploymentRate.missing</th>\n",
       "      <th>BLS_LaborForcePopulation.data</th>\n",
       "      <th>BLS_LaborForcePopulation.missing</th>\n",
       "      <th>AverageDailyTemperature.data</th>\n",
       "      <th>AverageDailyTemperature.missing</th>\n",
       "      <th>AverageDewPoint.data</th>\n",
       "      <th>AverageDewPoint.missing</th>\n",
       "      <th>AverageRelativeHumidity.data</th>\n",
       "      <th>AverageRelativeHumidity.missing</th>\n",
       "      <th>AverageSurfaceAirPressure.data</th>\n",
       "      <th>AverageSurfaceAirPressure.missing</th>\n",
       "      <th>AveragePrecipitationTotal.data</th>\n",
       "      <th>AveragePrecipitationTotal.missing</th>\n",
       "      <th>AveragePrecipitation.data</th>\n",
       "      <th>AveragePrecipitation.missing</th>\n",
       "      <th>AverageWindDirection.data</th>\n",
       "      <th>AverageWindDirection.missing</th>\n",
       "      <th>AverageWindSpeed.data</th>\n",
       "      <th>AverageWindSpeed.missing</th>\n",
       "      <th>hospitalIcuBeds</th>\n",
       "      <th>hospitalStaffedBeds</th>\n",
       "      <th>hospitalLicensedBeds</th>\n",
       "      <th>latestTotalPopulation</th>\n",
       "      <th>fips</th>\n",
       "      <th>county</th>\n",
       "      <th>jhu_daily_death</th>\n",
       "      <th>jhu_daily_cases</th>\n",
       "      <th>jhu_daily_new_cases</th>\n",
       "      <th>jhu_daily_death_rolling_7</th>\n",
       "      <th>jhu_daily_cases_rolling_7</th>\n",
       "      <th>jhu_daily_new_cases_rolling_7</th>\n",
       "      <th>jhu_daily_death_rolling_30</th>\n",
       "      <th>jhu_daily_cases_rolling_30</th>\n",
       "      <th>jhu_daily_new_cases_rolling_30</th>\n",
       "      <th>LND110210</th>\n",
       "      <th>jhu_death_rate</th>\n",
       "      <th>jhu_case_rate</th>\n",
       "      <th>jhu_new_case_rate</th>\n",
       "      <th>density</th>\n",
       "      <th>icu_beds_per_person</th>\n",
       "      <th>staffed_beds_per_person</th>\n",
       "      <th>licensed_beds_per_person</th>\n",
       "      <th>cold_days</th>\n",
       "      <th>hot_days</th>\n",
       "      <th>moderate_days</th>\n",
       "      <th>gte_65_percent</th>\n",
       "      <th>lt_18_percent</th>\n",
       "      <th>employed_percent</th>\n",
       "      <th>unemployed_percent</th>\n",
       "      <th>totalMoved</th>\n",
       "      <th>movedWithinState</th>\n",
       "      <th>movedWithoutState</th>\n",
       "      <th>movedFromAbroad</th>\n",
       "      <th>publicTrans</th>\n",
       "      <th>totalTrans</th>\n",
       "      <th>householdsTotal</th>\n",
       "      <th>houseWith65</th>\n",
       "      <th>house2+with65</th>\n",
       "      <th>houseFamily65</th>\n",
       "      <th>houseNonfam65</th>\n",
       "      <th>houseNo65</th>\n",
       "      <th>house2+No65</th>\n",
       "      <th>houseFamilyNo65</th>\n",
       "      <th>houseNonfamNo65</th>\n",
       "      <th>householdStructuresTotal</th>\n",
       "      <th>householdIncomeMedian</th>\n",
       "      <th>gini</th>\n",
       "      <th>hoursWorkedMean</th>\n",
       "      <th>unitsInStructure</th>\n",
       "      <th>healthInsTotal</th>\n",
       "      <th>healthInsNativeWith</th>\n",
       "      <th>healthInsForeignNatWith</th>\n",
       "      <th>healthInsForeignNoncitWith</th>\n",
       "      <th>healthInsForeignNatNo</th>\n",
       "      <th>healthInsForeignNoncitNo</th>\n",
       "      <th>healthInsNativeNo</th>\n",
       "      <th>countyStateName</th>\n",
       "      <th>stateFip</th>\n",
       "      <th>countyFip</th>\n",
       "      <th>pm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.659722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.590139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.326389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.732639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.541667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.537708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.520417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.770833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.395833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.642708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.458333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.197917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.548944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.705556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879584</th>\n",
       "      <td>2022-01-12</td>\n",
       "      <td>501.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.302083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.791667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>Ziebach_SouthDakota_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1961.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>Ziebach County, South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>137</td>\n",
       "      <td>4.322196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879585</th>\n",
       "      <td>2022-01-13</td>\n",
       "      <td>501.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.854167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.958333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.303125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>Ziebach_SouthDakota_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1961.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>Ziebach County, South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>137</td>\n",
       "      <td>4.322196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879586</th>\n",
       "      <td>2022-01-14</td>\n",
       "      <td>501.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.104167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.729167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.562500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.498125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>Ziebach_SouthDakota_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1961.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>Ziebach County, South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>137</td>\n",
       "      <td>4.322196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879587</th>\n",
       "      <td>2022-01-15</td>\n",
       "      <td>501.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.412500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>Ziebach_SouthDakota_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1961.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>Ziebach County, South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>137</td>\n",
       "      <td>4.322196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879588</th>\n",
       "      <td>2022-01-16</td>\n",
       "      <td>501.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.265000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>Ziebach_SouthDakota_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1961.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>Ziebach County, South Dakota</td>\n",
       "      <td>46</td>\n",
       "      <td>137</td>\n",
       "      <td>4.322196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1879589 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dates  JHU_ConfirmedCases.data  NYT_ConfirmedCases.data  \\\n",
       "0        2020-03-19                      1.0                      1.0   \n",
       "1        2020-03-20                      1.0                      1.0   \n",
       "2        2020-03-21                      1.0                      1.0   \n",
       "3        2020-03-22                      1.0                      1.0   \n",
       "4        2020-03-23                      1.0                      1.0   \n",
       "...             ...                      ...                      ...   \n",
       "1879584  2022-01-12                    501.0                    501.0   \n",
       "1879585  2022-01-13                    501.0                    501.0   \n",
       "1879586  2022-01-14                    501.0                    501.0   \n",
       "1879587  2022-01-15                    501.0                    501.0   \n",
       "1879588  2022-01-16                    501.0                    501.0   \n",
       "\n",
       "         JHU_ConfirmedDeaths.data  NYT_ConfirmedDeaths.data  \\\n",
       "0                             0.0                       0.0   \n",
       "1                             0.0                       0.0   \n",
       "2                             0.0                       0.0   \n",
       "3                             0.0                       0.0   \n",
       "4                             0.0                       0.0   \n",
       "...                           ...                       ...   \n",
       "1879584                      11.0                      11.0   \n",
       "1879585                      11.0                      11.0   \n",
       "1879586                      11.0                      11.0   \n",
       "1879587                      11.0                      11.0   \n",
       "1879588                      11.0                      11.0   \n",
       "\n",
       "         NYT_ConfirmedDeaths.missing  TotalPopulation.data  \\\n",
       "0                                  0               24527.0   \n",
       "1                                  0               24527.0   \n",
       "2                                  0               24527.0   \n",
       "3                                  0               24527.0   \n",
       "4                                  0               24527.0   \n",
       "...                              ...                   ...   \n",
       "1879584                            0                2756.0   \n",
       "1879585                            0                2756.0   \n",
       "1879586                            0                2756.0   \n",
       "1879587                            0                2756.0   \n",
       "1879588                            0                2756.0   \n",
       "\n",
       "         MaleAndFemale_AtLeast65_Population.data  Male_Total_Population.data  \\\n",
       "0                                         5343.0                     11868.0   \n",
       "1                                         5343.0                     11868.0   \n",
       "2                                         5343.0                     11868.0   \n",
       "3                                         5343.0                     11868.0   \n",
       "4                                         5343.0                     11868.0   \n",
       "...                                          ...                         ...   \n",
       "1879584                                    263.0                      1369.0   \n",
       "1879585                                    263.0                      1369.0   \n",
       "1879586                                    263.0                      1369.0   \n",
       "1879587                                    263.0                      1369.0   \n",
       "1879588                                    263.0                      1369.0   \n",
       "\n",
       "         Female_Total_Population.data  MaleAndFemale_Under18_Population.data  \\\n",
       "0                             12673.0                                 4924.0   \n",
       "1                             12673.0                                 4924.0   \n",
       "2                             12673.0                                 4924.0   \n",
       "3                             12673.0                                 4924.0   \n",
       "4                             12673.0                                 4924.0   \n",
       "...                               ...                                    ...   \n",
       "1879584                        1373.0                                  774.0   \n",
       "1879585                        1373.0                                  774.0   \n",
       "1879586                        1373.0                                  774.0   \n",
       "1879587                        1373.0                                  774.0   \n",
       "1879588                        1373.0                                  774.0   \n",
       "\n",
       "         BLS_EmployedPopulation.data  BLS_EmployedPopulation.missing  \\\n",
       "0                             9716.5                             0.0   \n",
       "1                             9716.5                             0.0   \n",
       "2                             9716.5                             0.0   \n",
       "3                             9716.5                             0.0   \n",
       "4                             9716.5                             0.0   \n",
       "...                              ...                             ...   \n",
       "1879584                        978.0                             0.0   \n",
       "1879585                        978.0                             0.0   \n",
       "1879586                        978.0                             0.0   \n",
       "1879587                        978.0                             0.0   \n",
       "1879588                        978.0                             0.0   \n",
       "\n",
       "         BLS_UnemployedPopulation.data  BLS_UnemployedPopulation.missing  \\\n",
       "0                                386.5                               0.0   \n",
       "1                                386.5                               0.0   \n",
       "2                                386.5                               0.0   \n",
       "3                                386.5                               0.0   \n",
       "4                                386.5                               0.0   \n",
       "...                                ...                               ...   \n",
       "1879584                           38.0                               0.0   \n",
       "1879585                           38.0                               0.0   \n",
       "1879586                           38.0                               0.0   \n",
       "1879587                           38.0                               0.0   \n",
       "1879588                           38.0                               0.0   \n",
       "\n",
       "         BLS_UnemploymentRate.data  BLS_UnemploymentRate.missing  \\\n",
       "0                         3.825596                           0.0   \n",
       "1                         3.825596                           0.0   \n",
       "2                         3.825596                           0.0   \n",
       "3                         3.825596                           0.0   \n",
       "4                         3.825596                           0.0   \n",
       "...                            ...                           ...   \n",
       "1879584                   3.740157                           0.0   \n",
       "1879585                   3.740157                           0.0   \n",
       "1879586                   3.740157                           0.0   \n",
       "1879587                   3.740157                           0.0   \n",
       "1879588                   3.740157                           0.0   \n",
       "\n",
       "         BLS_LaborForcePopulation.data  BLS_LaborForcePopulation.missing  \\\n",
       "0                              10103.0                               0.0   \n",
       "1                              10103.0                               0.0   \n",
       "2                              10103.0                               0.0   \n",
       "3                              10103.0                               0.0   \n",
       "4                              10103.0                               0.0   \n",
       "...                                ...                               ...   \n",
       "1879584                         1016.0                               0.0   \n",
       "1879585                         1016.0                               0.0   \n",
       "1879586                         1016.0                               0.0   \n",
       "1879587                         1016.0                               0.0   \n",
       "1879588                         1016.0                               0.0   \n",
       "\n",
       "         AverageDailyTemperature.data  AverageDailyTemperature.missing  \\\n",
       "0                           68.659722                              0.0   \n",
       "1                           72.541667                              0.0   \n",
       "2                           69.208333                              0.0   \n",
       "3                           53.916667                              0.0   \n",
       "4                           49.863889                              0.0   \n",
       "...                               ...                              ...   \n",
       "1879584                     40.666667                              0.0   \n",
       "1879585                     36.854167                              0.0   \n",
       "1879586                     24.104167                              0.0   \n",
       "1879587                     28.500000                              0.0   \n",
       "1879588                     33.708333                              0.0   \n",
       "\n",
       "         AverageDewPoint.data  AverageDewPoint.missing  \\\n",
       "0                   63.152778                      0.0   \n",
       "1                   60.583333                      0.0   \n",
       "2                   56.708333                      0.0   \n",
       "3                   49.770833                      0.0   \n",
       "4                   49.863889                      0.0   \n",
       "...                       ...                      ...   \n",
       "1879584             27.625000                      0.0   \n",
       "1879585             30.958333                      0.0   \n",
       "1879586             19.729167                      0.0   \n",
       "1879587             19.833333                      0.0   \n",
       "1879588             24.708333                      0.0   \n",
       "\n",
       "         AverageRelativeHumidity.data  AverageRelativeHumidity.missing  \\\n",
       "0                           84.152778                              0.0   \n",
       "1                           68.708333                              0.0   \n",
       "2                           66.583333                              0.0   \n",
       "3                           86.395833                              0.0   \n",
       "4                          100.000000                              0.0   \n",
       "...                               ...                              ...   \n",
       "1879584                     60.708333                              0.0   \n",
       "1879585                     79.625000                              0.0   \n",
       "1879586                     83.562500                              0.0   \n",
       "1879587                     72.000000                              0.0   \n",
       "1879588                     70.750000                              0.0   \n",
       "\n",
       "         AverageSurfaceAirPressure.data  AverageSurfaceAirPressure.missing  \\\n",
       "0                             29.590139                                0.0   \n",
       "1                             29.537708                                0.0   \n",
       "2                             29.520417                                0.0   \n",
       "3                             29.642708                                0.0   \n",
       "4                             29.548944                                0.0   \n",
       "...                                 ...                                ...   \n",
       "1879584                       27.302083                                0.0   \n",
       "1879585                       27.303125                                0.0   \n",
       "1879586                       27.498125                                0.0   \n",
       "1879587                       27.412500                                0.0   \n",
       "1879588                       27.265000                                0.0   \n",
       "\n",
       "         AveragePrecipitationTotal.data  AveragePrecipitationTotal.missing  \\\n",
       "0                              0.000833                                0.0   \n",
       "1                              0.000000                                0.0   \n",
       "2                              0.000000                                0.0   \n",
       "3                              0.000000                                0.0   \n",
       "4                              0.013750                                0.0   \n",
       "...                                 ...                                ...   \n",
       "1879584                        0.000000                                0.0   \n",
       "1879585                        0.000000                                0.0   \n",
       "1879586                        0.000000                                0.0   \n",
       "1879587                        0.000000                                0.0   \n",
       "1879588                        0.000000                                0.0   \n",
       "\n",
       "         AveragePrecipitation.data  AveragePrecipitation.missing  \\\n",
       "0                         0.000000                           0.0   \n",
       "1                         0.000000                           0.0   \n",
       "2                         0.000000                           0.0   \n",
       "3                         0.011354                           0.0   \n",
       "4                         0.008042                           0.0   \n",
       "...                            ...                           ...   \n",
       "1879584                   0.000000                           0.0   \n",
       "1879585                   0.000000                           0.0   \n",
       "1879586                   0.000000                           0.0   \n",
       "1879587                   0.000000                           0.0   \n",
       "1879588                   0.000000                           0.0   \n",
       "\n",
       "         AverageWindDirection.data  AverageWindDirection.missing  \\\n",
       "0                       192.326389                           0.0   \n",
       "1                       208.125000                           0.0   \n",
       "2                       252.916667                           0.0   \n",
       "3                        91.458333                           0.0   \n",
       "4                        83.333333                           0.0   \n",
       "...                            ...                           ...   \n",
       "1879584                 283.750000                           0.0   \n",
       "1879585                 209.166667                           0.0   \n",
       "1879586                 270.416667                           0.0   \n",
       "1879587                 208.333333                           0.0   \n",
       "1879588                 295.000000                           0.0   \n",
       "\n",
       "         AverageWindSpeed.data  AverageWindSpeed.missing  hospitalIcuBeds  \\\n",
       "0                     6.732639                       0.0              6.0   \n",
       "1                    10.833333                       0.0              6.0   \n",
       "2                     8.125000                       0.0              6.0   \n",
       "3                     7.197917                       0.0              6.0   \n",
       "4                     3.705556                       0.0              6.0   \n",
       "...                        ...                       ...              ...   \n",
       "1879584              14.791667                       0.0              1.0   \n",
       "1879585              12.291667                       0.0              1.0   \n",
       "1879586              17.875000                       0.0              1.0   \n",
       "1879587              15.416667                       0.0              1.0   \n",
       "1879588              16.333333                       0.0              1.0   \n",
       "\n",
       "         hospitalStaffedBeds  hospitalLicensedBeds  latestTotalPopulation  \\\n",
       "0                       25.0                  25.0                24527.0   \n",
       "1                       25.0                  25.0                24527.0   \n",
       "2                       25.0                  25.0                24527.0   \n",
       "3                       25.0                  25.0                24527.0   \n",
       "4                       25.0                  25.0                24527.0   \n",
       "...                      ...                   ...                    ...   \n",
       "1879584                  8.0                   8.0                 2756.0   \n",
       "1879585                  8.0                   8.0                 2756.0   \n",
       "1879586                  8.0                   8.0                 2756.0   \n",
       "1879587                  8.0                   8.0                 2756.0   \n",
       "1879588                  8.0                   8.0                 2756.0   \n",
       "\n",
       "          fips                                county  jhu_daily_death  \\\n",
       "0        45001  Abbeville_SouthCarolina_UnitedStates              0.0   \n",
       "1        45001  Abbeville_SouthCarolina_UnitedStates              0.0   \n",
       "2        45001  Abbeville_SouthCarolina_UnitedStates              0.0   \n",
       "3        45001  Abbeville_SouthCarolina_UnitedStates              0.0   \n",
       "4        45001  Abbeville_SouthCarolina_UnitedStates              0.0   \n",
       "...        ...                                   ...              ...   \n",
       "1879584  46137      Ziebach_SouthDakota_UnitedStates              0.0   \n",
       "1879585  46137      Ziebach_SouthDakota_UnitedStates              0.0   \n",
       "1879586  46137      Ziebach_SouthDakota_UnitedStates              0.0   \n",
       "1879587  46137      Ziebach_SouthDakota_UnitedStates              0.0   \n",
       "1879588  46137      Ziebach_SouthDakota_UnitedStates              0.0   \n",
       "\n",
       "         jhu_daily_cases  jhu_daily_new_cases  jhu_daily_death_rolling_7  \\\n",
       "0                    0.0                  0.0                        0.0   \n",
       "1                    0.0                  0.0                        0.0   \n",
       "2                    0.0                  0.0                        0.0   \n",
       "3                    0.0                  0.0                        0.0   \n",
       "4                    0.0                  0.0                        0.0   \n",
       "...                  ...                  ...                        ...   \n",
       "1879584              3.0                  3.0                        0.0   \n",
       "1879585              0.0                  0.0                        0.0   \n",
       "1879586              0.0                  0.0                        0.0   \n",
       "1879587              0.0                  0.0                        0.0   \n",
       "1879588              0.0                  0.0                        0.0   \n",
       "\n",
       "         jhu_daily_cases_rolling_7  jhu_daily_new_cases_rolling_7  \\\n",
       "0                         0.000000                       0.000000   \n",
       "1                         0.000000                       0.000000   \n",
       "2                         0.000000                       0.000000   \n",
       "3                         0.000000                       0.000000   \n",
       "4                         0.000000                       0.000000   \n",
       "...                            ...                            ...   \n",
       "1879584                   1.571429                       1.571429   \n",
       "1879585                   1.285714                       1.285714   \n",
       "1879586                   1.000000                       1.000000   \n",
       "1879587                   1.000000                       1.000000   \n",
       "1879588                   1.000000                       1.000000   \n",
       "\n",
       "         jhu_daily_death_rolling_30  jhu_daily_cases_rolling_30  \\\n",
       "0                          0.000000                    0.000000   \n",
       "1                          0.000000                    0.000000   \n",
       "2                          0.000000                    0.000000   \n",
       "3                          0.000000                    0.000000   \n",
       "4                          0.000000                    0.000000   \n",
       "...                             ...                         ...   \n",
       "1879584                    0.033333                    0.700000   \n",
       "1879585                    0.033333                    0.700000   \n",
       "1879586                    0.033333                    0.700000   \n",
       "1879587                    0.033333                    0.666667   \n",
       "1879588                    0.033333                    0.666667   \n",
       "\n",
       "         jhu_daily_new_cases_rolling_30  LND110210  jhu_death_rate  \\\n",
       "0                              0.000000     490.48             0.0   \n",
       "1                              0.000000     490.48             0.0   \n",
       "2                              0.000000     490.48             0.0   \n",
       "3                              0.000000     490.48             0.0   \n",
       "4                              0.000000     490.48             0.0   \n",
       "...                                 ...        ...             ...   \n",
       "1879584                        0.700000    1961.27             0.0   \n",
       "1879585                        0.700000    1961.27             0.0   \n",
       "1879586                        0.700000    1961.27             0.0   \n",
       "1879587                        0.666667    1961.27             0.0   \n",
       "1879588                        0.666667    1961.27             0.0   \n",
       "\n",
       "         jhu_case_rate  jhu_new_case_rate    density  icu_beds_per_person  \\\n",
       "0             0.000000           0.000000  50.006116             0.000245   \n",
       "1             0.000000           0.000000  50.006116             0.000245   \n",
       "2             0.000000           0.000000  50.006116             0.000245   \n",
       "3             0.000000           0.000000  50.006116             0.000245   \n",
       "4             0.000000           0.000000  50.006116             0.000245   \n",
       "...                ...                ...        ...                  ...   \n",
       "1879584       0.000570           0.000570   1.405212             0.000363   \n",
       "1879585       0.000467           0.000467   1.405212             0.000363   \n",
       "1879586       0.000363           0.000363   1.405212             0.000363   \n",
       "1879587       0.000363           0.000363   1.405212             0.000363   \n",
       "1879588       0.000363           0.000363   1.405212             0.000363   \n",
       "\n",
       "         staffed_beds_per_person  licensed_beds_per_person  cold_days  \\\n",
       "0                       0.001019                  0.001019          0   \n",
       "1                       0.001019                  0.001019          0   \n",
       "2                       0.001019                  0.001019          0   \n",
       "3                       0.001019                  0.001019          0   \n",
       "4                       0.001019                  0.001019          1   \n",
       "...                          ...                       ...        ...   \n",
       "1879584                 0.002903                  0.002903          1   \n",
       "1879585                 0.002903                  0.002903          1   \n",
       "1879586                 0.002903                  0.002903          1   \n",
       "1879587                 0.002903                  0.002903          1   \n",
       "1879588                 0.002903                  0.002903          1   \n",
       "\n",
       "         hot_days  moderate_days  gte_65_percent  lt_18_percent  \\\n",
       "0               0              1        0.217842       0.200758   \n",
       "1               0              1        0.217842       0.200758   \n",
       "2               0              1        0.217842       0.200758   \n",
       "3               0              1        0.217842       0.200758   \n",
       "4               0              0        0.217842       0.200758   \n",
       "...           ...            ...             ...            ...   \n",
       "1879584         0              0        0.095428       0.280842   \n",
       "1879585         0              0        0.095428       0.280842   \n",
       "1879586         0              0        0.095428       0.280842   \n",
       "1879587         0              0        0.095428       0.280842   \n",
       "1879588         0              0        0.095428       0.280842   \n",
       "\n",
       "         employed_percent  unemployed_percent  totalMoved  movedWithinState  \\\n",
       "0                0.961744            0.038256     24317.0            1118.0   \n",
       "1                0.961744            0.038256     24317.0            1118.0   \n",
       "2                0.961744            0.038256     24317.0            1118.0   \n",
       "3                0.961744            0.038256     24317.0            1118.0   \n",
       "4                0.961744            0.038256     24317.0            1118.0   \n",
       "...                   ...                 ...         ...               ...   \n",
       "1879584          0.962598            0.037402      2753.0              43.0   \n",
       "1879585          0.962598            0.037402      2753.0              43.0   \n",
       "1879586          0.962598            0.037402      2753.0              43.0   \n",
       "1879587          0.962598            0.037402      2753.0              43.0   \n",
       "1879588          0.962598            0.037402      2753.0              43.0   \n",
       "\n",
       "         movedWithoutState  movedFromAbroad  publicTrans  totalTrans  \\\n",
       "0                    329.0             12.0           44        9791   \n",
       "1                    329.0             12.0           44        9791   \n",
       "2                    329.0             12.0           44        9791   \n",
       "3                    329.0             12.0           44        9791   \n",
       "4                    329.0             12.0           44        9791   \n",
       "...                    ...              ...          ...         ...   \n",
       "1879584                8.0              0.0            1         951   \n",
       "1879585                8.0              0.0            1         951   \n",
       "1879586                8.0              0.0            1         951   \n",
       "1879587                8.0              0.0            1         951   \n",
       "1879588                8.0              0.0            1         951   \n",
       "\n",
       "         householdsTotal  houseWith65  house2+with65  houseFamily65  \\\n",
       "0                   9660         3645           2081           2042   \n",
       "1                   9660         3645           2081           2042   \n",
       "2                   9660         3645           2081           2042   \n",
       "3                   9660         3645           2081           2042   \n",
       "4                   9660         3645           2081           2042   \n",
       "...                  ...          ...            ...            ...   \n",
       "1879584              754          185            137            134   \n",
       "1879585              754          185            137            134   \n",
       "1879586              754          185            137            134   \n",
       "1879587              754          185            137            134   \n",
       "1879588              754          185            137            134   \n",
       "\n",
       "         houseNonfam65  houseNo65  house2+No65  houseFamilyNo65  \\\n",
       "0                   39       6015         4531             4206   \n",
       "1                   39       6015         4531             4206   \n",
       "2                   39       6015         4531             4206   \n",
       "3                   39       6015         4531             4206   \n",
       "4                   39       6015         4531             4206   \n",
       "...                ...        ...          ...              ...   \n",
       "1879584              3        569          482              474   \n",
       "1879585              3        569          482              474   \n",
       "1879586              3        569          482              474   \n",
       "1879587              3        569          482              474   \n",
       "1879588              3        569          482              474   \n",
       "\n",
       "         houseNonfamNo65  householdStructuresTotal  householdIncomeMedian  \\\n",
       "0                    325                      9660                  38741   \n",
       "1                    325                      9660                  38741   \n",
       "2                    325                      9660                  38741   \n",
       "3                    325                      9660                  38741   \n",
       "4                    325                      9660                  38741   \n",
       "...                  ...                       ...                    ...   \n",
       "1879584                8                       754                  37400   \n",
       "1879585                8                       754                  37400   \n",
       "1879586                8                       754                  37400   \n",
       "1879587                8                       754                  37400   \n",
       "1879588                8                       754                  37400   \n",
       "\n",
       "           gini  hoursWorkedMean  unitsInStructure  healthInsTotal  \\\n",
       "0        0.4790             38.1             12191           24397   \n",
       "1        0.4790             38.1             12191           24397   \n",
       "2        0.4790             38.1             12191           24397   \n",
       "3        0.4790             38.1             12191           24397   \n",
       "4        0.4790             38.1             12191           24397   \n",
       "...         ...              ...               ...             ...   \n",
       "1879584  0.4834             40.6              1009            2791   \n",
       "1879585  0.4834             40.6              1009            2791   \n",
       "1879586  0.4834             40.6              1009            2791   \n",
       "1879587  0.4834             40.6              1009            2791   \n",
       "1879588  0.4834             40.6              1009            2791   \n",
       "\n",
       "         healthInsNativeWith  healthInsForeignNatWith  \\\n",
       "0                      21415                      179   \n",
       "1                      21415                      179   \n",
       "2                      21415                      179   \n",
       "3                      21415                      179   \n",
       "4                      21415                      179   \n",
       "...                      ...                      ...   \n",
       "1879584                 2020                        4   \n",
       "1879585                 2020                        4   \n",
       "1879586                 2020                        4   \n",
       "1879587                 2020                        4   \n",
       "1879588                 2020                        4   \n",
       "\n",
       "         healthInsForeignNoncitWith  healthInsForeignNatNo  \\\n",
       "0                                87                     39   \n",
       "1                                87                     39   \n",
       "2                                87                     39   \n",
       "3                                87                     39   \n",
       "4                                87                     39   \n",
       "...                             ...                    ...   \n",
       "1879584                           0                      0   \n",
       "1879585                           0                      0   \n",
       "1879586                           0                      0   \n",
       "1879587                           0                      0   \n",
       "1879588                           0                      0   \n",
       "\n",
       "         healthInsForeignNoncitNo  healthInsNativeNo  \\\n",
       "0                             122               2555   \n",
       "1                             122               2555   \n",
       "2                             122               2555   \n",
       "3                             122               2555   \n",
       "4                             122               2555   \n",
       "...                           ...                ...   \n",
       "1879584                        16                751   \n",
       "1879585                        16                751   \n",
       "1879586                        16                751   \n",
       "1879587                        16                751   \n",
       "1879588                        16                751   \n",
       "\n",
       "                          countyStateName stateFip countyFip      pm25  \n",
       "0        Abbeville County, South Carolina       45       001  9.618551  \n",
       "1        Abbeville County, South Carolina       45       001  9.618551  \n",
       "2        Abbeville County, South Carolina       45       001  9.618551  \n",
       "3        Abbeville County, South Carolina       45       001  9.618551  \n",
       "4        Abbeville County, South Carolina       45       001  9.618551  \n",
       "...                                   ...      ...       ...       ...  \n",
       "1879584      Ziebach County, South Dakota       46       137  4.322196  \n",
       "1879585      Ziebach County, South Dakota       46       137  4.322196  \n",
       "1879586      Ziebach County, South Dakota       46       137  4.322196  \n",
       "1879587      Ziebach County, South Dakota       46       137  4.322196  \n",
       "1879588      Ziebach County, South Dakota       46       137  4.322196  \n",
       "\n",
       "[1879589 rows x 96 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linux version (problems with bz2 file)\n",
    "\n",
    "golden_dataset_file_name = os.path.join('/', 'data', 'projects', 'drexel', 'dsci592', 'feeFiFoFum.pkl')\n",
    "\n",
    "df = pd.read_pickle(golden_dataset_file_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fde0d-13e5-4198-9f8e-5689f1130301",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "#### Drop non-numeric and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e761798-4dba-4c6c-96db-04ba85fb6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['NYT_ConfirmedCases.data','NYT_ConfirmedDeaths.data','NYT_ConfirmedDeaths.missing','county','LND110210','countyStateName','stateFip','countyFip']\n",
    "\n",
    "df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf9d9-f105-4376-80ce-65a15ad0bca0",
   "metadata": {},
   "source": [
    "#### Temporarily, replace FIPS codes with latitude and longitude of the centroid of each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d613f0d-2b87-4c84-80d1-a34cb680909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = pd.read_csv('2021_Gaz_counties_national.txt', delimiter='\\t')\n",
    "counties.rename(columns={'INTPTLONG                                                                                                               ': 'longitude',\n",
    "                        'INTPTLAT': 'latitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8cce69e-9e11-4ebf-ad6a-7257ea693173",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = counties[['GEOID', 'latitude', 'longitude' ]]\n",
    "df.fips = df.fips.astype('int64')\n",
    "\n",
    "df = df.merge(counties, how='left', left_on='fips', right_on='GEOID')\n",
    "df.drop(['GEOID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324af6e-050d-4195-b5d9-4c9d51dac540",
   "metadata": {},
   "source": [
    "#### Replace dates with monotonically increasing integers starting with the minimum date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54d464b3-5904-4671-9ef8-97b2450bfc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-03-11 00:00:00'),\n",
       " Timestamp('2022-01-16 00:00:00'),\n",
       " dtype('<M8[ns]'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dates = pd.to_datetime(df.dates, format='%Y-%m-%d')\n",
    "min_date = min(df.dates)\n",
    "max_date = max(df.dates)\n",
    "min_date, max_date, df.dates.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a52c51-f7f0-4835-a57e-8ec4298e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] =(df.dates - min_date).dt.days\n",
    "df.drop(['dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0581ee-2583-4748-8a3c-e4ccadc6a3ba",
   "metadata": {},
   "source": [
    "#### Replace the integer representation of date with sin and cosine encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4517112d-13a1-4685-9f96-22e5ece969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclical_interval = 365\n",
    "continuous_interval = 3650\n",
    "df['cyclical_sin'] = np.sin((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['cyclical_cos'] = np.cos((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['continuous_sin'] = np.sin((df.day * 2 * np.pi)/continuous_interval)\n",
    "df['continuous_cos'] = np.cos((df.day * 2 * np.pi)/continuous_interval)\n",
    "df.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc592ca-b282-47f6-a26c-da7033ac9194",
   "metadata": {},
   "source": [
    "#### Get the feature column for latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5743a233-aa53-4a8d-a047-4f05805fac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 19:30:43.511875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.512123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.535784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.536014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.536211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.536425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.537036: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-15 19:30:43.659779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.659999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.660184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.660352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.660518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:43.660685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:44.156038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:44.156282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:44.156459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:44.156639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:44.156816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:44.156989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3976 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2022-02-15 19:30:44.157224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-15 19:30:44.157381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 4424 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# crossed_latlong = pp.get_latlong_fc(df)\n",
    "\n",
    "lat_buckets = list(np.linspace(df.latitude.min(), df.latitude.max(),100))\n",
    "long_buckets = list(np.linspace(df.longitude.min(), df.longitude.max(),100))\n",
    "\n",
    "#make feature columns\n",
    "lat_fc = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'),lat_buckets)\n",
    "long_fc= tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'),long_buckets)\n",
    "    \n",
    "# crossed columns tell the model how the features relate\n",
    "crossed_latlong = tf.feature_column.crossed_column(keys=[lat_fc, long_fc], hash_bucket_size=1000) # No precise rule, maybe 1000 buckets will be good?\n",
    "    \n",
    "embedded_latlong = tf.feature_column.embedding_column(crossed_latlong,9)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(embedded_latlong)\n",
    "\n",
    "df[['geo0', 'geo1', 'geo2','geo3', 'geo4','geo5','geo6','geo7','geo8']] = feature_layer({'latitude': df.latitude, 'longitude': df.longitude})\n",
    "\n",
    "# df.drop(['longitude', 'latitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961ad6ca-75b4-4be9-84d7-5deddf14bd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JHU_ConfirmedCases.data</th>\n",
       "      <th>JHU_ConfirmedDeaths.data</th>\n",
       "      <th>TotalPopulation.data</th>\n",
       "      <th>MaleAndFemale_AtLeast65_Population.data</th>\n",
       "      <th>Male_Total_Population.data</th>\n",
       "      <th>Female_Total_Population.data</th>\n",
       "      <th>MaleAndFemale_Under18_Population.data</th>\n",
       "      <th>BLS_EmployedPopulation.data</th>\n",
       "      <th>BLS_EmployedPopulation.missing</th>\n",
       "      <th>BLS_UnemployedPopulation.data</th>\n",
       "      <th>BLS_UnemployedPopulation.missing</th>\n",
       "      <th>BLS_UnemploymentRate.data</th>\n",
       "      <th>BLS_UnemploymentRate.missing</th>\n",
       "      <th>BLS_LaborForcePopulation.data</th>\n",
       "      <th>BLS_LaborForcePopulation.missing</th>\n",
       "      <th>AverageDailyTemperature.data</th>\n",
       "      <th>AverageDailyTemperature.missing</th>\n",
       "      <th>AverageDewPoint.data</th>\n",
       "      <th>AverageDewPoint.missing</th>\n",
       "      <th>AverageRelativeHumidity.data</th>\n",
       "      <th>AverageRelativeHumidity.missing</th>\n",
       "      <th>AverageSurfaceAirPressure.data</th>\n",
       "      <th>AverageSurfaceAirPressure.missing</th>\n",
       "      <th>AveragePrecipitationTotal.data</th>\n",
       "      <th>AveragePrecipitationTotal.missing</th>\n",
       "      <th>AveragePrecipitation.data</th>\n",
       "      <th>AveragePrecipitation.missing</th>\n",
       "      <th>AverageWindDirection.data</th>\n",
       "      <th>AverageWindDirection.missing</th>\n",
       "      <th>AverageWindSpeed.data</th>\n",
       "      <th>AverageWindSpeed.missing</th>\n",
       "      <th>hospitalIcuBeds</th>\n",
       "      <th>hospitalStaffedBeds</th>\n",
       "      <th>hospitalLicensedBeds</th>\n",
       "      <th>latestTotalPopulation</th>\n",
       "      <th>fips</th>\n",
       "      <th>jhu_daily_death</th>\n",
       "      <th>jhu_daily_cases</th>\n",
       "      <th>jhu_daily_new_cases</th>\n",
       "      <th>jhu_daily_death_rolling_7</th>\n",
       "      <th>jhu_daily_cases_rolling_7</th>\n",
       "      <th>jhu_daily_new_cases_rolling_7</th>\n",
       "      <th>jhu_daily_death_rolling_30</th>\n",
       "      <th>jhu_daily_cases_rolling_30</th>\n",
       "      <th>jhu_daily_new_cases_rolling_30</th>\n",
       "      <th>jhu_death_rate</th>\n",
       "      <th>jhu_case_rate</th>\n",
       "      <th>jhu_new_case_rate</th>\n",
       "      <th>density</th>\n",
       "      <th>icu_beds_per_person</th>\n",
       "      <th>staffed_beds_per_person</th>\n",
       "      <th>licensed_beds_per_person</th>\n",
       "      <th>cold_days</th>\n",
       "      <th>hot_days</th>\n",
       "      <th>moderate_days</th>\n",
       "      <th>gte_65_percent</th>\n",
       "      <th>lt_18_percent</th>\n",
       "      <th>employed_percent</th>\n",
       "      <th>unemployed_percent</th>\n",
       "      <th>totalMoved</th>\n",
       "      <th>movedWithinState</th>\n",
       "      <th>movedWithoutState</th>\n",
       "      <th>movedFromAbroad</th>\n",
       "      <th>publicTrans</th>\n",
       "      <th>totalTrans</th>\n",
       "      <th>householdsTotal</th>\n",
       "      <th>houseWith65</th>\n",
       "      <th>house2+with65</th>\n",
       "      <th>houseFamily65</th>\n",
       "      <th>houseNonfam65</th>\n",
       "      <th>houseNo65</th>\n",
       "      <th>house2+No65</th>\n",
       "      <th>houseFamilyNo65</th>\n",
       "      <th>houseNonfamNo65</th>\n",
       "      <th>householdStructuresTotal</th>\n",
       "      <th>householdIncomeMedian</th>\n",
       "      <th>gini</th>\n",
       "      <th>hoursWorkedMean</th>\n",
       "      <th>unitsInStructure</th>\n",
       "      <th>healthInsTotal</th>\n",
       "      <th>healthInsNativeWith</th>\n",
       "      <th>healthInsForeignNatWith</th>\n",
       "      <th>healthInsForeignNoncitWith</th>\n",
       "      <th>healthInsForeignNatNo</th>\n",
       "      <th>healthInsForeignNoncitNo</th>\n",
       "      <th>healthInsNativeNo</th>\n",
       "      <th>pm25</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cyclical_sin</th>\n",
       "      <th>cyclical_cos</th>\n",
       "      <th>continuous_sin</th>\n",
       "      <th>continuous_cos</th>\n",
       "      <th>geo0</th>\n",
       "      <th>geo1</th>\n",
       "      <th>geo2</th>\n",
       "      <th>geo3</th>\n",
       "      <th>geo4</th>\n",
       "      <th>geo5</th>\n",
       "      <th>geo6</th>\n",
       "      <th>geo7</th>\n",
       "      <th>geo8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.659722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.590139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.326389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.732639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.137279</td>\n",
       "      <td>0.990532</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>0.228193</td>\n",
       "      <td>0.295447</td>\n",
       "      <td>-0.247483</td>\n",
       "      <td>0.157952</td>\n",
       "      <td>0.272770</td>\n",
       "      <td>-0.168312</td>\n",
       "      <td>-0.329012</td>\n",
       "      <td>0.395681</td>\n",
       "      <td>0.359578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.541667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.537708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.154309</td>\n",
       "      <td>0.988023</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>0.228193</td>\n",
       "      <td>0.295447</td>\n",
       "      <td>-0.247483</td>\n",
       "      <td>0.157952</td>\n",
       "      <td>0.272770</td>\n",
       "      <td>-0.168312</td>\n",
       "      <td>-0.329012</td>\n",
       "      <td>0.395681</td>\n",
       "      <td>0.359578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.520417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.171293</td>\n",
       "      <td>0.985220</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.228193</td>\n",
       "      <td>0.295447</td>\n",
       "      <td>-0.247483</td>\n",
       "      <td>0.157952</td>\n",
       "      <td>0.272770</td>\n",
       "      <td>-0.168312</td>\n",
       "      <td>-0.329012</td>\n",
       "      <td>0.395681</td>\n",
       "      <td>0.359578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.770833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.395833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.642708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.458333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.197917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.982126</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>0.228193</td>\n",
       "      <td>0.295447</td>\n",
       "      <td>-0.247483</td>\n",
       "      <td>0.157952</td>\n",
       "      <td>0.272770</td>\n",
       "      <td>-0.168312</td>\n",
       "      <td>-0.329012</td>\n",
       "      <td>0.395681</td>\n",
       "      <td>0.359578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.548944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.705556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.205104</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>0.228193</td>\n",
       "      <td>0.295447</td>\n",
       "      <td>-0.247483</td>\n",
       "      <td>0.157952</td>\n",
       "      <td>0.272770</td>\n",
       "      <td>-0.168312</td>\n",
       "      <td>-0.329012</td>\n",
       "      <td>0.395681</td>\n",
       "      <td>0.359578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879584</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.302083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.791667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>0.541628</td>\n",
       "      <td>0.915518</td>\n",
       "      <td>0.402276</td>\n",
       "      <td>0.210199</td>\n",
       "      <td>-0.273700</td>\n",
       "      <td>-0.203158</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>-0.084981</td>\n",
       "      <td>-0.199701</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>-0.227182</td>\n",
       "      <td>-0.050565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879585</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.854167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.958333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.303125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.831171</td>\n",
       "      <td>0.556017</td>\n",
       "      <td>0.916210</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>0.210199</td>\n",
       "      <td>-0.273700</td>\n",
       "      <td>-0.203158</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>-0.084981</td>\n",
       "      <td>-0.199701</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>-0.227182</td>\n",
       "      <td>-0.050565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879586</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.104167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.729167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.562500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.498125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.821477</td>\n",
       "      <td>0.570242</td>\n",
       "      <td>0.916898</td>\n",
       "      <td>0.399122</td>\n",
       "      <td>0.210199</td>\n",
       "      <td>-0.273700</td>\n",
       "      <td>-0.203158</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>-0.084981</td>\n",
       "      <td>-0.199701</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>-0.227182</td>\n",
       "      <td>-0.050565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879587</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.412500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.811539</td>\n",
       "      <td>0.584298</td>\n",
       "      <td>0.917584</td>\n",
       "      <td>0.397543</td>\n",
       "      <td>0.210199</td>\n",
       "      <td>-0.273700</td>\n",
       "      <td>-0.203158</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>-0.084981</td>\n",
       "      <td>-0.199701</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>-0.227182</td>\n",
       "      <td>-0.050565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879588</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.265000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.801361</td>\n",
       "      <td>0.598181</td>\n",
       "      <td>0.918267</td>\n",
       "      <td>0.395963</td>\n",
       "      <td>0.210199</td>\n",
       "      <td>-0.273700</td>\n",
       "      <td>-0.203158</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>-0.084981</td>\n",
       "      <td>-0.199701</td>\n",
       "      <td>0.187983</td>\n",
       "      <td>-0.227182</td>\n",
       "      <td>-0.050565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1879589 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         JHU_ConfirmedCases.data  JHU_ConfirmedDeaths.data  \\\n",
       "0                            1.0                       0.0   \n",
       "1                            1.0                       0.0   \n",
       "2                            1.0                       0.0   \n",
       "3                            1.0                       0.0   \n",
       "4                            1.0                       0.0   \n",
       "...                          ...                       ...   \n",
       "1879584                    501.0                      11.0   \n",
       "1879585                    501.0                      11.0   \n",
       "1879586                    501.0                      11.0   \n",
       "1879587                    501.0                      11.0   \n",
       "1879588                    501.0                      11.0   \n",
       "\n",
       "         TotalPopulation.data  MaleAndFemale_AtLeast65_Population.data  \\\n",
       "0                     24527.0                                   5343.0   \n",
       "1                     24527.0                                   5343.0   \n",
       "2                     24527.0                                   5343.0   \n",
       "3                     24527.0                                   5343.0   \n",
       "4                     24527.0                                   5343.0   \n",
       "...                       ...                                      ...   \n",
       "1879584                2756.0                                    263.0   \n",
       "1879585                2756.0                                    263.0   \n",
       "1879586                2756.0                                    263.0   \n",
       "1879587                2756.0                                    263.0   \n",
       "1879588                2756.0                                    263.0   \n",
       "\n",
       "         Male_Total_Population.data  Female_Total_Population.data  \\\n",
       "0                           11868.0                       12673.0   \n",
       "1                           11868.0                       12673.0   \n",
       "2                           11868.0                       12673.0   \n",
       "3                           11868.0                       12673.0   \n",
       "4                           11868.0                       12673.0   \n",
       "...                             ...                           ...   \n",
       "1879584                      1369.0                        1373.0   \n",
       "1879585                      1369.0                        1373.0   \n",
       "1879586                      1369.0                        1373.0   \n",
       "1879587                      1369.0                        1373.0   \n",
       "1879588                      1369.0                        1373.0   \n",
       "\n",
       "         MaleAndFemale_Under18_Population.data  BLS_EmployedPopulation.data  \\\n",
       "0                                       4924.0                       9716.5   \n",
       "1                                       4924.0                       9716.5   \n",
       "2                                       4924.0                       9716.5   \n",
       "3                                       4924.0                       9716.5   \n",
       "4                                       4924.0                       9716.5   \n",
       "...                                        ...                          ...   \n",
       "1879584                                  774.0                        978.0   \n",
       "1879585                                  774.0                        978.0   \n",
       "1879586                                  774.0                        978.0   \n",
       "1879587                                  774.0                        978.0   \n",
       "1879588                                  774.0                        978.0   \n",
       "\n",
       "         BLS_EmployedPopulation.missing  BLS_UnemployedPopulation.data  \\\n",
       "0                                   0.0                          386.5   \n",
       "1                                   0.0                          386.5   \n",
       "2                                   0.0                          386.5   \n",
       "3                                   0.0                          386.5   \n",
       "4                                   0.0                          386.5   \n",
       "...                                 ...                            ...   \n",
       "1879584                             0.0                           38.0   \n",
       "1879585                             0.0                           38.0   \n",
       "1879586                             0.0                           38.0   \n",
       "1879587                             0.0                           38.0   \n",
       "1879588                             0.0                           38.0   \n",
       "\n",
       "         BLS_UnemployedPopulation.missing  BLS_UnemploymentRate.data  \\\n",
       "0                                     0.0                   3.825596   \n",
       "1                                     0.0                   3.825596   \n",
       "2                                     0.0                   3.825596   \n",
       "3                                     0.0                   3.825596   \n",
       "4                                     0.0                   3.825596   \n",
       "...                                   ...                        ...   \n",
       "1879584                               0.0                   3.740157   \n",
       "1879585                               0.0                   3.740157   \n",
       "1879586                               0.0                   3.740157   \n",
       "1879587                               0.0                   3.740157   \n",
       "1879588                               0.0                   3.740157   \n",
       "\n",
       "         BLS_UnemploymentRate.missing  BLS_LaborForcePopulation.data  \\\n",
       "0                                 0.0                        10103.0   \n",
       "1                                 0.0                        10103.0   \n",
       "2                                 0.0                        10103.0   \n",
       "3                                 0.0                        10103.0   \n",
       "4                                 0.0                        10103.0   \n",
       "...                               ...                            ...   \n",
       "1879584                           0.0                         1016.0   \n",
       "1879585                           0.0                         1016.0   \n",
       "1879586                           0.0                         1016.0   \n",
       "1879587                           0.0                         1016.0   \n",
       "1879588                           0.0                         1016.0   \n",
       "\n",
       "         BLS_LaborForcePopulation.missing  AverageDailyTemperature.data  \\\n",
       "0                                     0.0                     68.659722   \n",
       "1                                     0.0                     72.541667   \n",
       "2                                     0.0                     69.208333   \n",
       "3                                     0.0                     53.916667   \n",
       "4                                     0.0                     49.863889   \n",
       "...                                   ...                           ...   \n",
       "1879584                               0.0                     40.666667   \n",
       "1879585                               0.0                     36.854167   \n",
       "1879586                               0.0                     24.104167   \n",
       "1879587                               0.0                     28.500000   \n",
       "1879588                               0.0                     33.708333   \n",
       "\n",
       "         AverageDailyTemperature.missing  AverageDewPoint.data  \\\n",
       "0                                    0.0             63.152778   \n",
       "1                                    0.0             60.583333   \n",
       "2                                    0.0             56.708333   \n",
       "3                                    0.0             49.770833   \n",
       "4                                    0.0             49.863889   \n",
       "...                                  ...                   ...   \n",
       "1879584                              0.0             27.625000   \n",
       "1879585                              0.0             30.958333   \n",
       "1879586                              0.0             19.729167   \n",
       "1879587                              0.0             19.833333   \n",
       "1879588                              0.0             24.708333   \n",
       "\n",
       "         AverageDewPoint.missing  AverageRelativeHumidity.data  \\\n",
       "0                            0.0                     84.152778   \n",
       "1                            0.0                     68.708333   \n",
       "2                            0.0                     66.583333   \n",
       "3                            0.0                     86.395833   \n",
       "4                            0.0                    100.000000   \n",
       "...                          ...                           ...   \n",
       "1879584                      0.0                     60.708333   \n",
       "1879585                      0.0                     79.625000   \n",
       "1879586                      0.0                     83.562500   \n",
       "1879587                      0.0                     72.000000   \n",
       "1879588                      0.0                     70.750000   \n",
       "\n",
       "         AverageRelativeHumidity.missing  AverageSurfaceAirPressure.data  \\\n",
       "0                                    0.0                       29.590139   \n",
       "1                                    0.0                       29.537708   \n",
       "2                                    0.0                       29.520417   \n",
       "3                                    0.0                       29.642708   \n",
       "4                                    0.0                       29.548944   \n",
       "...                                  ...                             ...   \n",
       "1879584                              0.0                       27.302083   \n",
       "1879585                              0.0                       27.303125   \n",
       "1879586                              0.0                       27.498125   \n",
       "1879587                              0.0                       27.412500   \n",
       "1879588                              0.0                       27.265000   \n",
       "\n",
       "         AverageSurfaceAirPressure.missing  AveragePrecipitationTotal.data  \\\n",
       "0                                      0.0                        0.000833   \n",
       "1                                      0.0                        0.000000   \n",
       "2                                      0.0                        0.000000   \n",
       "3                                      0.0                        0.000000   \n",
       "4                                      0.0                        0.013750   \n",
       "...                                    ...                             ...   \n",
       "1879584                                0.0                        0.000000   \n",
       "1879585                                0.0                        0.000000   \n",
       "1879586                                0.0                        0.000000   \n",
       "1879587                                0.0                        0.000000   \n",
       "1879588                                0.0                        0.000000   \n",
       "\n",
       "         AveragePrecipitationTotal.missing  AveragePrecipitation.data  \\\n",
       "0                                      0.0                   0.000000   \n",
       "1                                      0.0                   0.000000   \n",
       "2                                      0.0                   0.000000   \n",
       "3                                      0.0                   0.011354   \n",
       "4                                      0.0                   0.008042   \n",
       "...                                    ...                        ...   \n",
       "1879584                                0.0                   0.000000   \n",
       "1879585                                0.0                   0.000000   \n",
       "1879586                                0.0                   0.000000   \n",
       "1879587                                0.0                   0.000000   \n",
       "1879588                                0.0                   0.000000   \n",
       "\n",
       "         AveragePrecipitation.missing  AverageWindDirection.data  \\\n",
       "0                                 0.0                 192.326389   \n",
       "1                                 0.0                 208.125000   \n",
       "2                                 0.0                 252.916667   \n",
       "3                                 0.0                  91.458333   \n",
       "4                                 0.0                  83.333333   \n",
       "...                               ...                        ...   \n",
       "1879584                           0.0                 283.750000   \n",
       "1879585                           0.0                 209.166667   \n",
       "1879586                           0.0                 270.416667   \n",
       "1879587                           0.0                 208.333333   \n",
       "1879588                           0.0                 295.000000   \n",
       "\n",
       "         AverageWindDirection.missing  AverageWindSpeed.data  \\\n",
       "0                                 0.0               6.732639   \n",
       "1                                 0.0              10.833333   \n",
       "2                                 0.0               8.125000   \n",
       "3                                 0.0               7.197917   \n",
       "4                                 0.0               3.705556   \n",
       "...                               ...                    ...   \n",
       "1879584                           0.0              14.791667   \n",
       "1879585                           0.0              12.291667   \n",
       "1879586                           0.0              17.875000   \n",
       "1879587                           0.0              15.416667   \n",
       "1879588                           0.0              16.333333   \n",
       "\n",
       "         AverageWindSpeed.missing  hospitalIcuBeds  hospitalStaffedBeds  \\\n",
       "0                             0.0              6.0                 25.0   \n",
       "1                             0.0              6.0                 25.0   \n",
       "2                             0.0              6.0                 25.0   \n",
       "3                             0.0              6.0                 25.0   \n",
       "4                             0.0              6.0                 25.0   \n",
       "...                           ...              ...                  ...   \n",
       "1879584                       0.0              1.0                  8.0   \n",
       "1879585                       0.0              1.0                  8.0   \n",
       "1879586                       0.0              1.0                  8.0   \n",
       "1879587                       0.0              1.0                  8.0   \n",
       "1879588                       0.0              1.0                  8.0   \n",
       "\n",
       "         hospitalLicensedBeds  latestTotalPopulation   fips  jhu_daily_death  \\\n",
       "0                        25.0                24527.0  45001              0.0   \n",
       "1                        25.0                24527.0  45001              0.0   \n",
       "2                        25.0                24527.0  45001              0.0   \n",
       "3                        25.0                24527.0  45001              0.0   \n",
       "4                        25.0                24527.0  45001              0.0   \n",
       "...                       ...                    ...    ...              ...   \n",
       "1879584                   8.0                 2756.0  46137              0.0   \n",
       "1879585                   8.0                 2756.0  46137              0.0   \n",
       "1879586                   8.0                 2756.0  46137              0.0   \n",
       "1879587                   8.0                 2756.0  46137              0.0   \n",
       "1879588                   8.0                 2756.0  46137              0.0   \n",
       "\n",
       "         jhu_daily_cases  jhu_daily_new_cases  jhu_daily_death_rolling_7  \\\n",
       "0                    0.0                  0.0                        0.0   \n",
       "1                    0.0                  0.0                        0.0   \n",
       "2                    0.0                  0.0                        0.0   \n",
       "3                    0.0                  0.0                        0.0   \n",
       "4                    0.0                  0.0                        0.0   \n",
       "...                  ...                  ...                        ...   \n",
       "1879584              3.0                  3.0                        0.0   \n",
       "1879585              0.0                  0.0                        0.0   \n",
       "1879586              0.0                  0.0                        0.0   \n",
       "1879587              0.0                  0.0                        0.0   \n",
       "1879588              0.0                  0.0                        0.0   \n",
       "\n",
       "         jhu_daily_cases_rolling_7  jhu_daily_new_cases_rolling_7  \\\n",
       "0                         0.000000                       0.000000   \n",
       "1                         0.000000                       0.000000   \n",
       "2                         0.000000                       0.000000   \n",
       "3                         0.000000                       0.000000   \n",
       "4                         0.000000                       0.000000   \n",
       "...                            ...                            ...   \n",
       "1879584                   1.571429                       1.571429   \n",
       "1879585                   1.285714                       1.285714   \n",
       "1879586                   1.000000                       1.000000   \n",
       "1879587                   1.000000                       1.000000   \n",
       "1879588                   1.000000                       1.000000   \n",
       "\n",
       "         jhu_daily_death_rolling_30  jhu_daily_cases_rolling_30  \\\n",
       "0                          0.000000                    0.000000   \n",
       "1                          0.000000                    0.000000   \n",
       "2                          0.000000                    0.000000   \n",
       "3                          0.000000                    0.000000   \n",
       "4                          0.000000                    0.000000   \n",
       "...                             ...                         ...   \n",
       "1879584                    0.033333                    0.700000   \n",
       "1879585                    0.033333                    0.700000   \n",
       "1879586                    0.033333                    0.700000   \n",
       "1879587                    0.033333                    0.666667   \n",
       "1879588                    0.033333                    0.666667   \n",
       "\n",
       "         jhu_daily_new_cases_rolling_30  jhu_death_rate  jhu_case_rate  \\\n",
       "0                              0.000000             0.0       0.000000   \n",
       "1                              0.000000             0.0       0.000000   \n",
       "2                              0.000000             0.0       0.000000   \n",
       "3                              0.000000             0.0       0.000000   \n",
       "4                              0.000000             0.0       0.000000   \n",
       "...                                 ...             ...            ...   \n",
       "1879584                        0.700000             0.0       0.000570   \n",
       "1879585                        0.700000             0.0       0.000467   \n",
       "1879586                        0.700000             0.0       0.000363   \n",
       "1879587                        0.666667             0.0       0.000363   \n",
       "1879588                        0.666667             0.0       0.000363   \n",
       "\n",
       "         jhu_new_case_rate    density  icu_beds_per_person  \\\n",
       "0                 0.000000  50.006116             0.000245   \n",
       "1                 0.000000  50.006116             0.000245   \n",
       "2                 0.000000  50.006116             0.000245   \n",
       "3                 0.000000  50.006116             0.000245   \n",
       "4                 0.000000  50.006116             0.000245   \n",
       "...                    ...        ...                  ...   \n",
       "1879584           0.000570   1.405212             0.000363   \n",
       "1879585           0.000467   1.405212             0.000363   \n",
       "1879586           0.000363   1.405212             0.000363   \n",
       "1879587           0.000363   1.405212             0.000363   \n",
       "1879588           0.000363   1.405212             0.000363   \n",
       "\n",
       "         staffed_beds_per_person  licensed_beds_per_person  cold_days  \\\n",
       "0                       0.001019                  0.001019          0   \n",
       "1                       0.001019                  0.001019          0   \n",
       "2                       0.001019                  0.001019          0   \n",
       "3                       0.001019                  0.001019          0   \n",
       "4                       0.001019                  0.001019          1   \n",
       "...                          ...                       ...        ...   \n",
       "1879584                 0.002903                  0.002903          1   \n",
       "1879585                 0.002903                  0.002903          1   \n",
       "1879586                 0.002903                  0.002903          1   \n",
       "1879587                 0.002903                  0.002903          1   \n",
       "1879588                 0.002903                  0.002903          1   \n",
       "\n",
       "         hot_days  moderate_days  gte_65_percent  lt_18_percent  \\\n",
       "0               0              1        0.217842       0.200758   \n",
       "1               0              1        0.217842       0.200758   \n",
       "2               0              1        0.217842       0.200758   \n",
       "3               0              1        0.217842       0.200758   \n",
       "4               0              0        0.217842       0.200758   \n",
       "...           ...            ...             ...            ...   \n",
       "1879584         0              0        0.095428       0.280842   \n",
       "1879585         0              0        0.095428       0.280842   \n",
       "1879586         0              0        0.095428       0.280842   \n",
       "1879587         0              0        0.095428       0.280842   \n",
       "1879588         0              0        0.095428       0.280842   \n",
       "\n",
       "         employed_percent  unemployed_percent  totalMoved  movedWithinState  \\\n",
       "0                0.961744            0.038256     24317.0            1118.0   \n",
       "1                0.961744            0.038256     24317.0            1118.0   \n",
       "2                0.961744            0.038256     24317.0            1118.0   \n",
       "3                0.961744            0.038256     24317.0            1118.0   \n",
       "4                0.961744            0.038256     24317.0            1118.0   \n",
       "...                   ...                 ...         ...               ...   \n",
       "1879584          0.962598            0.037402      2753.0              43.0   \n",
       "1879585          0.962598            0.037402      2753.0              43.0   \n",
       "1879586          0.962598            0.037402      2753.0              43.0   \n",
       "1879587          0.962598            0.037402      2753.0              43.0   \n",
       "1879588          0.962598            0.037402      2753.0              43.0   \n",
       "\n",
       "         movedWithoutState  movedFromAbroad  publicTrans  totalTrans  \\\n",
       "0                    329.0             12.0           44        9791   \n",
       "1                    329.0             12.0           44        9791   \n",
       "2                    329.0             12.0           44        9791   \n",
       "3                    329.0             12.0           44        9791   \n",
       "4                    329.0             12.0           44        9791   \n",
       "...                    ...              ...          ...         ...   \n",
       "1879584                8.0              0.0            1         951   \n",
       "1879585                8.0              0.0            1         951   \n",
       "1879586                8.0              0.0            1         951   \n",
       "1879587                8.0              0.0            1         951   \n",
       "1879588                8.0              0.0            1         951   \n",
       "\n",
       "         householdsTotal  houseWith65  house2+with65  houseFamily65  \\\n",
       "0                   9660         3645           2081           2042   \n",
       "1                   9660         3645           2081           2042   \n",
       "2                   9660         3645           2081           2042   \n",
       "3                   9660         3645           2081           2042   \n",
       "4                   9660         3645           2081           2042   \n",
       "...                  ...          ...            ...            ...   \n",
       "1879584              754          185            137            134   \n",
       "1879585              754          185            137            134   \n",
       "1879586              754          185            137            134   \n",
       "1879587              754          185            137            134   \n",
       "1879588              754          185            137            134   \n",
       "\n",
       "         houseNonfam65  houseNo65  house2+No65  houseFamilyNo65  \\\n",
       "0                   39       6015         4531             4206   \n",
       "1                   39       6015         4531             4206   \n",
       "2                   39       6015         4531             4206   \n",
       "3                   39       6015         4531             4206   \n",
       "4                   39       6015         4531             4206   \n",
       "...                ...        ...          ...              ...   \n",
       "1879584              3        569          482              474   \n",
       "1879585              3        569          482              474   \n",
       "1879586              3        569          482              474   \n",
       "1879587              3        569          482              474   \n",
       "1879588              3        569          482              474   \n",
       "\n",
       "         houseNonfamNo65  householdStructuresTotal  householdIncomeMedian  \\\n",
       "0                    325                      9660                  38741   \n",
       "1                    325                      9660                  38741   \n",
       "2                    325                      9660                  38741   \n",
       "3                    325                      9660                  38741   \n",
       "4                    325                      9660                  38741   \n",
       "...                  ...                       ...                    ...   \n",
       "1879584                8                       754                  37400   \n",
       "1879585                8                       754                  37400   \n",
       "1879586                8                       754                  37400   \n",
       "1879587                8                       754                  37400   \n",
       "1879588                8                       754                  37400   \n",
       "\n",
       "           gini  hoursWorkedMean  unitsInStructure  healthInsTotal  \\\n",
       "0        0.4790             38.1             12191           24397   \n",
       "1        0.4790             38.1             12191           24397   \n",
       "2        0.4790             38.1             12191           24397   \n",
       "3        0.4790             38.1             12191           24397   \n",
       "4        0.4790             38.1             12191           24397   \n",
       "...         ...              ...               ...             ...   \n",
       "1879584  0.4834             40.6              1009            2791   \n",
       "1879585  0.4834             40.6              1009            2791   \n",
       "1879586  0.4834             40.6              1009            2791   \n",
       "1879587  0.4834             40.6              1009            2791   \n",
       "1879588  0.4834             40.6              1009            2791   \n",
       "\n",
       "         healthInsNativeWith  healthInsForeignNatWith  \\\n",
       "0                      21415                      179   \n",
       "1                      21415                      179   \n",
       "2                      21415                      179   \n",
       "3                      21415                      179   \n",
       "4                      21415                      179   \n",
       "...                      ...                      ...   \n",
       "1879584                 2020                        4   \n",
       "1879585                 2020                        4   \n",
       "1879586                 2020                        4   \n",
       "1879587                 2020                        4   \n",
       "1879588                 2020                        4   \n",
       "\n",
       "         healthInsForeignNoncitWith  healthInsForeignNatNo  \\\n",
       "0                                87                     39   \n",
       "1                                87                     39   \n",
       "2                                87                     39   \n",
       "3                                87                     39   \n",
       "4                                87                     39   \n",
       "...                             ...                    ...   \n",
       "1879584                           0                      0   \n",
       "1879585                           0                      0   \n",
       "1879586                           0                      0   \n",
       "1879587                           0                      0   \n",
       "1879588                           0                      0   \n",
       "\n",
       "         healthInsForeignNoncitNo  healthInsNativeNo      pm25   latitude  \\\n",
       "0                             122               2555  9.618551  34.229041   \n",
       "1                             122               2555  9.618551  34.229041   \n",
       "2                             122               2555  9.618551  34.229041   \n",
       "3                             122               2555  9.618551  34.229041   \n",
       "4                             122               2555  9.618551  34.229041   \n",
       "...                           ...                ...       ...        ...   \n",
       "1879584                        16                751  4.322196  44.989764   \n",
       "1879585                        16                751  4.322196  44.989764   \n",
       "1879586                        16                751  4.322196  44.989764   \n",
       "1879587                        16                751  4.322196  44.989764   \n",
       "1879588                        16                751  4.322196  44.989764   \n",
       "\n",
       "          longitude  cyclical_sin  cyclical_cos  continuous_sin  \\\n",
       "0        -82.454058      0.137279      0.990532        0.013771   \n",
       "1        -82.454058      0.154309      0.988023        0.015492   \n",
       "2        -82.454058      0.171293      0.985220        0.017213   \n",
       "3        -82.454058      0.188227      0.982126        0.018934   \n",
       "4        -82.454058      0.205104      0.978740        0.020656   \n",
       "...             ...           ...           ...             ...   \n",
       "1879584 -101.660827     -0.840618      0.541628        0.915518   \n",
       "1879585 -101.660827     -0.831171      0.556017        0.916210   \n",
       "1879586 -101.660827     -0.821477      0.570242        0.916898   \n",
       "1879587 -101.660827     -0.811539      0.584298        0.917584   \n",
       "1879588 -101.660827     -0.801361      0.598181        0.918267   \n",
       "\n",
       "         continuous_cos      geo0      geo1      geo2      geo3      geo4  \\\n",
       "0              0.999905  0.228193  0.295447 -0.247483  0.157952  0.272770   \n",
       "1              0.999880  0.228193  0.295447 -0.247483  0.157952  0.272770   \n",
       "2              0.999852  0.228193  0.295447 -0.247483  0.157952  0.272770   \n",
       "3              0.999821  0.228193  0.295447 -0.247483  0.157952  0.272770   \n",
       "4              0.999787  0.228193  0.295447 -0.247483  0.157952  0.272770   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "1879584        0.402276  0.210199 -0.273700 -0.203158  0.275165 -0.084981   \n",
       "1879585        0.400700  0.210199 -0.273700 -0.203158  0.275165 -0.084981   \n",
       "1879586        0.399122  0.210199 -0.273700 -0.203158  0.275165 -0.084981   \n",
       "1879587        0.397543  0.210199 -0.273700 -0.203158  0.275165 -0.084981   \n",
       "1879588        0.395963  0.210199 -0.273700 -0.203158  0.275165 -0.084981   \n",
       "\n",
       "             geo5      geo6      geo7      geo8  \n",
       "0       -0.168312 -0.329012  0.395681  0.359578  \n",
       "1       -0.168312 -0.329012  0.395681  0.359578  \n",
       "2       -0.168312 -0.329012  0.395681  0.359578  \n",
       "3       -0.168312 -0.329012  0.395681  0.359578  \n",
       "4       -0.168312 -0.329012  0.395681  0.359578  \n",
       "...           ...       ...       ...       ...  \n",
       "1879584 -0.199701  0.187983 -0.227182 -0.050565  \n",
       "1879585 -0.199701  0.187983 -0.227182 -0.050565  \n",
       "1879586 -0.199701  0.187983 -0.227182 -0.050565  \n",
       "1879587 -0.199701  0.187983 -0.227182 -0.050565  \n",
       "1879588 -0.199701  0.187983 -0.227182 -0.050565  \n",
       "\n",
       "[1879589 rows x 102 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f842b-c049-404a-bb19-55f26e8efa25",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513da66f-15f7-4907-bf2d-f136a335f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = [\n",
    "       'TotalPopulation.data', 'MaleAndFemale_AtLeast65_Population.data',\n",
    "       'Male_Total_Population.data', 'Female_Total_Population.data',\n",
    "       'MaleAndFemale_Under18_Population.data', 'BLS_EmployedPopulation.data',\n",
    "       'BLS_EmployedPopulation.missing', 'BLS_UnemployedPopulation.data',\n",
    "       'BLS_UnemployedPopulation.missing', 'BLS_UnemploymentRate.data',\n",
    "       'BLS_UnemploymentRate.missing', 'BLS_LaborForcePopulation.data',\n",
    "       'BLS_LaborForcePopulation.missing', 'AverageDailyTemperature.data',\n",
    "       'AverageDailyTemperature.missing', 'AverageDewPoint.data',\n",
    "       'AverageDewPoint.missing', 'AverageRelativeHumidity.data',\n",
    "       'AverageRelativeHumidity.missing', 'AverageSurfaceAirPressure.data',\n",
    "       'AverageSurfaceAirPressure.missing', 'AveragePrecipitationTotal.data',\n",
    "       'AveragePrecipitationTotal.missing', 'AveragePrecipitation.data',\n",
    "       'AveragePrecipitation.missing', 'AverageWindDirection.data',\n",
    "       'AverageWindDirection.missing', 'AverageWindSpeed.data',\n",
    "       'AverageWindSpeed.missing', 'hospitalIcuBeds', 'hospitalStaffedBeds',\n",
    "       'hospitalLicensedBeds', 'latestTotalPopulation', 'jhu_daily_death',\n",
    "       'jhu_daily_cases', 'jhu_daily_new_cases', \n",
    "    'jhu_daily_death_rolling_7',\n",
    "       'jhu_daily_cases_rolling_7', 'jhu_daily_new_cases_rolling_7',\n",
    "       'jhu_daily_death_rolling_30', 'jhu_daily_cases_rolling_30',\n",
    "       'jhu_daily_new_cases_rolling_30', 'jhu_death_rate', 'jhu_case_rate',\n",
    "       'jhu_new_case_rate', 'density', 'icu_beds_per_person',\n",
    "       'staffed_beds_per_person', 'licensed_beds_per_person', 'cold_days',\n",
    "       'hot_days', 'moderate_days', 'gte_65_percent', 'lt_18_percent',\n",
    "       'employed_percent', 'unemployed_percent', 'totalMoved',\n",
    "       'movedWithinState', 'movedWithoutState', 'movedFromAbroad',\n",
    "       'publicTrans', 'totalTrans', 'householdsTotal', 'houseWith65',\n",
    "       'house2+with65', 'houseFamily65', 'houseNonfam65', 'houseNo65',\n",
    "       'house2+No65', 'houseFamilyNo65', 'houseNonfamNo65',\n",
    "       'householdStructuresTotal', 'householdIncomeMedian', 'gini',\n",
    "       'hoursWorkedMean', 'unitsInStructure', 'healthInsTotal',\n",
    "       'healthInsNativeWith', 'healthInsForeignNatWith',\n",
    "       'healthInsForeignNoncitWith', 'healthInsForeignNatNo',\n",
    "       'healthInsForeignNoncitNo', 'healthInsNativeNo', 'pm25', 'latitude',\n",
    "       'longitude']\n",
    "cols_raw = ['fips','JHU_ConfirmedCases.data', 'JHU_ConfirmedDeaths.data', 'cyclical_sin', 'cyclical_cos', 'continuous_sin',\n",
    "       'continuous_cos', 'geo0', 'geo1', 'geo2','geo3', 'geo4','geo5','geo6','geo7','geo8']\n",
    "df_normalized = df[cols_to_normalize]\n",
    "df_normalized = (df_normalized - df_normalized.mean())/df_normalized.std()\n",
    "df_raw = df[cols_raw]\n",
    "df = pd.concat([df_raw, df_normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bf3ef-a6d5-4f22-b336-f46f2798a363",
   "metadata": {},
   "source": [
    "#### Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7731216-cef5-4d0a-a9d2-51dfbe00172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_history = 30\n",
    "days_to_predict = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b3c1f6-ec25-4420-8f70-b8d7f0cef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = df.fips.unique()\n",
    "\n",
    "# def x_generator(data, days_of_history=30, days_to_predict=1):\n",
    "#     for j, fip in enumerate(fips):\n",
    "#         if not j % 100: print(j, end=' ')\n",
    "#         county = data[data.fips == fip]\n",
    "#         for i in range(days_of_history, len(county) - days_to_predict):\n",
    "#             data_matrix = data.iloc[i - days_of_history: i, 1:].to_numpy()\n",
    "#             yield data_matrix\n",
    "            \n",
    "# def y_generator(data, days_of_history=30, days_to_predict=1):\n",
    "#     for fip in fips:\n",
    "#         county = data[data.fips == fip]\n",
    "#         for i in range(days_of_history, len(county) - days_to_predict):\n",
    "#             data_matrix = data.iloc[i: i + days_to_predict, 1:3].to_numpy()\n",
    "#             yield data_matrix\n",
    "    \n",
    "def xy_generator(data, days=31):\n",
    "    for j, fip in enumerate(fips):\n",
    "        if not j % 100: print(j, end=' ')\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days, len(county) + 1):\n",
    "            data_matrix = data.iloc[i - days: i, 1:].to_numpy()\n",
    "            yield data_matrix\n",
    "            \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac163c8-45f9-4a5c-972d-babb0d65816a",
   "metadata": {},
   "source": [
    "##### Save the raw X and Y to files of 50,000 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32add75b-d9ef-432b-a607-6fe85948c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = []\n",
    "j = 0\n",
    "\n",
    "N_SAMPLES = 200\n",
    "\n",
    "for i, x in enumerate(xy_generator(df)):\n",
    "    Xi.append(x)\n",
    "    if i and not i % (N_SAMPLES - 1):\n",
    "        X = np.asarray(Xi)\n",
    "        np.save(os.path.join('.','data', f'x_{j}.npy'), X)\n",
    "        j += 1\n",
    "        Xi = []\n",
    "if Xi:\n",
    "    X = np.asarray(Xi)\n",
    "    np.save(os.path.join('.','data', f'x_{j}.npy'), X)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac1bc8-469d-4412-a243-74e5113661c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('./data/x_0.npy')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664610a-0086-4410-b237-4ae731bce0d7",
   "metadata": {},
   "source": [
    "##### Split into train, test, eval directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabce3f5-5188-408b-8bb1-711fdb8af856",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "def set_seed():\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee865b7d-1f2b-4c1e-b24b-ee56a59ca907",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_files = glob('./data/x_*.npy')\n",
    "random.shuffle(x_files)\n",
    "n_files = len(x_files)\n",
    "print(n_files)\n",
    "n_train = int(n_files * 0.70)\n",
    "print(n_train)\n",
    "n_eval = int(n_files * 0.15)\n",
    "print(n_eval)\n",
    "n_test = n_files - n_train - n_eval\n",
    "print(n_test)\n",
    "train_files = x_files[:n_train]\n",
    "# print(len(train_files))\n",
    "eval_files = x_files[n_train:n_train+n_test]\n",
    "# print(len(eval_files))\n",
    "test_files = x_files[n_train+n_test:]\n",
    "assert n_files == len(train_files) + len(eval_files) + len(test_files)\n",
    "for (subdir, lst) in [['train', train_files], ['eval', eval_files], ['test', test_files]]:\n",
    "    for file in lst:\n",
    "        shutil.move(file, os.path.join('.', 'data', subdir))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72038235-dbcf-47e3-8a70-7b8021b68f7f",
   "metadata": {},
   "source": [
    "##### Create the Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c917f4-f604-48cf-a8dd-d07a1fffed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 16:36:12.807873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.808120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.834207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.834439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.834634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.834844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.835574: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-17 16:36:12.970487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.970709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.970888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.971066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.971243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:12.971413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:13.478921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:13.479163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:13.479350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:13.479527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:13.479700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:13.479874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3152 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2022-02-17 16:36:13.480123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-17 16:36:13.480282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 4424 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "train_files = glob('./data/train/x_*.npy')\n",
    "eval_files = glob('./data/eval/x_*.npy')\n",
    "test_files = glob('./data/test/x_*.npy')\n",
    "\n",
    "n_readers = 5\n",
    "n_parse_threads = 5\n",
    "len_array = 995\n",
    "\n",
    "def create_generator(files, cycle_length=5):\n",
    "    set_seed()\n",
    "    random.shuffle(files)\n",
    "    for i in range(0, len(files), cycle_length):\n",
    "        subset = files[i:i+cycle_length]\n",
    "        np_arrays = [np.load(s) for s in subset]\n",
    "        np_array = np.concatenate(np_arrays, axis=0)\n",
    "        np.random.shuffle(np_array)\n",
    "        yield np_array\n",
    "            \n",
    "\n",
    "def split_xy(np_array):\n",
    "    X = np_array[:,:-1,:]\n",
    "    y = np_array[:,-1:,:1]\n",
    "    return X,y\n",
    "        \n",
    "    \n",
    "train_ds = tf.data.Dataset.from_generator(lambda: create_generator(train_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "train_ds = train_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(lambda: create_generator(eval_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "val_ds = val_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(lambda: create_generator(test_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "test_ds = test_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e1b39-b6af-4662-9975-060c06c987ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in train_ds.take(3):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaea1d7-a885-4ca5-9f52-83846c1bd2df",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351ce58-2517-4ec3-9a7f-36e819e7f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'recurrent_layer_type': [keras.layers.LSTM, keras.layers.GRU],\n",
    "    'n_recurrent': [1, 2, 3],\n",
    "    'unit': [32, 64, 128],\n",
    "    'dropout': [0.0, 0.10, 0.20],\n",
    "    'lr': [1e-2, 1e-3],   \n",
    "    'epochs': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc01b2-659b-4d10-b0ba-a2ae967e58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(param, shape=(30,101)):\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(param['n_recurrent']):\n",
    "        last = param['recurrent_layer_type'](param['unit'])(last)\n",
    "        if param['dropout']:\n",
    "            last = keras.layers.Dropout(param['dropout'])(last)\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=param['lr']),  loss='mae',  metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2195ed-28cc-4565-9d05-31d855c1ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "n_trial = 3\n",
    "\n",
    "name = f'trial_{n_trial}'\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(f'./data/model/{name}_{timestamp}.h5', save_best_only=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(name))\n",
    "\n",
    "\n",
    "hypermodel = lambda param: build_model(param=param)\n",
    "\n",
    "kgs = KerasGridSearch(hypermodel, param_grid, monitor='val_loss', greater_is_better=False, tuner_verbose=1)\n",
    "kgs.search(train_ds, validation_data=val_ds, callbacks=[early_stopping_cb, checkpoint_cb, tensorboard], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea660af-bf47-4c55-a9ae-989a7d972e45",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning \n",
    "\n",
    "Using technique from https://medium.com/ml-book/neural-networks-hyperparameter-tuning-in-tensorflow-2-0-a7b4e2b574a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1978f67-3294-44bf-b79e-007a6645b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([1, 2, 3]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([32, 64, 128]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb81ad5-9d75-4725-ac82-5d514fcc5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,92)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "        output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6dcc69d-4d33-472c-a9b4-11eff17d74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/64\n",
      "      1/Unknown - 2s 2s/step - loss: 1573.9980"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-12 21:04:21.564677: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 30s 23ms/step - loss: 1655.9430 - val_loss: 1607.1738\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 1531.2352 - val_loss: 1488.4918\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 1418.1123 - val_loss: 1377.8768\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 1312.5516 - val_loss: 1273.7897\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 1216.1958 - val_loss: 1179.6952\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 1123.4619 - val_loss: 1087.0924\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 1038.9768 - val_loss: 1005.7421\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 961.2827 - val_loss: 928.5158\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 883.5455 - val_loss: 845.8647\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 807.2160 - val_loss: 770.0787\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 733.0685 - val_loss: 692.1030\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 658.5605 - val_loss: 613.7551\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 584.2672 - val_loss: 543.3002\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 515.5746 - val_loss: 471.6342\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 448.4738 - val_loss: 403.8286\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 385.2357 - val_loss: 342.5936\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 329.0291 - val_loss: 282.9219\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 283.0390 - val_loss: 245.7628\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 253.9978 - val_loss: 207.1026\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 237.9304 - val_loss: 192.7419\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 224.1305 - val_loss: 175.3768\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 209.5072 - val_loss: 168.0711\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 193.7213 - val_loss: 143.4017\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 182.7104 - val_loss: 131.8923\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 168.2168 - val_loss: 115.6937\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 153.0387 - val_loss: 104.3729\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 144.8915 - val_loss: 101.0609\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 134.0377 - val_loss: 91.2784\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 120.3218 - val_loss: 80.2682\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 113.0539 - val_loss: 58.4465\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 102.3584 - val_loss: 66.7713\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 96.3690 - val_loss: 43.7365\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 90.1235 - val_loss: 55.2684\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 89.1494 - val_loss: 37.2435\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 85.8963 - val_loss: 35.8754\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 92.2760 - val_loss: 42.3647\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 88.1875 - val_loss: 34.3044\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 85.9018 - val_loss: 33.3730\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 84.4147 - val_loss: 27.9821\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 86.6176 - val_loss: 26.3376\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 84.2492 - val_loss: 23.7702\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 82.5836 - val_loss: 26.5749\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 82.5288 - val_loss: 30.0234\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 81.4704 - val_loss: 30.3525\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 82.3144 - val_loss: 23.1992\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 80.8286 - val_loss: 21.8921\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 81.2200 - val_loss: 21.3950\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 80.4988 - val_loss: 22.5946\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 80.4885 - val_loss: 19.1527\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 80.0486 - val_loss: 21.7024\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 82.5976 - val_loss: 32.3238\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 82.1515 - val_loss: 31.4244\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 80.2175 - val_loss: 20.6556\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 80.3793 - val_loss: 24.1422\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 80.4731 - val_loss: 15.7410\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 84.9211 - val_loss: 23.9132\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 83.5627 - val_loss: 32.1263\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 82.3776 - val_loss: 23.2460\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 80.9947 - val_loss: 19.4846\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 81.9208 - val_loss: 20.9024\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 81.4395 - val_loss: 938.2432\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 86.4808 - val_loss: 25.0491\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 82.2320 - val_loss: 22.5670\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 81.8465 - val_loss: 17.6192\n",
      "270/270 [==============================] - 5s 17ms/step - loss: 18.1118\n",
      "--- Starting trial: run-1\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/64\n",
      "1258/1258 [==============================] - 45s 34ms/step - loss: 1653.5116 - val_loss: 1601.6819\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 1521.2778 - val_loss: 1473.4053\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 1399.0345 - val_loss: 1355.5433\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 1286.7819 - val_loss: 1245.1232\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 1181.7646 - val_loss: 1142.7811\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 1085.2412 - val_loss: 1048.1852\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 995.4984 - val_loss: 959.8311\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 909.0156 - val_loss: 872.8608\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 826.9280 - val_loss: 789.9618\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 44s 35ms/step - loss: 748.2893 - val_loss: 708.2676\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 670.8496 - val_loss: 631.1177\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 594.6801 - val_loss: 562.5140\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 521.0616 - val_loss: 482.4478\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 449.3401 - val_loss: 413.8011\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 382.8470 - val_loss: 349.4834\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 325.2393 - val_loss: 283.4860\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 278.0651 - val_loss: 246.6163\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 44s 35ms/step - loss: 250.2228 - val_loss: 217.7283\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 235.8650 - val_loss: 216.7444\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 222.2675 - val_loss: 180.5942\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 202.8575 - val_loss: 165.2648\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 186.0150 - val_loss: 515.9953\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 180.7045 - val_loss: 134.4141\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 160.6409 - val_loss: 117.5513\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 140.0733 - val_loss: 93.1799\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 120.4633 - val_loss: 96.1289\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 104.6662 - val_loss: 76.0532\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 44s 35ms/step - loss: 92.8174 - val_loss: 83.7206\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 84.9629 - val_loss: 61.5382\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 80.5377 - val_loss: 85.5241\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 81.1378 - val_loss: 50.0108\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 75.8960 - val_loss: 110.6821\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 75.3415 - val_loss: 100.5284\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 83.6327 - val_loss: 69.5506\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 78.2008 - val_loss: 87.3920\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 77.5559 - val_loss: 59.4870\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 76.9863 - val_loss: 63.8583\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 44s 35ms/step - loss: 76.0840 - val_loss: 72.5051\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 73.5444 - val_loss: 61.0833\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 73.3510 - val_loss: 86.3673\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 74.1215 - val_loss: 104.9530\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 74.9256 - val_loss: 67.9791\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 72.9338 - val_loss: 86.5936\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 72.4122 - val_loss: 97.7837\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 72.9206 - val_loss: 68.5159\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 71.4699 - val_loss: 62.6150\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 72.4193 - val_loss: 40.2620\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 72.6988 - val_loss: 69.8038\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 72.2990 - val_loss: 67.2267\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 70.7802 - val_loss: 79.7243\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 71.5036 - val_loss: 68.2547\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 70.5526 - val_loss: 72.1766\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 77.1144 - val_loss: 77.5714\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 70.8908 - val_loss: 51.4930\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 73.4757 - val_loss: 61.2497\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 70.9469 - val_loss: 45.3301\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 71.3639 - val_loss: 54.7921\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 70.3081 - val_loss: 46.5638\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 71.2281 - val_loss: 49.8308\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 71.4778 - val_loss: 81.6286\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 71.7219 - val_loss: 51.0988\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 69.3023 - val_loss: 183.3495\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 71.9520 - val_loss: 36.2652\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 68.6702 - val_loss: 54.0042\n",
      "270/270 [==============================] - 5s 18ms/step - loss: 53.2273\n",
      "--- Starting trial: run-2\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/64\n",
      "1258/1258 [==============================] - 61s 46ms/step - loss: 1662.7030 - val_loss: 1625.0164\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 1531.2968 - val_loss: 1479.4143\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 1405.3748 - val_loss: 1362.3220\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 1295.2452 - val_loss: 1254.4340\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 1191.2875 - val_loss: 1152.1096\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 1095.7611 - val_loss: 1058.2599\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 1006.3226 - val_loss: 968.6581\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 920.4658 - val_loss: 880.7459\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 838.1090 - val_loss: 799.6846\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 760.4537 - val_loss: 721.8941\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 685.0392 - val_loss: 646.4544\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 609.6254 - val_loss: 567.4199\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 536.2993 - val_loss: 492.6892\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 465.0597 - val_loss: 420.0005\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 397.5929 - val_loss: 354.3432\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 338.5605 - val_loss: 296.3436\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 288.5141 - val_loss: 245.9988\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 255.2857 - val_loss: 215.5980\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 238.9434 - val_loss: 199.8137\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 223.2380 - val_loss: 180.9861\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 205.5724 - val_loss: 162.8197\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 189.5708 - val_loss: 149.0770\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 173.6343 - val_loss: 130.5443\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 157.1167 - val_loss: 115.0491\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 139.3318 - val_loss: 100.4629\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 123.2060 - val_loss: 76.8244\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 107.0371 - val_loss: 61.6500\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 93.0461 - val_loss: 46.3429\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 84.6575 - val_loss: 38.0788\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 78.1149 - val_loss: 37.4698\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 76.2573 - val_loss: 29.4841\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 74.5278 - val_loss: 25.9293\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 78.2455 - val_loss: 26.9310\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 72.4801 - val_loss: 27.8343\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 71.8445 - val_loss: 22.9186\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 71.4303 - val_loss: 23.3770\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 70.2137 - val_loss: 18.4611\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 70.6659 - val_loss: 30.3256\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 69.2517 - val_loss: 22.4553\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 69.3062 - val_loss: 26.4395\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 70.0755 - val_loss: 32.1193\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 70.4662 - val_loss: 25.7990\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 68.3547 - val_loss: 21.7702\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 68.9065 - val_loss: 28.0949\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 68.4039 - val_loss: 18.9290\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 67.9897 - val_loss: 19.7344\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 67.1266 - val_loss: 34.7699\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 67.9796 - val_loss: 21.2646\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 66.5786 - val_loss: 16.6927\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.8012 - val_loss: 16.9046\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 67.3886 - val_loss: 15.7473\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.9350 - val_loss: 19.0108\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.7357 - val_loss: 16.5610\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 66.2005 - val_loss: 14.0234\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.8741 - val_loss: 21.8302\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.1309 - val_loss: 11.2359\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 64.3421 - val_loss: 19.7054\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 64.4000 - val_loss: 14.2276\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 63.6757 - val_loss: 18.1658\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 64.7532 - val_loss: 22.5983\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.3139 - val_loss: 18.1153\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 64.3290 - val_loss: 21.5893\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 63.5282 - val_loss: 16.7133\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 63.0393 - val_loss: 17.7646\n",
      "270/270 [==============================] - 5s 20ms/step - loss: 17.5613\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd7ef0-b079-4654-844d-c9c1958e07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38795671-b9c2-453a-92b0-bec2444c07fd",
   "metadata": {},
   "source": [
    "#### Testing 4, 5, and 6 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c00c2e5-4dfa-4637-aaab-35a7ecbe5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([4, 5, 6]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([32, 64, 128]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([256]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning6').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e722ee-b3d4-46dc-bedf-8a4616789a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,101)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb6e570-cc34-4cbd-9071-f4a17e802e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 4, 'n_unit': 256, 'dropout': 0.2, 'lr': 0.001}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 30, 101)]         0         \n",
      "                                                                 \n",
      " lstm_28 (LSTM)              (None, 30, 256)           366592    \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_29 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_30 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_31 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,942,785\n",
      "Trainable params: 1,942,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "1258/1258 [==============================] - 197s 154ms/step - loss: 1607.7527 - val_loss: 1527.4852\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1441.3444 - val_loss: 1392.5176\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1335.6595 - val_loss: 1305.0586\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1270.3264 - val_loss: 1254.5035\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1234.0780 - val_loss: 1223.6012\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1211.0061 - val_loss: 1203.3838\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1197.5437 - val_loss: 1192.0120\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1190.6106 - val_loss: 1185.8134\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1186.7582 - val_loss: 1181.7013\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1184.1683 - val_loss: 1178.6150\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1182.3234 - val_loss: 1176.3604\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1180.9785 - val_loss: 1174.5930\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1179.9333 - val_loss: 1173.0140\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 201s 159ms/step - loss: 1178.8268 - val_loss: 1171.3958\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1178.0159 - val_loss: 1170.0715\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1177.4442 - val_loss: 1169.0901\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1176.8892 - val_loss: 1168.0836\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1176.4908 - val_loss: 1167.3669\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1176.2637 - val_loss: 1166.8608\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1176.0619 - val_loss: 1166.4099\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.9083 - val_loss: 1166.0507\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.8408 - val_loss: 1165.8365\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.7555 - val_loss: 1165.6152\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6836 - val_loss: 1165.4650\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.7246 - val_loss: 1165.3585\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6921 - val_loss: 1165.3071\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6915 - val_loss: 1165.2542\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6302 - val_loss: 1165.2244\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.5585 - val_loss: 1165.1924\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.5222 - val_loss: 1165.1877\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6252 - val_loss: 1165.1674\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 200s 159ms/step - loss: 1175.5797 - val_loss: 1165.1658\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 200s 159ms/step - loss: 1175.6525 - val_loss: 1165.1455\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6335 - val_loss: 1165.1367\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6160 - val_loss: 1165.1155\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6716 - val_loss: 1165.1100\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6494 - val_loss: 1165.1154\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6770 - val_loss: 1165.0936\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6255 - val_loss: 1165.0978\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6317 - val_loss: 1165.0786\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6854 - val_loss: 1165.0967\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5565 - val_loss: 1165.0917\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5465 - val_loss: 1165.0919\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5736 - val_loss: 1165.0677\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6010 - val_loss: 1165.0825\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6506 - val_loss: 1165.0645\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6647 - val_loss: 1165.0608\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6432 - val_loss: 1165.0704\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6121 - val_loss: 1165.0596\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.6440 - val_loss: 1165.0631\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 200s 159ms/step - loss: 1175.6586 - val_loss: 1165.0634\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.7211 - val_loss: 1165.0499\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 200s 158ms/step - loss: 1175.5736 - val_loss: 1165.0563\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5668 - val_loss: 1165.0435\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6553 - val_loss: 1165.0566\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5908 - val_loss: 1165.0604\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5538 - val_loss: 1165.0630\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 199s 157ms/step - loss: 1175.6238 - val_loss: 1165.0648\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5884 - val_loss: 1165.0631\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.5852 - val_loss: 1165.0665\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.7400 - val_loss: 1165.0527\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6458 - val_loss: 1165.0662\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.7272 - val_loss: 1165.0598\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 199s 158ms/step - loss: 1175.6136 - val_loss: 1165.0559\n",
      "270/270 [==============================] - 17s 61ms/step - loss: 1163.4731\n",
      "--- Starting trial: run-1\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 5, 'n_unit': 256, 'dropout': 0.2, 'lr': 0.001}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 30, 101)]         0         \n",
      "                                                                 \n",
      " lstm_32 (LSTM)              (None, 30, 256)           366592    \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_33 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_34 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_35 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_36 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,468,097\n",
      "Trainable params: 2,468,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "1258/1258 [==============================] - 255s 199ms/step - loss: 1607.0083 - val_loss: 1524.3069\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 251s 199ms/step - loss: 1436.6438 - val_loss: 1387.3057\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 251s 199ms/step - loss: 1330.2729 - val_loss: 1299.8295\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 251s 199ms/step - loss: 1266.6963 - val_loss: 1251.7935\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 251s 199ms/step - loss: 1231.6820 - val_loss: 1221.1906\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 251s 199ms/step - loss: 1209.4344 - val_loss: 1201.8267\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1196.3995 - val_loss: 1190.9172\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1189.6930 - val_loss: 1184.9194\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1186.0217 - val_loss: 1180.9698\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1183.5930 - val_loss: 1177.8065\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1181.9110 - val_loss: 1175.7341\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1180.5262 - val_loss: 1173.9895\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1179.4897 - val_loss: 1172.3203\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1178.5332 - val_loss: 1170.7941\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1177.7993 - val_loss: 1169.6564\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1177.2356 - val_loss: 1168.4955\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1176.7589 - val_loss: 1167.7051\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1176.3169 - val_loss: 1167.0878\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1176.0710 - val_loss: 1166.5659\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1175.9954 - val_loss: 1166.1517\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1175.8608 - val_loss: 1165.9143\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1175.8225 - val_loss: 1165.7496\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1175.7024 - val_loss: 1165.5178\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.7073 - val_loss: 1165.4064\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6962 - val_loss: 1165.3646\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6575 - val_loss: 1165.2963\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6588 - val_loss: 1165.2563\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.5428 - val_loss: 1165.2311\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6498 - val_loss: 1165.2057\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1175.5405 - val_loss: 1165.2023\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1175.5844 - val_loss: 1165.1827\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.5636 - val_loss: 1165.1455\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.5724 - val_loss: 1165.1403\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1175.6327 - val_loss: 1165.1436\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1175.6367 - val_loss: 1165.1606\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 250s 198ms/step - loss: 1175.5726 - val_loss: 1165.1407\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1175.6089 - val_loss: 1165.1316\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1175.6749 - val_loss: 1165.1119\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.7112 - val_loss: 1165.1036\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 248s 197ms/step - loss: 1175.6500 - val_loss: 1165.0924\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 248s 197ms/step - loss: 1175.6587 - val_loss: 1165.0952\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 248s 197ms/step - loss: 1175.6661 - val_loss: 1165.0709\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6033 - val_loss: 1165.0657\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6229 - val_loss: 1165.0881\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6047 - val_loss: 1165.0920\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6134 - val_loss: 1165.0826\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6555 - val_loss: 1165.0792\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.5990 - val_loss: 1165.0842\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6100 - val_loss: 1165.1036\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1175.6351 - val_loss: 1165.0917\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 249s 198ms/step - loss: 1175.6510 - val_loss: 1165.0850\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6680 - val_loss: 1165.0710\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6257 - val_loss: 1165.0591\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.5309 - val_loss: 1165.0468\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 248s 197ms/step - loss: 1175.6814 - val_loss: 1165.0646\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 248s 197ms/step - loss: 1175.6672 - val_loss: 1165.0499\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 248s 197ms/step - loss: 1175.6396 - val_loss: 1165.0642\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6117 - val_loss: 1165.0563\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6415 - val_loss: 1165.0668\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6542 - val_loss: 1165.0707\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6948 - val_loss: 1165.0757\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6232 - val_loss: 1165.0582\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.5347 - val_loss: 1165.0540\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 249s 197ms/step - loss: 1175.6716 - val_loss: 1165.0781\n",
      "270/270 [==============================] - 20s 76ms/step - loss: 1163.4825\n",
      "--- Starting trial: run-2\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 6, 'n_unit': 256, 'dropout': 0.2, 'lr': 0.001}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 30, 101)]         0         \n",
      "                                                                 \n",
      " lstm_37 (LSTM)              (None, 30, 256)           366592    \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_38 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_39 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_40 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_41 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_42 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,993,409\n",
      "Trainable params: 2,993,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/64\n",
      "1258/1258 [==============================] - 306s 240ms/step - loss: 1607.6483 - val_loss: 1525.8564\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 301s 239ms/step - loss: 1438.9929 - val_loss: 1389.8311\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 301s 238ms/step - loss: 1332.5604 - val_loss: 1302.0989\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 301s 239ms/step - loss: 1267.8218 - val_loss: 1252.2933\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1232.3280 - val_loss: 1222.1416\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1209.9738 - val_loss: 1202.4071\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1196.7970 - val_loss: 1191.4106\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1190.2545 - val_loss: 1185.4897\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1186.5120 - val_loss: 1181.4004\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1183.9755 - val_loss: 1178.3182\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1182.2839 - val_loss: 1176.2074\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1180.8831 - val_loss: 1174.4391\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1179.7833 - val_loss: 1172.7631\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 301s 239ms/step - loss: 1178.8007 - val_loss: 1171.2433\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1177.9923 - val_loss: 1169.9220\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1177.2921 - val_loss: 1168.8209\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1176.7571 - val_loss: 1167.9081\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1176.3695 - val_loss: 1167.2531\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1176.0505 - val_loss: 1166.6833\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.9142 - val_loss: 1166.2495\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 300s 237ms/step - loss: 1175.8057 - val_loss: 1165.9227\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 300s 237ms/step - loss: 1175.7943 - val_loss: 1165.7015\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6721 - val_loss: 1165.5353\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 300s 237ms/step - loss: 1175.7687 - val_loss: 1165.4180\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1175.7107 - val_loss: 1165.3367\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1175.6592 - val_loss: 1165.3083\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1175.6366 - val_loss: 1165.2766\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1175.4990 - val_loss: 1165.2380\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 300s 237ms/step - loss: 1175.7180 - val_loss: 1165.1976\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5985 - val_loss: 1165.1737\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5536 - val_loss: 1165.1420\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6967 - val_loss: 1165.1392\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5662 - val_loss: 1165.1227\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6748 - val_loss: 1165.1084\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5659 - val_loss: 1165.1211\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6594 - val_loss: 1165.0999\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5168 - val_loss: 1165.0956\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.7043 - val_loss: 1165.0845\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6616 - val_loss: 1165.0808\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 300s 238ms/step - loss: 1175.6073 - val_loss: 1165.0775\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5715 - val_loss: 1165.0978\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6479 - val_loss: 1165.0964\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6279 - val_loss: 1165.1052\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6342 - val_loss: 1165.1021\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6072 - val_loss: 1165.1010\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.7028 - val_loss: 1165.1010\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5537 - val_loss: 1165.0873\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6744 - val_loss: 1165.0663\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6138 - val_loss: 1165.0676\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5679 - val_loss: 1165.0599\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5685 - val_loss: 1165.0665\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6979 - val_loss: 1165.0774\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5353 - val_loss: 1165.0542\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5814 - val_loss: 1165.0635\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5865 - val_loss: 1165.0658\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6022 - val_loss: 1165.0548\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6143 - val_loss: 1165.0645\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6180 - val_loss: 1165.0520\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6284 - val_loss: 1165.0573\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5785 - val_loss: 1165.0394\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5975 - val_loss: 1165.0681\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.5370 - val_loss: 1165.0598\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6161 - val_loss: 1165.0554\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 299s 237ms/step - loss: 1175.6819 - val_loss: 1165.0492\n",
      "270/270 [==============================] - 24s 90ms/step - loss: 1163.4703\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning6/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250522c-de3c-426f-a9ce-a8b4c75363d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2152bffd-bde8-4e80-a2d9-f2dea66d3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([3]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128, 256, 512]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128, 256, 512]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning5').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32b57339-bdb2-44b6-96b3-5ac4f0e3d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,101)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7deaee76-e048-404a-9f86-10038aa964b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 16:36:53.548027: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 62s 47ms/step - loss: 1661.1746 - val_loss: 1622.7023\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 57s 45ms/step - loss: 1555.8944 - val_loss: 1486.3132\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 58s 45ms/step - loss: 1409.1813 - val_loss: 1363.9415\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 1294.5742 - val_loss: 1252.1602\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 1190.1820 - val_loss: 1150.6349\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 1094.5149 - val_loss: 1056.9688\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 1006.1715 - val_loss: 969.2693\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 921.3048 - val_loss: 883.3784\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 840.2504 - val_loss: 802.2787\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 762.4982 - val_loss: 725.4400\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 685.3651 - val_loss: 644.4866\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 608.7477 - val_loss: 566.3945\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 535.3616 - val_loss: 494.4081\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 464.5135 - val_loss: 418.5449\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 397.1922 - val_loss: 352.8314\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 336.8033 - val_loss: 292.1996\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 290.7431 - val_loss: 245.0836\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 254.9182 - val_loss: 212.4283\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 238.5000 - val_loss: 202.6682\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 221.4700 - val_loss: 175.0379\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 203.8197 - val_loss: 158.2563\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 186.4195 - val_loss: 147.9978\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 168.4673 - val_loss: 129.7428\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 150.4281 - val_loss: 103.4887\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 132.0218 - val_loss: 87.7154\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 114.9042 - val_loss: 74.6613\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 99.3942 - val_loss: 52.2601\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 86.3151 - val_loss: 41.6477\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 78.7997 - val_loss: 28.7213\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 74.9716 - val_loss: 44.7499\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 72.6366 - val_loss: 27.1009\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 71.4322 - val_loss: 25.7550\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 71.4173 - val_loss: 33.5859\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 70.6387 - val_loss: 31.8106\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 69.8480 - val_loss: 34.3708\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 69.2207 - val_loss: 45.2535\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 68.6370 - val_loss: 28.8770\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 83.8120 - val_loss: 33.9533\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 71.8902 - val_loss: 25.2061\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 68.5134 - val_loss: 20.8052\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 69.4910 - val_loss: 24.1158\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 70.8369 - val_loss: 27.3071\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 69.0373 - val_loss: 30.8727\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 67.6533 - val_loss: 22.7922\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 71.2859 - val_loss: 22.9159\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 68.4328 - val_loss: 35.9221\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 68.6597 - val_loss: 25.1560\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 67.8716 - val_loss: 21.7573\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 66.5599 - val_loss: 14.6406\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 69.1890 - val_loss: 27.7044\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 67.1172 - val_loss: 19.4790\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 66.2658 - val_loss: 17.5764\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.4221 - val_loss: 26.9325\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 67.2628 - val_loss: 14.4982\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 66.1376 - val_loss: 24.0888\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 66.9343 - val_loss: 17.2028\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.6540 - val_loss: 23.3407\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 65.5315 - val_loss: 18.9452\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 64.8310 - val_loss: 35.1339\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 68.5688 - val_loss: 26.0434\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 64.9581 - val_loss: 19.0320\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 64.1719 - val_loss: 22.7884\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 58s 46ms/step - loss: 71.1075 - val_loss: 30.3906\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 59s 46ms/step - loss: 67.2273 - val_loss: 20.9687\n",
      "270/270 [==============================] - 7s 24ms/step - loss: 21.0876\n",
      "--- Starting trial: run-1\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 256, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/64\n",
      "1258/1258 [==============================] - 152s 119ms/step - loss: 1600.0452 - val_loss: 1481.3835\n",
      "Epoch 2/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 1348.6736 - val_loss: 1251.9369\n",
      "Epoch 3/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 1142.7509 - val_loss: 1061.3778\n",
      "Epoch 4/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 969.2493 - val_loss: 892.8746\n",
      "Epoch 5/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 812.1468 - val_loss: 739.4127\n",
      "Epoch 6/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 666.3255 - val_loss: 594.1100\n",
      "Epoch 7/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 523.5789 - val_loss: 458.8968\n",
      "Epoch 8/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 391.4889 - val_loss: 322.3504\n",
      "Epoch 9/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 290.6183 - val_loss: 235.2072\n",
      "Epoch 10/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 240.3852 - val_loss: 212.8215\n",
      "Epoch 11/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 219.9218 - val_loss: 196.0893\n",
      "Epoch 12/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 188.1588 - val_loss: 143.0691\n",
      "Epoch 13/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 166.0862 - val_loss: 161.3627\n",
      "Epoch 14/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 147.2801 - val_loss: 93.7110\n",
      "Epoch 15/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 103.5252 - val_loss: 56.7467\n",
      "Epoch 16/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 115.2763 - val_loss: 85.1180\n",
      "Epoch 17/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 71.0544 - val_loss: 31.0226\n",
      "Epoch 18/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 62.8672 - val_loss: 45.8331\n",
      "Epoch 19/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 60.1788 - val_loss: 29.5218\n",
      "Epoch 20/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 60.1141 - val_loss: 34.5716\n",
      "Epoch 21/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 57.8644 - val_loss: 22.7753\n",
      "Epoch 22/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 55.5094 - val_loss: 35.8061\n",
      "Epoch 23/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 56.3114 - val_loss: 19.6905\n",
      "Epoch 24/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 54.4612 - val_loss: 36.7767\n",
      "Epoch 25/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 54.5127 - val_loss: 27.8950\n",
      "Epoch 26/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 62.8413 - val_loss: 35.1457\n",
      "Epoch 27/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 61.8907 - val_loss: 30.9016\n",
      "Epoch 28/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 59.9696 - val_loss: 28.7275\n",
      "Epoch 29/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 78.6511 - val_loss: 39.9967\n",
      "Epoch 30/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 67.5667 - val_loss: 29.5513\n",
      "Epoch 31/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 60.6849 - val_loss: 25.7106\n",
      "Epoch 32/64\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 60.0937 - val_loss: 39.3364\n",
      "Epoch 33/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 62.4499 - val_loss: 26.8727\n",
      "Epoch 34/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 57.9900 - val_loss: 27.6383\n",
      "Epoch 35/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 57.2883 - val_loss: 28.6407\n",
      "Epoch 36/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 56.7734 - val_loss: 30.8244\n",
      "Epoch 37/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 58.4992 - val_loss: 25.0345\n",
      "Epoch 38/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 58.2959 - val_loss: 22.0396\n",
      "Epoch 39/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 57.0764 - val_loss: 22.2135\n",
      "Epoch 40/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 56.3956 - val_loss: 37.1358\n",
      "Epoch 41/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 81.3169 - val_loss: 35.6664\n",
      "Epoch 42/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 59.6633 - val_loss: 29.9119\n",
      "Epoch 43/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 57.9191 - val_loss: 59.5433\n",
      "Epoch 44/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 57.9666 - val_loss: 27.6630\n",
      "Epoch 45/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 56.8088 - val_loss: 34.3807\n",
      "Epoch 46/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 57.9768 - val_loss: 28.4024\n",
      "Epoch 47/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 55.7750 - val_loss: 20.2483\n",
      "Epoch 48/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 54.7861 - val_loss: 21.5884\n",
      "Epoch 49/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 54.7104 - val_loss: 24.3394\n",
      "Epoch 50/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 53.6443 - val_loss: 22.2603\n",
      "Epoch 51/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 53.5745 - val_loss: 20.8775\n",
      "Epoch 52/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 52.3433 - val_loss: 23.4997\n",
      "Epoch 53/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 52.2945 - val_loss: 16.4076\n",
      "Epoch 54/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 51.7818 - val_loss: 26.1669\n",
      "Epoch 55/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 52.5175 - val_loss: 30.3271\n",
      "Epoch 56/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 50.9529 - val_loss: 13.5830\n",
      "Epoch 57/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 52.9787 - val_loss: 17.2907\n",
      "Epoch 58/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 51.1938 - val_loss: 15.5777\n",
      "Epoch 59/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 49.8580 - val_loss: 13.4098\n",
      "Epoch 60/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 50.0232 - val_loss: 12.5571\n",
      "Epoch 61/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 49.3238 - val_loss: 11.3282\n",
      "Epoch 62/64\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 49.4122 - val_loss: 15.2882\n",
      "Epoch 63/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 48.5982 - val_loss: 11.1743\n",
      "Epoch 64/64\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 50.9561 - val_loss: 20.9361\n",
      "270/270 [==============================] - 13s 47ms/step - loss: 21.1558\n",
      "--- Starting trial: run-2\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 512, 'dropout': 0.2, 'lr': 0.001}\n",
      "Epoch 1/64\n",
      "1258/1258 [==============================] - 501s 397ms/step - loss: 1520.2526 - val_loss: 1390.6075\n",
      "Epoch 2/64\n",
      "1106/1258 [=========================>....] - ETA: 55s - loss: 1127.5464"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning5/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3619c57-c71e-4046-a136-9c695001ca73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeabaac-292d-4b7e-b16b-9865e57f7ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6563f7-1956-45a4-9d1a-48b1d0123d40",
   "metadata": {},
   "source": [
    "# IGNORE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a22f3-db2b-46cc-b86f-145672a92814",
   "metadata": {},
   "source": [
    "#### Output from first full trial run without geoencoding. From 2/11/22 - 2/12/22.  Logs in hparam_tuning_2022-02-11-no-geoencoding-no-seed\n",
    "\n",
    "    --- Starting trial: run-0\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1714.8448 - mae: 1714.8448 - val_loss: 1698.0995 - val_mae: 1698.0995\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1676.5148 - mae: 1676.5148 - val_loss: 1662.7485 - val_mae: 1662.7485\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1640.6196 - mae: 1640.6196 - val_loss: 1628.1760 - val_mae: 1628.1760\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1605.7626 - mae: 1605.7626 - val_loss: 1593.2338 - val_mae: 1593.2338\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1572.2823 - mae: 1572.2823 - val_loss: 1558.5253 - val_mae: 1558.5253\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1537.3369 - mae: 1537.3369 - val_loss: 1526.2665 - val_mae: 1526.2665\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1503.9446 - mae: 1503.9446 - val_loss: 1490.5928 - val_mae: 1490.5928\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1470.5991 - mae: 1470.5991 - val_loss: 1459.7194 - val_mae: 1459.7194\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1438.0496 - mae: 1438.0496 - val_loss: 1427.7711 - val_mae: 1427.7711\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1406.5685 - mae: 1406.5685 - val_loss: 1397.7261 - val_mae: 1397.7261\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1375.7841 - mae: 1375.7841 - val_loss: 1368.0007 - val_mae: 1368.0007\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1345.6710 - mae: 1345.6710 - val_loss: 1335.7712 - val_mae: 1335.7712\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1315.6511 - mae: 1315.6511 - val_loss: 1307.7990 - val_mae: 1307.7990\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1286.4814 - mae: 1286.4814 - val_loss: 1277.9666 - val_mae: 1277.9666\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1258.2283 - mae: 1258.2283 - val_loss: 1252.6665 - val_mae: 1252.6665\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1229.5457 - mae: 1229.5457 - val_loss: 1225.6205 - val_mae: 1225.6205\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1210.3488 - mae: 1210.3488\n",
    "    --- Starting trial: run-1\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 35ms/step - loss: 1558.9589 - mae: 1558.9589 - val_loss: 1404.5382 - val_mae: 1404.5382\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1262.5734 - mae: 1262.5734 - val_loss: 1144.1647 - val_mae: 1144.1647\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1042.4114 - mae: 1042.4114 - val_loss: 988.6213 - val_mae: 988.6213\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 884.6074 - mae: 884.6074 - val_loss: 805.5174 - val_mae: 805.5174\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 712.0137 - mae: 712.0137 - val_loss: 639.9149 - val_mae: 639.9149\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 560.2214 - mae: 560.2214 - val_loss: 535.0701 - val_mae: 535.0701\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 470.0902 - mae: 470.0903 - val_loss: 440.2448 - val_mae: 440.2448\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 402.9714 - mae: 402.9714 - val_loss: 378.6845 - val_mae: 378.6845\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 376.1292 - mae: 376.1292 - val_loss: 351.0140 - val_mae: 351.0140\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 335.2832 - mae: 335.2832 - val_loss: 357.6178 - val_mae: 357.6178\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 309.3643 - mae: 309.3643 - val_loss: 274.2231 - val_mae: 274.2231\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 259.6491 - mae: 259.6491 - val_loss: 252.2940 - val_mae: 252.2940\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 240.8009 - mae: 240.8009 - val_loss: 249.1926 - val_mae: 249.1926\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 254.2686 - mae: 254.2686 - val_loss: 226.4079 - val_mae: 226.4079\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 216.1730 - mae: 216.1730 - val_loss: 212.1332 - val_mae: 212.1332\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 210.3830 - mae: 210.3830 - val_loss: 228.0083 - val_mae: 228.0083\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 222.4450 - mae: 222.4450\n",
    "    --- Starting trial: run-2\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.0021 - mae: 1715.0021 - val_loss: 1696.0353 - val_mae: 1696.0353\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1675.9495 - mae: 1675.9495 - val_loss: 1661.2244 - val_mae: 1661.2244\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1640.5969 - mae: 1640.5969 - val_loss: 1625.9960 - val_mae: 1625.9960\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1605.8707 - mae: 1605.8707 - val_loss: 1592.2338 - val_mae: 1592.2338\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1571.0924 - mae: 1571.0924 - val_loss: 1555.5453 - val_mae: 1555.5453\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1537.3981 - mae: 1537.3981 - val_loss: 1524.9481 - val_mae: 1524.9481\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1504.1108 - mae: 1504.1108 - val_loss: 1489.3528 - val_mae: 1489.3528\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1471.0813 - mae: 1471.0813 - val_loss: 1458.1285 - val_mae: 1458.1285\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1438.5751 - mae: 1438.5751 - val_loss: 1428.0801 - val_mae: 1428.0801\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1407.9596 - mae: 1407.9596 - val_loss: 1395.9967 - val_mae: 1395.9967\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1377.3335 - mae: 1377.3335 - val_loss: 1367.2148 - val_mae: 1367.2148\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1346.6747 - mae: 1346.6747 - val_loss: 1335.2466 - val_mae: 1335.2466\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1317.4336 - mae: 1317.4336 - val_loss: 1307.0999 - val_mae: 1307.0999\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1289.2543 - mae: 1289.2543 - val_loss: 1277.9841 - val_mae: 1277.9841\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1260.2249 - mae: 1260.2249 - val_loss: 1249.4365 - val_mae: 1249.4365\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1231.8981 - mae: 1231.8981 - val_loss: 1222.8695 - val_mae: 1222.8695\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1203.4991 - mae: 1203.4991\n",
    "    --- Starting trial: run-3\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1563.2950 - mae: 1563.2950 - val_loss: 1408.8458 - val_mae: 1408.8458\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1273.8225 - mae: 1273.8225 - val_loss: 1148.4290 - val_mae: 1148.4290\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1064.4722 - mae: 1064.4722 - val_loss: 963.9249 - val_mae: 963.9249\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 917.4946 - mae: 917.4946 - val_loss: 863.0434 - val_mae: 863.0434\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 822.7327 - mae: 822.7327 - val_loss: 775.3434 - val_mae: 775.3434\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 724.7830 - mae: 724.7830 - val_loss: 639.7692 - val_mae: 639.7692\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 614.3043 - mae: 614.3043 - val_loss: 516.8727 - val_mae: 516.8727\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 537.7888 - mae: 537.7888 - val_loss: 456.5970 - val_mae: 456.5970\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 467.8847 - mae: 467.8847 - val_loss: 357.0389 - val_mae: 357.0389\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 411.8556 - mae: 411.8556 - val_loss: 351.1302 - val_mae: 351.1302\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 410.8646 - mae: 410.8646 - val_loss: 283.1604 - val_mae: 283.1604\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 390.0403 - mae: 390.0403 - val_loss: 304.8136 - val_mae: 304.8136\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 386.8397 - mae: 386.8397 - val_loss: 258.3987 - val_mae: 258.3987\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 377.4386 - mae: 377.4386 - val_loss: 260.6350 - val_mae: 260.6350\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 373.3785 - mae: 373.3785 - val_loss: 276.4822 - val_mae: 276.4822\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 370.0834 - mae: 370.0834 - val_loss: 258.6117 - val_mae: 258.6117\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 251.2752 - mae: 251.2752\n",
    "    --- Starting trial: run-4\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1694.6875 - mae: 1694.6875 - val_loss: 1659.0698 - val_mae: 1659.0698\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1622.3441 - mae: 1622.3441 - val_loss: 1592.0505 - val_mae: 1592.0505\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1555.8229 - mae: 1555.8229 - val_loss: 1526.0814 - val_mae: 1526.0814\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1489.6663 - mae: 1489.6663 - val_loss: 1462.4453 - val_mae: 1462.4453\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1426.1626 - mae: 1426.1626 - val_loss: 1401.9027 - val_mae: 1401.9027\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1365.9249 - mae: 1365.9249 - val_loss: 1343.3584 - val_mae: 1343.3584\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1307.6086 - mae: 1307.6086 - val_loss: 1287.0707 - val_mae: 1287.0707\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1251.5591 - mae: 1251.5591 - val_loss: 1231.2085 - val_mae: 1231.2085\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1196.7355 - mae: 1196.7355 - val_loss: 1178.4388 - val_mae: 1178.4388\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1144.3530 - mae: 1144.3530 - val_loss: 1128.5942 - val_mae: 1128.5942\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1094.1368 - mae: 1094.1368 - val_loss: 1078.8323 - val_mae: 1078.8323\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1045.3096 - mae: 1045.3096 - val_loss: 1032.2638 - val_mae: 1032.2638\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 999.4599 - mae: 999.4599 - val_loss: 988.3935 - val_mae: 988.3935\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 953.3505 - mae: 953.3505 - val_loss: 942.0163 - val_mae: 942.0163\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 908.6214 - mae: 908.6214 - val_loss: 897.7542 - val_mae: 897.7542\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 865.5666 - mae: 865.5666 - val_loss: 855.2752 - val_mae: 855.2752\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 842.3766 - mae: 842.3766\n",
    "    --- Starting trial: run-5\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1419.5444 - mae: 1419.5444 - val_loss: 1181.4783 - val_mae: 1181.4783\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 999.2672 - mae: 999.2672 - val_loss: 847.1257 - val_mae: 847.1257\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 769.5046 - mae: 769.5046 - val_loss: 727.3767 - val_mae: 727.3767\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 642.5268 - mae: 642.5268 - val_loss: 578.0294 - val_mae: 578.0294\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 466.1321 - mae: 466.1321 - val_loss: 441.7265 - val_mae: 441.7265\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 408.8712 - mae: 408.8712 - val_loss: 422.5520 - val_mae: 422.5520\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 402.7164 - mae: 402.7164 - val_loss: 364.4363 - val_mae: 364.4363\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 372.0241 - mae: 372.0241 - val_loss: 397.2017 - val_mae: 397.2017\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 350.7146 - mae: 350.7146 - val_loss: 340.6723 - val_mae: 340.6723\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 282.5263 - mae: 282.5263 - val_loss: 294.4954 - val_mae: 294.4954\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 247.9475 - mae: 247.9475 - val_loss: 262.1410 - val_mae: 262.1410\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 261.2445 - mae: 261.2445 - val_loss: 264.5265 - val_mae: 264.5265\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 238.0075 - mae: 238.0075 - val_loss: 239.8622 - val_mae: 239.8622\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 232.3582 - mae: 232.3582 - val_loss: 237.3781 - val_mae: 237.3781\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 240.3781 - mae: 240.3781 - val_loss: 257.9360 - val_mae: 257.9360\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 234.7059 - mae: 234.7059 - val_loss: 244.7222 - val_mae: 244.7222\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 239.0346 - mae: 239.0346\n",
    "    --- Starting trial: run-6\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 35ms/step - loss: 1695.3423 - mae: 1695.3423 - val_loss: 1660.6970 - val_mae: 1660.6970\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1623.3080 - mae: 1623.3080 - val_loss: 1592.8999 - val_mae: 1592.8999\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1555.6199 - mae: 1555.6199 - val_loss: 1526.4407 - val_mae: 1526.4407\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1490.6748 - mae: 1490.6748 - val_loss: 1462.4221 - val_mae: 1462.4221\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1427.6664 - mae: 1427.6664 - val_loss: 1401.7178 - val_mae: 1401.7178\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1366.9703 - mae: 1366.9703 - val_loss: 1342.8413 - val_mae: 1342.8413\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1310.2562 - mae: 1310.2562 - val_loss: 1286.1354 - val_mae: 1286.1354\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1253.6902 - mae: 1253.6902 - val_loss: 1231.1028 - val_mae: 1231.1028\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1199.5375 - mae: 1199.5375 - val_loss: 1177.7694 - val_mae: 1177.7694\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1148.2744 - mae: 1148.2744 - val_loss: 1127.2009 - val_mae: 1127.2009\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1098.7969 - mae: 1098.7969 - val_loss: 1080.4926 - val_mae: 1080.4926\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1050.2742 - mae: 1050.2742 - val_loss: 1032.7545 - val_mae: 1032.7545\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1004.1852 - mae: 1004.1852 - val_loss: 985.5518 - val_mae: 985.5518\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 958.0642 - mae: 958.0642 - val_loss: 937.9445 - val_mae: 937.9445\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 913.8459 - mae: 913.8459 - val_loss: 895.4848 - val_mae: 895.4848\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 870.8777 - mae: 870.8777 - val_loss: 851.7709 - val_mae: 851.7709\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 839.1649 - mae: 839.1649\n",
    "    --- Starting trial: run-7\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1421.1002 - mae: 1421.1002 - val_loss: 1167.2504 - val_mae: 1167.2504\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1052.9386 - mae: 1052.9386 - val_loss: 935.0981 - val_mae: 935.0981\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 837.0973 - mae: 837.0973 - val_loss: 686.8972 - val_mae: 686.8972\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 632.3118 - mae: 632.3118 - val_loss: 578.2532 - val_mae: 578.2532\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 549.2214 - mae: 549.2214 - val_loss: 431.4684 - val_mae: 431.4684\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 413.2884 - mae: 413.2884 - val_loss: 404.3950 - val_mae: 404.3950\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 401.4612 - mae: 401.4612 - val_loss: 305.3578 - val_mae: 305.3578\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 377.7690 - mae: 377.7690 - val_loss: 289.4633 - val_mae: 289.4633\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 361.9821 - mae: 361.9821 - val_loss: 255.5692 - val_mae: 255.5692\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 350.9854 - mae: 350.9854 - val_loss: 259.7381 - val_mae: 259.7381\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 355.0371 - mae: 355.0371 - val_loss: 255.4603 - val_mae: 255.4603\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 356.7462 - mae: 356.7462 - val_loss: 280.8849 - val_mae: 280.8849\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 368.7358 - mae: 368.7358 - val_loss: 260.0245 - val_mae: 260.0245\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 419.5009 - mae: 419.5009 - val_loss: 295.2500 - val_mae: 295.2500\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 384.6466 - mae: 384.6466 - val_loss: 333.3893 - val_mae: 333.3893\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 412.6953 - mae: 412.6953 - val_loss: 292.2436 - val_mae: 292.2436\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 285.3722 - mae: 285.3722\n",
    "    --- Starting trial: run-8\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1655.6770 - mae: 1655.6769 - val_loss: 1590.8164 - val_mae: 1590.8164\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1520.9161 - mae: 1520.9161 - val_loss: 1460.7323 - val_mae: 1460.7323\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1396.8207 - mae: 1396.8207 - val_loss: 1346.4431 - val_mae: 1346.4431\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1284.3027 - mae: 1284.3027 - val_loss: 1236.5032 - val_mae: 1236.5032\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1177.3429 - mae: 1177.3429 - val_loss: 1135.1763 - val_mae: 1135.1763\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1079.6230 - mae: 1079.6230 - val_loss: 1042.6494 - val_mae: 1042.6494\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 989.1205 - mae: 989.1205 - val_loss: 956.0320 - val_mae: 956.0320\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 901.6415 - mae: 901.6415 - val_loss: 870.8302 - val_mae: 870.8302\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 820.0585 - mae: 820.0585 - val_loss: 792.4569 - val_mae: 792.4569\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 742.1097 - mae: 742.1097 - val_loss: 716.5116 - val_mae: 716.5116\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 665.9520 - mae: 665.9520 - val_loss: 640.8953 - val_mae: 640.8953\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 590.5352 - mae: 590.5352 - val_loss: 568.8193 - val_mae: 568.8193\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 518.8450 - mae: 518.8450 - val_loss: 494.9503 - val_mae: 494.9503\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 448.5501 - mae: 448.5501 - val_loss: 426.8623 - val_mae: 426.8623\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 380.4318 - mae: 380.4318 - val_loss: 360.3162 - val_mae: 360.3162\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 318.8451 - mae: 318.8451 - val_loss: 303.6874 - val_mae: 303.6874\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 296.1056 - mae: 296.1056\n",
    "    --- Starting trial: run-9\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1212.5996 - mae: 1212.5996 - val_loss: 875.3373 - val_mae: 875.3373\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 684.7706 - mae: 684.7706 - val_loss: 497.7931 - val_mae: 497.7931\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 406.5892 - mae: 406.5892 - val_loss: 359.6450 - val_mae: 359.6450\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 287.7188 - mae: 287.7188 - val_loss: 275.8997 - val_mae: 275.8997\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 250.5547 - mae: 250.5547 - val_loss: 262.4268 - val_mae: 262.4268\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 227.2360 - mae: 227.2360 - val_loss: 220.8222 - val_mae: 220.8222\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 213.4138 - mae: 213.4138 - val_loss: 217.0214 - val_mae: 217.0214\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 216.3246 - mae: 216.3246 - val_loss: 205.4248 - val_mae: 205.4248\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 202.3762 - mae: 202.3762 - val_loss: 207.0659 - val_mae: 207.0659\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 201.4633 - mae: 201.4633 - val_loss: 210.2004 - val_mae: 210.2004\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 200.4634 - mae: 200.4634 - val_loss: 204.3891 - val_mae: 204.3891\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 197.9046 - mae: 197.9046 - val_loss: 204.3471 - val_mae: 204.3471\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 198.0645 - mae: 198.0645 - val_loss: 203.8806 - val_mae: 203.8806\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 241.8784 - mae: 241.8784 - val_loss: 259.3530 - val_mae: 259.3530\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 234.5721 - mae: 234.5721 - val_loss: 239.7617 - val_mae: 239.7617\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 235.8562 - mae: 235.8562 - val_loss: 263.1089 - val_mae: 263.1089\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 255.6098 - mae: 255.6098\n",
    "    --- Starting trial: run-10\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1654.9674 - mae: 1654.9674 - val_loss: 1586.5536 - val_mae: 1586.5536\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1518.9548 - mae: 1518.9548 - val_loss: 1461.9506 - val_mae: 1461.9506\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1396.2662 - mae: 1396.2662 - val_loss: 1344.5743 - val_mae: 1344.5743\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1282.9016 - mae: 1282.9016 - val_loss: 1233.0769 - val_mae: 1233.0769\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1177.4990 - mae: 1177.4990 - val_loss: 1133.3776 - val_mae: 1133.3776\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1080.0483 - mae: 1080.0483 - val_loss: 1038.5483 - val_mae: 1038.5483\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 988.6093 - mae: 988.6093 - val_loss: 950.0257 - val_mae: 950.0257\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 901.9268 - mae: 901.9268 - val_loss: 864.5103 - val_mae: 864.5103\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 819.3678 - mae: 819.3678 - val_loss: 784.0034 - val_mae: 784.0034\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 740.5015 - mae: 740.5015 - val_loss: 706.0483 - val_mae: 706.0483\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 662.5690 - mae: 662.5690 - val_loss: 628.4536 - val_mae: 628.4536\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 586.1405 - mae: 586.1405 - val_loss: 552.5478 - val_mae: 552.5478\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 512.1078 - mae: 512.1078 - val_loss: 478.9315 - val_mae: 478.9315\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 441.2626 - mae: 441.2626 - val_loss: 407.5898 - val_mae: 407.5898\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 375.0634 - mae: 375.0634 - val_loss: 343.8361 - val_mae: 343.8361\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 317.2447 - mae: 317.2447 - val_loss: 285.1237 - val_mae: 285.1237\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 277.4681 - mae: 277.4681\n",
    "    --- Starting trial: run-11\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1234.6505 - mae: 1234.6505 - val_loss: 912.9206 - val_mae: 912.9206\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 710.6535 - mae: 710.6535 - val_loss: 520.6102 - val_mae: 520.6102\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 450.1460 - mae: 450.1460 - val_loss: 372.7587 - val_mae: 372.7587\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 359.3508 - mae: 359.3508 - val_loss: 288.9084 - val_mae: 288.9084\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.8996 - mae: 313.8996 - val_loss: 250.5107 - val_mae: 250.5107\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 318.7048 - mae: 318.7048 - val_loss: 254.1273 - val_mae: 254.1273\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 322.0482 - mae: 322.0482 - val_loss: 242.8332 - val_mae: 242.8332\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 325.5547 - mae: 325.5547 - val_loss: 252.1033 - val_mae: 252.1033\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 330.0924 - mae: 330.0924 - val_loss: 250.4689 - val_mae: 250.4689\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 338.8605 - mae: 338.8605 - val_loss: 260.5037 - val_mae: 260.5037\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 343.8581 - mae: 343.8581 - val_loss: 261.8107 - val_mae: 261.8107\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 347.2616 - mae: 347.2616 - val_loss: 264.5319 - val_mae: 264.5319\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 340.9579 - mae: 340.9579 - val_loss: 240.9453 - val_mae: 240.9453\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 333.7604 - mae: 333.7604 - val_loss: 226.1823 - val_mae: 226.1823\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 329.6325 - mae: 329.6325 - val_loss: 259.2430 - val_mae: 259.2430\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 343.8550 - mae: 343.8550 - val_loss: 261.8019 - val_mae: 261.8019\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 254.7327 - mae: 254.7327\n",
    "    --- Starting trial: run-12\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.8248 - mae: 1715.8248 - val_loss: 1697.2705 - val_mae: 1697.2705\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1677.9413 - mae: 1677.9413 - val_loss: 1664.1956 - val_mae: 1664.1956\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1642.4662 - mae: 1642.4662 - val_loss: 1625.6179 - val_mae: 1625.6179\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1607.7755 - mae: 1607.7755 - val_loss: 1595.0839 - val_mae: 1595.0839\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1573.5767 - mae: 1573.5767 - val_loss: 1561.6393 - val_mae: 1561.6393\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1539.0200 - mae: 1539.0200 - val_loss: 1525.4354 - val_mae: 1525.4354\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1505.3275 - mae: 1505.3275 - val_loss: 1492.3324 - val_mae: 1492.3324\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1472.4813 - mae: 1472.4813 - val_loss: 1462.2769 - val_mae: 1462.2769\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1439.2798 - mae: 1439.2798 - val_loss: 1429.9016 - val_mae: 1429.9016\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.4652 - mae: 1407.4652 - val_loss: 1398.5363 - val_mae: 1398.5363\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1377.0692 - mae: 1377.0692 - val_loss: 1368.0190 - val_mae: 1368.0190\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1346.9363 - mae: 1346.9363 - val_loss: 1337.4463 - val_mae: 1337.4463\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1317.8580 - mae: 1317.8580 - val_loss: 1309.9767 - val_mae: 1309.9767\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1288.1263 - mae: 1288.1263 - val_loss: 1281.7169 - val_mae: 1281.7169\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1259.7065 - mae: 1259.7065 - val_loss: 1252.4174 - val_mae: 1252.4174\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1231.6362 - mae: 1231.6362 - val_loss: 1225.9788 - val_mae: 1225.9788\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1207.9965 - mae: 1207.9965\n",
    "    --- Starting trial: run-13\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1566.5254 - mae: 1566.5254 - val_loss: 1404.3480 - val_mae: 1404.3480\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1265.8376 - mae: 1265.8376 - val_loss: 1180.4718 - val_mae: 1180.4718\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1098.5269 - mae: 1098.5269 - val_loss: 1044.3354 - val_mae: 1044.3354\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 978.0400 - mae: 978.0400 - val_loss: 939.7174 - val_mae: 939.7174\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 799.8273 - mae: 799.8273 - val_loss: 691.4581 - val_mae: 691.4581\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 776.6411 - mae: 776.6411 - val_loss: 809.2216 - val_mae: 809.2216\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 756.8688 - mae: 756.8688 - val_loss: 730.0041 - val_mae: 730.0041\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 683.1305 - mae: 683.1305 - val_loss: 490.2134 - val_mae: 490.2134\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 522.1832 - mae: 522.1832 - val_loss: 482.8342 - val_mae: 482.8342\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 530.3801 - mae: 530.3801 - val_loss: 555.1626 - val_mae: 555.1626\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 517.3920 - mae: 517.3920 - val_loss: 507.5374 - val_mae: 507.5374\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 461.6365 - mae: 461.6365 - val_loss: 705.4368 - val_mae: 705.4368\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 636.3152 - mae: 636.3152 - val_loss: 742.5924 - val_mae: 742.5924\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 449.6400 - mae: 449.6400 - val_loss: 799.6154 - val_mae: 799.6154\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 765.3070 - mae: 765.3070 - val_loss: 744.0755 - val_mae: 744.0755\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 839.9476 - mae: 839.9476 - val_loss: 857.7084 - val_mae: 857.7084\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 853.7109 - mae: 853.7109\n",
    "    --- Starting trial: run-14\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1714.8529 - mae: 1714.8529 - val_loss: 1698.8910 - val_mae: 1698.8910\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1677.5345 - mae: 1677.5345 - val_loss: 1661.2338 - val_mae: 1661.2338\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1642.2378 - mae: 1642.2378 - val_loss: 1627.2566 - val_mae: 1627.2566\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1607.4426 - mae: 1607.4426 - val_loss: 1592.7273 - val_mae: 1592.7273\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1572.9250 - mae: 1572.9250 - val_loss: 1559.6086 - val_mae: 1559.6086\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1538.4681 - mae: 1538.4681 - val_loss: 1524.4136 - val_mae: 1524.4136\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1505.1445 - mae: 1505.1445 - val_loss: 1493.8875 - val_mae: 1493.8875\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1472.4653 - mae: 1472.4653 - val_loss: 1460.0833 - val_mae: 1460.0833\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1439.9869 - mae: 1439.9869 - val_loss: 1427.9860 - val_mae: 1427.9860\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1408.8029 - mae: 1408.8029 - val_loss: 1399.9822 - val_mae: 1399.9822\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1377.9950 - mae: 1377.9950 - val_loss: 1366.0642 - val_mae: 1366.0642\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1348.1464 - mae: 1348.1464 - val_loss: 1337.9635 - val_mae: 1337.9635\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1318.6459 - mae: 1318.6459 - val_loss: 1308.7412 - val_mae: 1308.7412\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1289.9111 - mae: 1289.9111 - val_loss: 1278.3978 - val_mae: 1278.3978\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1261.1982 - mae: 1261.1982 - val_loss: 1253.7357 - val_mae: 1253.7357\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1232.6780 - mae: 1232.6780 - val_loss: 1224.5164 - val_mae: 1224.5164\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1209.4137 - mae: 1209.4137\n",
    "    --- Starting trial: run-15\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1562.0278 - mae: 1562.0278 - val_loss: 1406.2435 - val_mae: 1406.2435\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1268.8503 - mae: 1268.8503 - val_loss: 1149.9772 - val_mae: 1149.9772\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1038.7648 - mae: 1038.7648 - val_loss: 942.3271 - val_mae: 942.3271\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 840.6599 - mae: 840.6599 - val_loss: 775.3033 - val_mae: 775.3033\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 675.1769 - mae: 675.1769 - val_loss: 609.4270 - val_mae: 609.4270\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 541.7983 - mae: 541.7983 - val_loss: 507.8261 - val_mae: 507.8261\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 475.2537 - mae: 475.2537 - val_loss: 441.4498 - val_mae: 441.4498\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 447.5806 - mae: 447.5806 - val_loss: 422.3975 - val_mae: 422.3975\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 459.0667 - mae: 459.0667 - val_loss: 404.1974 - val_mae: 404.1974\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 364.6326 - mae: 364.6326 - val_loss: 350.4629 - val_mae: 350.4629\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 392.1742 - mae: 392.1742 - val_loss: 1082.8706 - val_mae: 1082.8706\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 363.4105 - mae: 363.4105 - val_loss: 332.4745 - val_mae: 332.4745\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 329.2193 - mae: 329.2193 - val_loss: 330.3246 - val_mae: 330.3246\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 320.8300 - mae: 320.8300 - val_loss: 319.8207 - val_mae: 319.8207\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 318.9174 - mae: 318.9174 - val_loss: 323.6231 - val_mae: 323.6231\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 316.3860 - mae: 316.3860 - val_loss: 311.9363 - val_mae: 311.9363\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 305.7241 - mae: 305.7241\n",
    "    --- Starting trial: run-16\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1694.1797 - mae: 1694.1797 - val_loss: 1662.0779 - val_mae: 1662.0779\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1623.5574 - mae: 1623.5574 - val_loss: 1592.0009 - val_mae: 1592.0009\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1556.7634 - mae: 1556.7634 - val_loss: 1528.4768 - val_mae: 1528.4768\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.7987 - mae: 1490.7987 - val_loss: 1463.1077 - val_mae: 1463.1077\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1427.0037 - mae: 1427.0037 - val_loss: 1402.7134 - val_mae: 1402.7134\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1367.0271 - mae: 1367.0271 - val_loss: 1344.2146 - val_mae: 1344.2146\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1309.4774 - mae: 1309.4774 - val_loss: 1288.8137 - val_mae: 1288.8137\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1253.0773 - mae: 1253.0773 - val_loss: 1232.4832 - val_mae: 1232.4832\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1198.2526 - mae: 1198.2526 - val_loss: 1178.3148 - val_mae: 1178.3148\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1146.0090 - mae: 1146.0090 - val_loss: 1128.2688 - val_mae: 1128.2688\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1095.0912 - mae: 1095.0912 - val_loss: 1078.5981 - val_mae: 1078.5981\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1047.3560 - mae: 1047.3560 - val_loss: 1032.5671 - val_mae: 1032.5671\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1001.1205 - mae: 1001.1205 - val_loss: 988.0533 - val_mae: 988.0533\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 955.5404 - mae: 955.5404 - val_loss: 944.2896 - val_mae: 944.2896\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 911.0602 - mae: 911.0602 - val_loss: 902.9487 - val_mae: 902.9487\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 867.1821 - mae: 867.1821 - val_loss: 858.6188 - val_mae: 858.6188\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 844.5689 - mae: 844.5689\n",
    "    --- Starting trial: run-17\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1419.4297 - mae: 1419.4297 - val_loss: 1196.6685 - val_mae: 1196.6685\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1006.7494 - mae: 1006.7494 - val_loss: 849.1498 - val_mae: 849.1498\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 796.4829 - mae: 796.4829 - val_loss: 821.0558 - val_mae: 821.0558\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 726.4808 - mae: 726.4808 - val_loss: 610.7658 - val_mae: 610.7658\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 501.3910 - mae: 501.3910 - val_loss: 518.0101 - val_mae: 518.0101\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 436.5090 - mae: 436.5090 - val_loss: 421.4387 - val_mae: 421.4387\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 429.1960 - mae: 429.1960 - val_loss: 496.3680 - val_mae: 496.3680\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 448.2284 - mae: 448.2284 - val_loss: 300.5168 - val_mae: 300.5168\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 425.0682 - mae: 425.0682 - val_loss: 308.5029 - val_mae: 308.5029\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 273.0026 - mae: 273.0026 - val_loss: 301.5456 - val_mae: 301.5456\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 263.3662 - mae: 263.3662 - val_loss: 266.9055 - val_mae: 266.9055\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 252.8967 - mae: 252.8967 - val_loss: 247.5253 - val_mae: 247.5253\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 430.7341 - mae: 430.7341 - val_loss: 333.0180 - val_mae: 333.0180\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 267.3344 - mae: 267.3344 - val_loss: 271.4555 - val_mae: 271.4555\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 266.0356 - mae: 266.0356 - val_loss: 268.0161 - val_mae: 268.0161\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 260.3858 - mae: 260.3858 - val_loss: 267.8378 - val_mae: 267.8378\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 260.0629 - mae: 260.0629\n",
    "    --- Starting trial: run-18\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1694.7997 - mae: 1694.7997 - val_loss: 1662.4231 - val_mae: 1662.4231\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1624.2465 - mae: 1624.2465 - val_loss: 1595.6680 - val_mae: 1595.6680\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1557.8837 - mae: 1557.8837 - val_loss: 1528.2695 - val_mae: 1528.2695\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1491.6995 - mae: 1491.6995 - val_loss: 1464.7749 - val_mae: 1464.7749\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1428.9586 - mae: 1428.9586 - val_loss: 1402.7351 - val_mae: 1402.7351\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1369.3558 - mae: 1369.3558 - val_loss: 1344.6344 - val_mae: 1344.6344\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1311.0494 - mae: 1311.0494 - val_loss: 1287.7323 - val_mae: 1287.7323\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1254.7527 - mae: 1254.7527 - val_loss: 1229.9874 - val_mae: 1229.9874\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1200.2535 - mae: 1200.2535 - val_loss: 1178.7148 - val_mae: 1178.7148\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1148.6194 - mae: 1148.6194 - val_loss: 1130.1833 - val_mae: 1130.1833\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1098.5969 - mae: 1098.5969 - val_loss: 1081.6409 - val_mae: 1081.6409\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1051.5981 - mae: 1051.5981 - val_loss: 1033.0592 - val_mae: 1033.0592\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1004.5767 - mae: 1004.5767 - val_loss: 987.8906 - val_mae: 987.8906\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 958.8726 - mae: 958.8726 - val_loss: 941.2385 - val_mae: 941.2385\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 914.5872 - mae: 914.5872 - val_loss: 896.4783 - val_mae: 896.4783\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 871.0402 - mae: 871.0402 - val_loss: 854.1886 - val_mae: 854.1886\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 841.2560 - mae: 841.2560\n",
    "    --- Starting trial: run-19\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1419.3757 - mae: 1419.3757 - val_loss: 1157.8914 - val_mae: 1157.8914\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 951.0938 - mae: 951.0938 - val_loss: 781.5115 - val_mae: 781.5115\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 612.8084 - mae: 612.8084 - val_loss: 458.9123 - val_mae: 458.9123\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 408.0029 - mae: 408.0029 - val_loss: 426.7612 - val_mae: 426.7612\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 375.3343 - mae: 375.3343 - val_loss: 342.4474 - val_mae: 342.4474\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 321.8578 - mae: 321.8578 - val_loss: 277.6904 - val_mae: 277.6904\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 305.2180 - mae: 305.2180 - val_loss: 283.7717 - val_mae: 283.7717\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 307.6875 - mae: 307.6875 - val_loss: 292.2432 - val_mae: 292.2432\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 391.0664 - mae: 391.0664 - val_loss: 422.2168 - val_mae: 422.2168\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 440.4038 - mae: 440.4038 - val_loss: 418.0160 - val_mae: 418.0160\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 439.6539 - mae: 439.6539 - val_loss: 422.3977 - val_mae: 422.3977\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 438.8077 - mae: 438.8077 - val_loss: 423.9795 - val_mae: 423.9795\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 437.6751 - mae: 437.6751 - val_loss: 417.5522 - val_mae: 417.5522\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 436.9357 - mae: 436.9357 - val_loss: 422.4628 - val_mae: 422.4628\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 437.2139 - mae: 437.2139 - val_loss: 422.5095 - val_mae: 422.5095\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 408.1948 - mae: 408.1948 - val_loss: 363.8466 - val_mae: 363.8466\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 356.8757 - mae: 356.8757\n",
    "    --- Starting trial: run-20\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1655.2758 - mae: 1655.2758 - val_loss: 1591.2709 - val_mae: 1591.2709\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1521.4144 - mae: 1521.4144 - val_loss: 1462.7307 - val_mae: 1462.7307\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1397.7123 - mae: 1397.7123 - val_loss: 1345.2852 - val_mae: 1345.2852\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1284.5764 - mae: 1284.5764 - val_loss: 1238.3728 - val_mae: 1238.3728\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1178.8931 - mae: 1178.8931 - val_loss: 1138.8553 - val_mae: 1138.8553\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1080.6917 - mae: 1080.6917 - val_loss: 1044.1903 - val_mae: 1044.1903\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 989.8049 - mae: 989.8049 - val_loss: 955.7908 - val_mae: 955.7908\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 903.2751 - mae: 903.2751 - val_loss: 873.7616 - val_mae: 873.7616\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 820.6246 - mae: 820.6246 - val_loss: 792.9376 - val_mae: 792.9376\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 743.0720 - mae: 743.0720 - val_loss: 724.5831 - val_mae: 724.5831\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 667.4967 - mae: 667.4967 - val_loss: 642.5319 - val_mae: 642.5319\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 592.8243 - mae: 592.8243 - val_loss: 568.1324 - val_mae: 568.1324\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 519.8990 - mae: 519.8990 - val_loss: 500.3583 - val_mae: 500.3583\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 449.8224 - mae: 449.8224 - val_loss: 437.3765 - val_mae: 437.3765\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 47s 37ms/step - loss: 382.6425 - mae: 382.6425 - val_loss: 362.7072 - val_mae: 362.7072\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 321.2919 - mae: 321.2919 - val_loss: 305.8570 - val_mae: 305.8570\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 296.7222 - mae: 296.7222\n",
    "    --- Starting trial: run-21\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1187.4447 - mae: 1187.4447 - val_loss: 799.1432 - val_mae: 799.1432\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 597.4485 - mae: 597.4485 - val_loss: 768.1321 - val_mae: 768.1321\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 479.0373 - mae: 479.0373 - val_loss: 363.7081 - val_mae: 363.7081\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 380.9041 - mae: 380.9041 - val_loss: 309.1804 - val_mae: 309.1804\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 267.3694 - mae: 267.3694 - val_loss: 313.9223 - val_mae: 313.9223\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 311.0589 - mae: 311.0589 - val_loss: 374.4903 - val_mae: 374.4903\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 274.8504 - mae: 274.8504 - val_loss: 271.0845 - val_mae: 271.0845\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 265.0187 - mae: 265.0187 - val_loss: 283.0406 - val_mae: 283.0406\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 256.0036 - mae: 256.0036 - val_loss: 266.4281 - val_mae: 266.4281\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 263.6881 - mae: 263.6881 - val_loss: 255.5882 - val_mae: 255.5882\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 269.9277 - mae: 269.9277 - val_loss: 255.0604 - val_mae: 255.0604\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 253.6220 - mae: 253.6220 - val_loss: 243.7174 - val_mae: 243.7174\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 248.7410 - mae: 248.7410 - val_loss: 297.6272 - val_mae: 297.6272\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 259.4736 - mae: 259.4736 - val_loss: 272.1282 - val_mae: 272.1282\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 260.2046 - mae: 260.2046 - val_loss: 260.6385 - val_mae: 260.6385\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 269.9621 - mae: 269.9621 - val_loss: 253.4625 - val_mae: 253.4625\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 246.4433 - mae: 246.4433\n",
    "    --- Starting trial: run-22\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1656.0427 - mae: 1656.0426 - val_loss: 1590.0323 - val_mae: 1590.0325\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1522.0487 - mae: 1522.0487 - val_loss: 1461.2520 - val_mae: 1461.2520\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1397.5829 - mae: 1397.5829 - val_loss: 1346.5229 - val_mae: 1346.5229\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1285.4709 - mae: 1285.4709 - val_loss: 1237.5023 - val_mae: 1237.5023\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1179.7264 - mae: 1179.7264 - val_loss: 1136.5604 - val_mae: 1136.5604\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1081.7936 - mae: 1081.7936 - val_loss: 1044.2214 - val_mae: 1044.2214\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 990.8118 - mae: 990.8118 - val_loss: 952.0539 - val_mae: 952.0539\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 903.2469 - mae: 903.2469 - val_loss: 870.6990 - val_mae: 870.6990\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 821.2315 - mae: 821.2314 - val_loss: 790.7815 - val_mae: 790.7815\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 742.0007 - mae: 742.0007 - val_loss: 713.6130 - val_mae: 713.6130\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 664.1683 - mae: 664.1683 - val_loss: 633.1598 - val_mae: 633.1598\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 587.0250 - mae: 587.0250 - val_loss: 560.1811 - val_mae: 560.1811\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 513.6300 - mae: 513.6300 - val_loss: 485.0017 - val_mae: 485.0017\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 441.8852 - mae: 441.8852 - val_loss: 420.1088 - val_mae: 420.1088\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 47s 37ms/step - loss: 375.7331 - mae: 375.7331 - val_loss: 348.5530 - val_mae: 348.5530\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 318.8253 - mae: 318.8253 - val_loss: 296.8869 - val_mae: 296.8869\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 288.9293 - mae: 288.9293\n",
    "    --- Starting trial: run-23\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1270.6874 - mae: 1270.6874 - val_loss: 817.3412 - val_mae: 817.3412\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 514.6238 - mae: 514.6238 - val_loss: 332.7230 - val_mae: 332.7230\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 330.9393 - mae: 330.9393 - val_loss: 289.5320 - val_mae: 289.5320\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 305.6551 - mae: 305.6551 - val_loss: 272.3338 - val_mae: 272.3338\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 318.8382 - mae: 318.8382 - val_loss: 311.9646 - val_mae: 311.9646\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 320.2411 - mae: 320.2411 - val_loss: 289.1891 - val_mae: 289.1891\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 319.9498 - mae: 319.9498 - val_loss: 305.3398 - val_mae: 305.3398\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 328.4742 - mae: 328.4742 - val_loss: 338.3555 - val_mae: 338.3555\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 357.2329 - mae: 357.2329 - val_loss: 463.6325 - val_mae: 463.6325\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 313.1739 - mae: 313.1739 - val_loss: 311.7986 - val_mae: 311.7986\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 318.0271 - mae: 318.0271 - val_loss: 332.7972 - val_mae: 332.7972\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 393.2340 - mae: 393.2340 - val_loss: 427.8026 - val_mae: 427.8026\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 380.4380 - mae: 380.4380 - val_loss: 297.6084 - val_mae: 297.6084\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 363.9304 - mae: 363.9304 - val_loss: 676.5594 - val_mae: 676.5594\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 331.8482 - mae: 331.8482 - val_loss: 332.9341 - val_mae: 332.9341\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 424.4282 - mae: 424.4282 - val_loss: 365.9059 - val_mae: 365.9059\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 359.0834 - mae: 359.0834\n",
    "    --- Starting trial: run-24\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.2584 - mae: 1715.2584 - val_loss: 1700.2073 - val_mae: 1700.2073\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1681.3557 - mae: 1681.3557 - val_loss: 1669.0344 - val_mae: 1669.0344\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1651.5637 - mae: 1651.5637 - val_loss: 1640.7041 - val_mae: 1640.7042\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1623.3107 - mae: 1623.3107 - val_loss: 1612.4390 - val_mae: 1612.4390\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1579.3289 - mae: 1579.3287 - val_loss: 1560.5695 - val_mae: 1560.5695\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1539.1229 - mae: 1539.1229 - val_loss: 1525.8325 - val_mae: 1525.8325\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1505.3556 - mae: 1505.3556 - val_loss: 1492.5293 - val_mae: 1492.5293\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1471.8590 - mae: 1471.8590 - val_loss: 1460.6884 - val_mae: 1460.6884\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1438.7396 - mae: 1438.7396 - val_loss: 1428.2408 - val_mae: 1428.2408\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.4236 - mae: 1407.4236 - val_loss: 1398.6946 - val_mae: 1398.6946\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1377.0978 - mae: 1377.0978 - val_loss: 1369.6388 - val_mae: 1369.6388\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1345.9254 - mae: 1345.9254 - val_loss: 1336.8884 - val_mae: 1336.8884\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1316.8832 - mae: 1316.8832 - val_loss: 1307.7303 - val_mae: 1307.7303\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1286.9031 - mae: 1286.9031 - val_loss: 1278.4175 - val_mae: 1278.4175\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1258.3693 - mae: 1258.3693 - val_loss: 1250.3518 - val_mae: 1250.3518\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1230.5686 - mae: 1230.5686 - val_loss: 1224.7064 - val_mae: 1224.7064\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1205.9163 - mae: 1205.9163\n",
    "    --- Starting trial: run-25\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1592.4969 - mae: 1592.4969 - val_loss: 1480.7988 - val_mae: 1480.7988\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1395.1003 - mae: 1395.1003 - val_loss: 1340.6558 - val_mae: 1340.6558\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1171.8480 - mae: 1171.8480 - val_loss: 1153.9695 - val_mae: 1153.9695\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1016.3562 - mae: 1016.3562 - val_loss: 963.1915 - val_mae: 963.1915\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 915.4506 - mae: 915.4506 - val_loss: 891.3950 - val_mae: 891.3950\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 780.9587 - mae: 780.9587 - val_loss: 733.4357 - val_mae: 733.4357\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 625.6290 - mae: 625.6290 - val_loss: 519.7130 - val_mae: 519.7130\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 581.6451 - mae: 581.6451 - val_loss: 631.0461 - val_mae: 631.0461\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 726.2726 - mae: 726.2726 - val_loss: 614.3286 - val_mae: 614.3286\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 437.8047 - mae: 437.8047 - val_loss: 364.6286 - val_mae: 364.6286\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 357.4409 - mae: 357.4409 - val_loss: 329.8905 - val_mae: 329.8905\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 284.4225 - mae: 284.4225 - val_loss: 339.6268 - val_mae: 339.6268\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.3744 - mae: 313.3744 - val_loss: 318.4585 - val_mae: 318.4585\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 516.9385 - mae: 516.9385 - val_loss: 583.9651 - val_mae: 583.9651\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 601.5540 - mae: 601.5540 - val_loss: 639.3000 - val_mae: 639.3000\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 555.8469 - mae: 555.8469 - val_loss: 570.8818 - val_mae: 570.8818\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 559.4802 - mae: 559.4802\n",
    "    --- Starting trial: run-26\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1716.0641 - mae: 1716.0641 - val_loss: 1701.7858 - val_mae: 1701.7858\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1680.3126 - mae: 1680.3126 - val_loss: 1664.3309 - val_mae: 1664.3309\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1643.3097 - mae: 1643.3097 - val_loss: 1630.5006 - val_mae: 1630.5006\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1607.8269 - mae: 1607.8269 - val_loss: 1595.0868 - val_mae: 1595.0868\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1573.8398 - mae: 1573.8398 - val_loss: 1560.7511 - val_mae: 1560.7511\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1538.8947 - mae: 1538.8947 - val_loss: 1526.4377 - val_mae: 1526.4377\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1505.6060 - mae: 1505.6060 - val_loss: 1495.1146 - val_mae: 1495.1146\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1472.7217 - mae: 1472.7217 - val_loss: 1460.9833 - val_mae: 1460.9833\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1440.0582 - mae: 1440.0582 - val_loss: 1429.7504 - val_mae: 1429.7504\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1409.5823 - mae: 1409.5823 - val_loss: 1398.1605 - val_mae: 1398.1605\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1388.9139 - mae: 1388.9139 - val_loss: 1367.7373 - val_mae: 1367.7373\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1351.1188 - mae: 1351.1188 - val_loss: 1339.1876 - val_mae: 1339.1876\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1318.9459 - mae: 1318.9459 - val_loss: 1310.9557 - val_mae: 1310.9557\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1289.9760 - mae: 1289.9760 - val_loss: 1278.6902 - val_mae: 1278.6902\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1261.5355 - mae: 1261.5355 - val_loss: 1251.9552 - val_mae: 1251.9552\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1233.4868 - mae: 1233.4868 - val_loss: 1223.1542 - val_mae: 1223.1542\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1208.6609 - mae: 1208.6609\n",
    "    --- Starting trial: run-27\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1567.7668 - mae: 1567.7668 - val_loss: 1406.9543 - val_mae: 1406.9543\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1264.6661 - mae: 1264.6661 - val_loss: 1143.7679 - val_mae: 1143.7679\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1029.7506 - mae: 1029.7506 - val_loss: 933.7031 - val_mae: 933.7031\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 847.8713 - mae: 847.8713 - val_loss: 776.5291 - val_mae: 776.5291\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 706.0790 - mae: 706.0790 - val_loss: 607.7781 - val_mae: 607.7781\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 510.8934 - mae: 510.8934 - val_loss: 418.7554 - val_mae: 418.7554\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 371.4506 - mae: 371.4506 - val_loss: 371.4606 - val_mae: 371.4606\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 532.1647 - mae: 532.1647 - val_loss: 349.2210 - val_mae: 349.2210\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 376.1600 - mae: 376.1600 - val_loss: 345.0703 - val_mae: 345.0703\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 411.3190 - mae: 411.3190 - val_loss: 401.2716 - val_mae: 401.2716\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 374.1356 - mae: 374.1356 - val_loss: 317.7419 - val_mae: 317.7419\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 403.0447 - mae: 403.0447 - val_loss: 327.2177 - val_mae: 327.2177\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 416.4857 - mae: 416.4857 - val_loss: 415.2970 - val_mae: 415.2970\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 448.0333 - mae: 448.0333 - val_loss: 414.7188 - val_mae: 414.7188\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 447.3336 - mae: 447.3336 - val_loss: 422.6076 - val_mae: 422.6076\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 517.6610 - mae: 517.6610 - val_loss: 509.3231 - val_mae: 509.3231\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 503.8761 - mae: 503.8761\n",
    "    --- Starting trial: run-28\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1696.7650 - mae: 1696.7650 - val_loss: 1668.9084 - val_mae: 1668.9084\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1628.7839 - mae: 1628.7839 - val_loss: 1593.6188 - val_mae: 1593.6188\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1556.1368 - mae: 1556.1368 - val_loss: 1526.3639 - val_mae: 1526.3639\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.2396 - mae: 1490.2396 - val_loss: 1461.8344 - val_mae: 1461.8344\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1426.0570 - mae: 1426.0570 - val_loss: 1402.6639 - val_mae: 1402.6639\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1366.0361 - mae: 1366.0361 - val_loss: 1343.7756 - val_mae: 1343.7756\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1308.5929 - mae: 1308.5929 - val_loss: 1286.7965 - val_mae: 1286.7965\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1251.5659 - mae: 1251.5659 - val_loss: 1233.4119 - val_mae: 1233.4119\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1197.6721 - mae: 1197.6721 - val_loss: 1180.1956 - val_mae: 1180.1956\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1145.1334 - mae: 1145.1334 - val_loss: 1128.7006 - val_mae: 1128.7006\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1095.2146 - mae: 1095.2146 - val_loss: 1081.0718 - val_mae: 1081.0718\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1046.3492 - mae: 1046.3492 - val_loss: 1033.7733 - val_mae: 1033.7733\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1001.0187 - mae: 1001.0187 - val_loss: 987.9163 - val_mae: 987.9163\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 955.1597 - mae: 955.1597 - val_loss: 945.1874 - val_mae: 945.1874\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 911.0806 - mae: 911.0806 - val_loss: 899.4464 - val_mae: 899.4464\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 867.8735 - mae: 867.8735 - val_loss: 858.7758 - val_mae: 858.7758\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 842.9398 - mae: 842.9398\n",
    "    --- Starting trial: run-29\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1471.6891 - mae: 1471.6891 - val_loss: 1200.9448 - val_mae: 1200.9448\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 978.2620 - mae: 978.2620 - val_loss: 786.5012 - val_mae: 786.5012\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 776.1443 - mae: 776.1443 - val_loss: 789.0593 - val_mae: 789.0593\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 656.6492 - mae: 656.6492 - val_loss: 644.4000 - val_mae: 644.4000\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 502.4906 - mae: 502.4906 - val_loss: 356.0527 - val_mae: 356.0527\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 389.9354 - mae: 389.9354 - val_loss: 413.3930 - val_mae: 413.3930\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 487.8478 - mae: 487.8478 - val_loss: 503.7323 - val_mae: 503.7323\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 500.8051 - mae: 500.8051 - val_loss: 503.9304 - val_mae: 503.9304\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 896.9134 - mae: 896.9134 - val_loss: 848.4318 - val_mae: 848.4318\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 694.9317 - mae: 694.9317 - val_loss: 598.9346 - val_mae: 598.9346\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 507.5535 - mae: 507.5535 - val_loss: 523.3628 - val_mae: 523.3628\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 501.6664 - mae: 501.6664 - val_loss: 497.1284 - val_mae: 497.1284\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 488.8983 - mae: 488.8983 - val_loss: 490.1184 - val_mae: 490.1184\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 487.4877 - mae: 487.4877 - val_loss: 490.1988 - val_mae: 490.1988\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 459.9962 - mae: 459.9962 - val_loss: 417.2057 - val_mae: 417.2057\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 414.3864 - mae: 414.3864 - val_loss: 416.7916 - val_mae: 416.7916\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 410.6290 - mae: 410.6290\n",
    "    --- Starting trial: run-30\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1697.3124 - mae: 1697.3124 - val_loss: 1669.5159 - val_mae: 1669.5159\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1628.5945 - mae: 1628.5945 - val_loss: 1595.8706 - val_mae: 1595.8706\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1557.4340 - mae: 1557.4340 - val_loss: 1527.5540 - val_mae: 1527.5540\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.7546 - mae: 1490.7546 - val_loss: 1463.7521 - val_mae: 1463.7521\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1427.8077 - mae: 1427.8077 - val_loss: 1401.4994 - val_mae: 1401.4994\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1367.8618 - mae: 1367.8618 - val_loss: 1340.8381 - val_mae: 1340.8381\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1309.7174 - mae: 1309.7174 - val_loss: 1287.9558 - val_mae: 1287.9558\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1254.0361 - mae: 1254.0361 - val_loss: 1232.8689 - val_mae: 1232.8689\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1199.3678 - mae: 1199.3678 - val_loss: 1179.3184 - val_mae: 1179.3184\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1147.4644 - mae: 1147.4644 - val_loss: 1125.9730 - val_mae: 1125.9730\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1097.3810 - mae: 1097.3810 - val_loss: 1077.9038 - val_mae: 1077.9038\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1049.7014 - mae: 1049.7014 - val_loss: 1032.2191 - val_mae: 1032.2191\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1002.9841 - mae: 1002.9841 - val_loss: 985.7662 - val_mae: 985.7662\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 1178.7196 - mae: 1178.7196 - val_loss: 1134.6793 - val_mae: 1134.6793\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 1080.5533 - mae: 1080.5533 - val_loss: 1040.9648 - val_mae: 1040.9648\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 990.7078 - mae: 990.7078 - val_loss: 955.0836 - val_mae: 955.0836\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 903.3972 - mae: 903.3972 - val_loss: 869.0078 - val_mae: 869.0078\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 820.5344 - mae: 820.5344 - val_loss: 786.7252 - val_mae: 786.7252\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 55s 43ms/step - loss: 741.6986 - mae: 741.6987 - val_loss: 710.1413 - val_mae: 710.1413\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 663.7181 - mae: 663.7181 - val_loss: 632.6093 - val_mae: 632.6093\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 586.7770 - mae: 586.7770 - val_loss: 556.9543 - val_mae: 556.9543\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 513.3420 - mae: 513.3420 - val_loss: 484.5912 - val_mae: 484.5912\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 441.1182 - mae: 441.1182 - val_loss: 411.9660 - val_mae: 411.9660\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 55s 44ms/step - loss: 375.5436 - mae: 375.5436 - val_loss: 346.7868 - val_mae: 346.7868\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 316.7939 - mae: 316.7939 - val_loss: 291.4937 - val_mae: 291.4937\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 283.1966 - mae: 283.1966\n",
    "    --- Starting trial: run-35\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 57s 43ms/step - loss: 1232.3571 - mae: 1232.3571 - val_loss: 783.3661 - val_mae: 783.3661\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 497.3005 - mae: 497.3005 - val_loss: 315.7192 - val_mae: 315.7192\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 286.1610 - mae: 286.1610 - val_loss: 312.7931 - val_mae: 312.7931\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 286.3627 - mae: 286.3627 - val_loss: 269.8783 - val_mae: 269.8783\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 281.8381 - mae: 281.8381 - val_loss: 272.8588 - val_mae: 272.8588\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 318.1495 - mae: 318.1495 - val_loss: 313.0517 - val_mae: 313.0517\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 333.5344 - mae: 333.5344 - val_loss: 304.4086 - val_mae: 304.4086\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 297.0162 - mae: 297.0162 - val_loss: 390.3582 - val_mae: 390.3582\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 307.0529 - mae: 307.0529 - val_loss: 476.4601 - val_mae: 476.4601\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 296.0189 - mae: 296.0189 - val_loss: 366.3567 - val_mae: 366.3567\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 322.2481 - mae: 322.2481 - val_loss: 526.5839 - val_mae: 526.5839\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 459.0078 - mae: 459.0078 - val_loss: 565.0197 - val_mae: 565.0197\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 405.0327 - mae: 405.0327 - val_loss: 375.6803 - val_mae: 375.6803\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 454.1462 - mae: 454.1462 - val_loss: 978.4724 - val_mae: 978.4724\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 600.6526 - mae: 600.6526 - val_loss: 426.8247 - val_mae: 426.8247\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 435.1583 - mae: 435.1583 - val_loss: 437.0663 - val_mae: 437.0663\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 432.2417 - mae: 432.2417\n",
    "    --- Starting trial: run-36\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1717.2101 - mae: 1717.2101 - val_loss: 1702.2744 - val_mae: 1702.2744\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1681.4932 - mae: 1681.4932 - val_loss: 1665.9336 - val_mae: 1665.9336\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1646.9165 - mae: 1646.9165 - val_loss: 1632.1249 - val_mae: 1632.1249\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1613.0847 - mae: 1613.0847 - val_loss: 1600.9336 - val_mae: 1600.9336\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1581.1233 - mae: 1581.1233 - val_loss: 1569.0088 - val_mae: 1569.0088\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1549.0660 - mae: 1549.0660 - val_loss: 1536.2501 - val_mae: 1536.2501\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1516.9631 - mae: 1516.9631 - val_loss: 1503.3112 - val_mae: 1503.3112\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1485.6986 - mae: 1485.6986 - val_loss: 1472.5837 - val_mae: 1472.5837\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1454.6257 - mae: 1454.6257 - val_loss: 1445.9840 - val_mae: 1445.9840\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1424.5015 - mae: 1424.5015 - val_loss: 1415.4459 - val_mae: 1415.4459\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1395.4667 - mae: 1395.4667 - val_loss: 1386.7228 - val_mae: 1386.7228\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1374.3759 - mae: 1374.3759 - val_loss: 1367.5306 - val_mae: 1367.5306\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1349.2841 - mae: 1349.2841 - val_loss: 1342.1028 - val_mae: 1342.1028\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1322.0649 - mae: 1322.0649 - val_loss: 1315.7899 - val_mae: 1315.7899\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1294.8787 - mae: 1294.8787 - val_loss: 1289.1993 - val_mae: 1289.1993\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1269.2714 - mae: 1269.2714 - val_loss: 1263.8138 - val_mae: 1263.8138\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1247.0769 - mae: 1247.0769\n",
    "    --- Starting trial: run-37\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1611.3224 - mae: 1611.3224 - val_loss: 1499.4774 - val_mae: 1499.4773\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1387.1478 - mae: 1387.1478 - val_loss: 1292.5542 - val_mae: 1292.5542\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1196.3809 - mae: 1196.3809 - val_loss: 1116.8970 - val_mae: 1116.8970\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1028.5817 - mae: 1028.5817 - val_loss: 959.9434 - val_mae: 959.9434\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 878.6913 - mae: 878.6913 - val_loss: 818.9588 - val_mae: 818.9588\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 771.8566 - mae: 771.8566 - val_loss: 743.7043 - val_mae: 743.7043\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 660.9505 - mae: 660.9505 - val_loss: 608.5946 - val_mae: 608.5946\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 532.9277 - mae: 532.9277 - val_loss: 529.6520 - val_mae: 529.6520\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 444.4644 - mae: 444.4644 - val_loss: 396.4323 - val_mae: 396.4323\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 363.3088 - mae: 363.3088 - val_loss: 335.2719 - val_mae: 335.2719\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 325.1737 - mae: 325.1737 - val_loss: 305.7590 - val_mae: 305.7590\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 246.6276 - mae: 246.6276 - val_loss: 233.7120 - val_mae: 233.7120\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 239.5803 - mae: 239.5803 - val_loss: 248.2155 - val_mae: 248.2155\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 224.4225 - mae: 224.4225 - val_loss: 228.9791 - val_mae: 228.9791\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 254.3052 - mae: 254.3052 - val_loss: 236.1215 - val_mae: 236.1215\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 241.6220 - mae: 241.6220 - val_loss: 239.1614 - val_mae: 239.1614\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 231.1915 - mae: 231.1915\n",
    "    --- Starting trial: run-38\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1715.2869 - mae: 1715.2869 - val_loss: 1698.2845 - val_mae: 1698.2845\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1676.3069 - mae: 1676.3069 - val_loss: 1660.4004 - val_mae: 1660.4004\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1639.7496 - mae: 1639.7496 - val_loss: 1624.7610 - val_mae: 1624.7610\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1605.3876 - mae: 1605.3876 - val_loss: 1590.8740 - val_mae: 1590.8740\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1571.2634 - mae: 1571.2634 - val_loss: 1557.4092 - val_mae: 1557.4092\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1536.8989 - mae: 1536.8989 - val_loss: 1524.5914 - val_mae: 1524.5914\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1502.9780 - mae: 1502.9780 - val_loss: 1491.7908 - val_mae: 1491.7908\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1470.3958 - mae: 1470.3958 - val_loss: 1456.8350 - val_mae: 1456.8350\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1437.9915 - mae: 1437.9915 - val_loss: 1425.9243 - val_mae: 1425.9243\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.0947 - mae: 1407.0947 - val_loss: 1394.1692 - val_mae: 1394.1692\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1376.3447 - mae: 1376.3447 - val_loss: 1365.2189 - val_mae: 1365.2189\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1346.2587 - mae: 1346.2587 - val_loss: 1335.6074 - val_mae: 1335.6074\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1317.1383 - mae: 1317.1383 - val_loss: 1306.1637 - val_mae: 1306.1637\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1288.7775 - mae: 1288.7775 - val_loss: 1290.1830 - val_mae: 1290.1830\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1272.1890 - mae: 1272.1890 - val_loss: 1260.7997 - val_mae: 1260.7997\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1244.9976 - mae: 1244.9976 - val_loss: 1236.9117 - val_mae: 1236.9117\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1218.4209 - mae: 1218.4209\n",
    "    --- Starting trial: run-39\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1585.4639 - mae: 1585.4639 - val_loss: 1449.8101 - val_mae: 1449.8101\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1326.8323 - mae: 1326.8323 - val_loss: 1219.2919 - val_mae: 1219.2919\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1116.0609 - mae: 1116.0609 - val_loss: 1021.7063 - val_mae: 1021.7063\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 935.3168 - mae: 935.3168 - val_loss: 847.9759 - val_mae: 847.9758\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 775.4478 - mae: 775.4478 - val_loss: 690.9235 - val_mae: 690.9235\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 626.6030 - mae: 626.6030 - val_loss: 558.0425 - val_mae: 558.0425\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 498.8426 - mae: 498.8426 - val_loss: 419.3124 - val_mae: 419.3124\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 384.6506 - mae: 384.6506 - val_loss: 347.4950 - val_mae: 347.4950\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 341.1142 - mae: 341.1142 - val_loss: 284.7704 - val_mae: 284.7704\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 307.8639 - mae: 307.8639 - val_loss: 241.8632 - val_mae: 241.8632\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 296.6828 - mae: 296.6828 - val_loss: 228.1328 - val_mae: 228.1328\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 311.5308 - mae: 311.5308 - val_loss: 251.6422 - val_mae: 251.6422\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.3402 - mae: 313.3402 - val_loss: 228.2218 - val_mae: 228.2218\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 295.0386 - mae: 295.0386 - val_loss: 223.6129 - val_mae: 223.6129\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 317.4360 - mae: 317.4360 - val_loss: 236.7501 - val_mae: 236.7501\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 325.2464 - mae: 325.2464 - val_loss: 279.0769 - val_mae: 279.0769\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 271.2676 - mae: 271.2676\n",
    "    --- Starting trial: run-40\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1695.5148 - mae: 1695.5148 - val_loss: 1662.8513 - val_mae: 1662.8513\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1626.9287 - mae: 1626.9287 - val_loss: 1600.3616 - val_mae: 1600.3616\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1564.5111 - mae: 1564.5111 - val_loss: 1540.4408 - val_mae: 1540.4408\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1503.3486 - mae: 1503.3486 - val_loss: 1478.1812 - val_mae: 1478.1812\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1443.7743 - mae: 1443.7743 - val_loss: 1419.5554 - val_mae: 1419.5554\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1387.6229 - mae: 1387.6229 - val_loss: 1365.7874 - val_mae: 1365.7874\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1332.5813 - mae: 1332.5813 - val_loss: 1311.7603 - val_mae: 1311.7603\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1279.4694 - mae: 1279.4694 - val_loss: 1261.5223 - val_mae: 1261.5223\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1227.0084 - mae: 1227.0084 - val_loss: 1210.1066 - val_mae: 1210.1066\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1178.0132 - mae: 1178.0132 - val_loss: 1161.7855 - val_mae: 1161.7855\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1129.8492 - mae: 1129.8492 - val_loss: 1113.4271 - val_mae: 1113.4271\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1083.3160 - mae: 1083.3160 - val_loss: 1071.1749 - val_mae: 1071.1749\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1038.5382 - mae: 1038.5382 - val_loss: 1026.0815 - val_mae: 1026.0815\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 995.6134 - mae: 995.6134 - val_loss: 985.9888 - val_mae: 985.9888\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 953.0128 - mae: 953.0128 - val_loss: 943.3952 - val_mae: 943.3952\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 911.9023 - mae: 911.9023 - val_loss: 904.7714 - val_mae: 904.7714\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 889.4129 - mae: 889.4129\n",
    "    --- Starting trial: run-41\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1493.8533 - mae: 1493.8533 - val_loss: 1287.2686 - val_mae: 1287.2686\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1116.1770 - mae: 1116.1770 - val_loss: 975.8002 - val_mae: 975.8002\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 831.2684 - mae: 831.2684 - val_loss: 713.1877 - val_mae: 713.1877\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 574.3075 - mae: 574.3075 - val_loss: 460.3633 - val_mae: 460.3633\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 356.9521 - mae: 356.9521 - val_loss: 315.4146 - val_mae: 315.4146\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 266.4378 - mae: 266.4378 - val_loss: 260.4943 - val_mae: 260.4943\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 223.5232 - mae: 223.5232 - val_loss: 229.5172 - val_mae: 229.5172\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 218.9276 - mae: 218.9276 - val_loss: 223.6136 - val_mae: 223.6136\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 214.6121 - mae: 214.6121 - val_loss: 220.9611 - val_mae: 220.9611\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 216.9231 - mae: 216.9231 - val_loss: 221.3165 - val_mae: 221.3165\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 216.4179 - mae: 216.4179 - val_loss: 250.1006 - val_mae: 250.1006\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 219.0348 - mae: 219.0348 - val_loss: 224.7102 - val_mae: 224.7102\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 220.1164 - mae: 220.1164 - val_loss: 243.0972 - val_mae: 243.0972\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 218.7108 - mae: 218.7108 - val_loss: 225.7091 - val_mae: 225.7091\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 230.5063 - mae: 230.5063 - val_loss: 224.8069 - val_mae: 224.8069\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 221.1390 - mae: 221.1390 - val_loss: 226.4967 - val_mae: 226.4967\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 219.0990 - mae: 219.0990\n",
    "    --- Starting trial: run-42\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1696.5499 - mae: 1696.5499 - val_loss: 1662.5385 - val_mae: 1662.5385\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1627.2061 - mae: 1627.2061 - val_loss: 1595.7772 - val_mae: 1595.7772\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1561.1919 - mae: 1561.1919 - val_loss: 1533.8206 - val_mae: 1533.8206\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1498.3416 - mae: 1498.3416 - val_loss: 1472.3705 - val_mae: 1472.3705\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1437.7375 - mae: 1437.7375 - val_loss: 1412.9568 - val_mae: 1412.9568\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1378.6720 - mae: 1378.6720 - val_loss: 1354.6799 - val_mae: 1354.6799\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1322.0428 - mae: 1322.0428 - val_loss: 1298.6385 - val_mae: 1298.6385\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1266.8304 - mae: 1266.8304 - val_loss: 1244.8041 - val_mae: 1244.8041\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1214.1224 - mae: 1214.1224 - val_loss: 1193.2323 - val_mae: 1193.2323\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1162.5565 - mae: 1162.5565 - val_loss: 1141.9067 - val_mae: 1141.9067\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1114.1790 - mae: 1114.1790 - val_loss: 1093.0789 - val_mae: 1093.0789\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1067.4868 - mae: 1067.4868 - val_loss: 1048.0905 - val_mae: 1048.0905\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1022.0480 - mae: 1022.0480 - val_loss: 1003.9880 - val_mae: 1003.9880\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 977.1545 - mae: 977.1545 - val_loss: 958.0012 - val_mae: 958.0012\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 933.5703 - mae: 933.5703 - val_loss: 916.3241 - val_mae: 916.3241\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 891.2424 - mae: 891.2424 - val_loss: 872.8399 - val_mae: 872.8399\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 861.5611 - mae: 861.5611\n",
    "    --- Starting trial: run-43\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1459.8247 - mae: 1459.8247 - val_loss: 1223.8055 - val_mae: 1223.8055\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1035.9802 - mae: 1035.9802 - val_loss: 872.6981 - val_mae: 872.6981\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 723.4987 - mae: 723.4987 - val_loss: 579.1658 - val_mae: 579.1658\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 452.6002 - mae: 452.6002 - val_loss: 339.4690 - val_mae: 339.4690\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 302.7859 - mae: 302.7859 - val_loss: 240.8830 - val_mae: 240.8830\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 275.8090 - mae: 275.8090 - val_loss: 227.5034 - val_mae: 227.5034\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 280.4904 - mae: 280.4904 - val_loss: 230.2850 - val_mae: 230.2850\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 271.3615 - mae: 271.3615 - val_loss: 217.9926 - val_mae: 217.9926\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 281.5079 - mae: 281.5079 - val_loss: 226.8006 - val_mae: 226.8006\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 293.2867 - mae: 293.2867 - val_loss: 234.4205 - val_mae: 234.4205\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 287.6664 - mae: 287.6664 - val_loss: 211.8952 - val_mae: 211.8952\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 301.7506 - mae: 301.7506 - val_loss: 247.1399 - val_mae: 247.1399\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 304.3917 - mae: 304.3917 - val_loss: 251.6075 - val_mae: 251.6075\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 316.4518 - mae: 316.4518 - val_loss: 258.4073 - val_mae: 258.4073\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 320.3954 - mae: 320.3954 - val_loss: 249.4064 - val_mae: 249.4064\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 321.5111 - mae: 321.5111 - val_loss: 241.1985 - val_mae: 241.1985\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 233.6783 - mae: 233.6783\n",
    "    --- Starting trial: run-44\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1658.6611 - mae: 1658.6611 - val_loss: 1596.2274 - val_mae: 1596.2274\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1532.4823 - mae: 1532.4823 - val_loss: 1478.1375 - val_mae: 1478.1375\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1416.1304 - mae: 1416.1304 - val_loss: 1366.5580 - val_mae: 1366.5580\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1310.2621 - mae: 1310.2621 - val_loss: 1266.9862 - val_mae: 1266.9862\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1209.6355 - mae: 1209.6355 - val_loss: 1171.2717 - val_mae: 1171.2717\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1116.1415 - mae: 1116.1415 - val_loss: 1081.3884 - val_mae: 1081.3884\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1029.4832 - mae: 1029.4832 - val_loss: 997.9949 - val_mae: 997.9949\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 946.7191 - mae: 946.7191 - val_loss: 917.0673 - val_mae: 917.0673\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 867.1848 - mae: 867.1848 - val_loss: 840.4197 - val_mae: 840.4197\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 791.6609 - mae: 791.6609 - val_loss: 766.9932 - val_mae: 766.9932\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 720.2026 - mae: 720.2026 - val_loss: 696.8099 - val_mae: 696.8099\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 649.1319 - mae: 649.1319 - val_loss: 626.9329 - val_mae: 626.9329\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 578.9605 - mae: 578.9605 - val_loss: 559.7176 - val_mae: 559.7176\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 514.1655 - mae: 514.1655 - val_loss: 506.8719 - val_mae: 506.8719\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 462.1168 - mae: 462.1168 - val_loss: 458.2577 - val_mae: 458.2577\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 411.0039 - mae: 411.0039 - val_loss: 385.6982 - val_mae: 385.6982\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 375.5765 - mae: 375.5765\n",
    "    --- Starting trial: run-45\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1301.4998 - mae: 1301.4998 - val_loss: 974.3832 - val_mae: 974.3832\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 743.1517 - mae: 743.1517 - val_loss: 538.9504 - val_mae: 538.9504\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 354.5157 - mae: 354.5157 - val_loss: 281.5479 - val_mae: 281.5479\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 227.7712 - mae: 227.7712 - val_loss: 230.4115 - val_mae: 230.4115\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 215.0101 - mae: 215.0101 - val_loss: 236.0885 - val_mae: 236.0885\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 212.4322 - mae: 212.4322 - val_loss: 219.3351 - val_mae: 219.3351\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 206.2629 - mae: 206.2629 - val_loss: 212.2724 - val_mae: 212.2724\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 208.5021 - mae: 208.5021 - val_loss: 230.5046 - val_mae: 230.5046\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 208.5500 - mae: 208.5500 - val_loss: 209.0724 - val_mae: 209.0724\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 195.6858 - mae: 195.6858 - val_loss: 195.0979 - val_mae: 195.0979\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 191.0587 - mae: 191.0587 - val_loss: 199.2634 - val_mae: 199.2634\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 187.6154 - mae: 187.6154 - val_loss: 233.6762 - val_mae: 233.6762\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 209.0860 - mae: 209.0860 - val_loss: 217.7801 - val_mae: 217.7801\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 220.1286 - mae: 220.1286 - val_loss: 230.8294 - val_mae: 230.8294\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 221.4493 - mae: 221.4493 - val_loss: 228.1542 - val_mae: 228.1542\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 228.5196 - mae: 228.5196 - val_loss: 243.7557 - val_mae: 243.7557\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 236.1976 - mae: 236.1976\n",
    "    --- Starting trial: run-46\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1658.5260 - mae: 1658.5260 - val_loss: 1597.6737 - val_mae: 1597.6737\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1531.5337 - mae: 1531.5337 - val_loss: 1471.7532 - val_mae: 1471.7532\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1410.5439 - mae: 1410.5439 - val_loss: 1357.5292 - val_mae: 1357.5292\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1299.5779 - mae: 1299.5779 - val_loss: 1253.6926 - val_mae: 1253.6926\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1196.8199 - mae: 1196.8199 - val_loss: 1152.6139 - val_mae: 1152.6139\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1102.1105 - mae: 1102.1105 - val_loss: 1060.9233 - val_mae: 1060.9233\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1013.0812 - mae: 1013.0812 - val_loss: 976.4382 - val_mae: 976.4382\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 928.9305 - mae: 928.9305 - val_loss: 892.9048 - val_mae: 892.9048\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 847.9688 - mae: 847.9688 - val_loss: 813.4162 - val_mae: 813.4162\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 774.0177 - mae: 774.0177 - val_loss: 743.8135 - val_mae: 743.8135\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 702.6194 - mae: 702.6195 - val_loss: 670.4092 - val_mae: 670.4092\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 628.5816 - mae: 628.5816 - val_loss: 597.4449 - val_mae: 597.4449\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 559.0278 - mae: 559.0278 - val_loss: 534.7897 - val_mae: 534.7897\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 495.0808 - mae: 495.0808 - val_loss: 465.4654 - val_mae: 465.4654\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 428.1409 - mae: 428.1409 - val_loss: 398.1586 - val_mae: 398.1586\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 366.6304 - mae: 366.6304 - val_loss: 336.1463 - val_mae: 336.1463\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 328.1187 - mae: 328.1187\n",
    "    --- Starting trial: run-47\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 35ms/step - loss: 1277.4482 - mae: 1277.4482 - val_loss: 912.3274 - val_mae: 912.3274\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 651.0007 - mae: 651.0007 - val_loss: 405.2114 - val_mae: 405.2114\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 295.3008 - mae: 295.3008 - val_loss: 210.4671 - val_mae: 210.4671\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 246.5305 - mae: 246.5305 - val_loss: 220.3249 - val_mae: 220.3249\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 251.0187 - mae: 251.0187 - val_loss: 234.6508 - val_mae: 234.6508\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 260.0192 - mae: 260.0192 - val_loss: 229.4008 - val_mae: 229.4008\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 266.8639 - mae: 266.8639 - val_loss: 215.5391 - val_mae: 215.5391\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 263.3572 - mae: 263.3572 - val_loss: 218.4511 - val_mae: 218.4511\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 272.1925 - mae: 272.1925 - val_loss: 209.1567 - val_mae: 209.1567\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 287.9868 - mae: 287.9868 - val_loss: 248.9987 - val_mae: 248.9987\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 295.4473 - mae: 295.4473 - val_loss: 236.4415 - val_mae: 236.4415\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 304.6299 - mae: 304.6299 - val_loss: 237.2237 - val_mae: 237.2237\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 319.2805 - mae: 319.2805 - val_loss: 248.2975 - val_mae: 248.2975\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 317.8445 - mae: 317.8445 - val_loss: 239.5460 - val_mae: 239.5460\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 320.9797 - mae: 320.9797 - val_loss: 247.0933 - val_mae: 247.0933\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 316.0599 - mae: 316.0599 - val_loss: 222.7614 - val_mae: 222.7614\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 217.4175 - mae: 217.4175\n",
    "    --- Starting trial: run-48\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.9551 - mae: 1715.9551 - val_loss: 1699.8214 - val_mae: 1699.8214\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1679.0156 - mae: 1679.0156 - val_loss: 1664.7561 - val_mae: 1664.7561\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1643.9565 - mae: 1643.9565 - val_loss: 1631.3694 - val_mae: 1631.3694\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1609.8430 - mae: 1609.8430 - val_loss: 1596.0638 - val_mae: 1596.0638\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1576.0675 - mae: 1576.0675 - val_loss: 1564.2960 - val_mae: 1564.2960\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1543.7927 - mae: 1543.7927 - val_loss: 1531.6519 - val_mae: 1531.6519\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1510.3280 - mae: 1510.3280 - val_loss: 1499.4088 - val_mae: 1499.4088\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1477.7925 - mae: 1477.7925 - val_loss: 1466.2664 - val_mae: 1466.2664\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1446.4912 - mae: 1446.4912 - val_loss: 1437.6030 - val_mae: 1437.6030\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1414.9462 - mae: 1414.9462 - val_loss: 1405.4879 - val_mae: 1405.4879\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1385.2902 - mae: 1385.2902 - val_loss: 1375.7491 - val_mae: 1375.7491\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1356.1321 - mae: 1356.1321 - val_loss: 1346.4485 - val_mae: 1346.4485\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1327.4646 - mae: 1327.4646 - val_loss: 1318.2596 - val_mae: 1318.2596\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1298.1749 - mae: 1298.1749 - val_loss: 1293.1355 - val_mae: 1293.1355\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1270.0986 - mae: 1270.0986 - val_loss: 1262.7316 - val_mae: 1262.7316\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1242.5336 - mae: 1242.5336 - val_loss: 1237.7620 - val_mae: 1237.7620\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1221.3234 - mae: 1221.3234\n",
    "    --- Starting trial: run-49\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1590.7982 - mae: 1590.7982 - val_loss: 1480.0334 - val_mae: 1480.0334\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1347.5516 - mae: 1347.5516 - val_loss: 1220.7795 - val_mae: 1220.7795\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1095.9360 - mae: 1095.9360 - val_loss: 973.3386 - val_mae: 973.3386\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 862.7107 - mae: 862.7107 - val_loss: 826.3100 - val_mae: 826.3100\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 728.9100 - mae: 728.9100 - val_loss: 646.0491 - val_mae: 646.0491\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 614.8599 - mae: 614.8599 - val_loss: 814.2495 - val_mae: 814.2495\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 640.9283 - mae: 640.9283 - val_loss: 885.9072 - val_mae: 885.9072\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 761.2232 - mae: 761.2232 - val_loss: 642.6960 - val_mae: 642.6960\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 540.6483 - mae: 540.6483 - val_loss: 521.8348 - val_mae: 521.8348\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 543.3502 - mae: 543.3502 - val_loss: 358.5876 - val_mae: 358.5876\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 281.2585 - mae: 281.2585 - val_loss: 264.4980 - val_mae: 264.4980\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 514.2418 - mae: 514.2418 - val_loss: 738.4224 - val_mae: 738.4224\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 686.8334 - mae: 686.8334 - val_loss: 480.1422 - val_mae: 480.1422\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 376.9387 - mae: 376.9387 - val_loss: 277.1323 - val_mae: 277.1323\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 313.5328 - mae: 313.5328 - val_loss: 286.8384 - val_mae: 286.8384\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 276.2646 - mae: 276.2646 - val_loss: 263.9634 - val_mae: 263.9634\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 257.0282 - mae: 257.0282\n",
    "    --- Starting trial: run-50\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1714.3967 - mae: 1714.3967 - val_loss: 1698.2332 - val_mae: 1698.2332\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1676.7014 - mae: 1676.7014 - val_loss: 1661.6172 - val_mae: 1661.6172\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1641.3059 - mae: 1641.3059 - val_loss: 1625.0699 - val_mae: 1625.0699\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1605.9208 - mae: 1605.9208 - val_loss: 1591.0507 - val_mae: 1591.0507\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1571.8026 - mae: 1571.8026 - val_loss: 1558.4971 - val_mae: 1558.4971\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1537.4042 - mae: 1537.4042 - val_loss: 1525.4352 - val_mae: 1525.4352\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1503.0554 - mae: 1503.0554 - val_loss: 1492.2814 - val_mae: 1492.2814\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1471.5179 - mae: 1471.5179 - val_loss: 1457.4338 - val_mae: 1457.4338\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1438.9426 - mae: 1438.9426 - val_loss: 1428.5422 - val_mae: 1428.5422\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1407.8379 - mae: 1407.8379 - val_loss: 1396.7726 - val_mae: 1396.7726\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1376.8347 - mae: 1376.8347 - val_loss: 1366.3796 - val_mae: 1366.3796\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1347.2869 - mae: 1347.2869 - val_loss: 1334.8883 - val_mae: 1334.8883\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1318.0419 - mae: 1318.0419 - val_loss: 1306.7230 - val_mae: 1306.7230\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1288.4674 - mae: 1288.4674 - val_loss: 1277.4142 - val_mae: 1277.4142\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1260.1821 - mae: 1260.1820 - val_loss: 1250.1243 - val_mae: 1250.1243\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1231.9685 - mae: 1231.9685 - val_loss: 1221.5571 - val_mae: 1221.5571\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1205.3240 - mae: 1205.3240\n",
    "    --- Starting trial: run-51\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1570.5490 - mae: 1570.5490 - val_loss: 1412.1570 - val_mae: 1412.1570\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1273.6757 - mae: 1273.6757 - val_loss: 1153.4180 - val_mae: 1153.4180\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1044.8888 - mae: 1044.8888 - val_loss: 946.6771 - val_mae: 946.6771\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 855.4116 - mae: 855.4116 - val_loss: 782.0162 - val_mae: 782.0162\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 683.1075 - mae: 683.1075 - val_loss: 586.1111 - val_mae: 586.1111\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 514.6006 - mae: 514.6006 - val_loss: 419.7685 - val_mae: 419.7685\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 368.9599 - mae: 368.9599 - val_loss: 303.8125 - val_mae: 303.8125\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 313.2482 - mae: 313.2482 - val_loss: 245.3420 - val_mae: 245.3420\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 297.4578 - mae: 297.4578 - val_loss: 263.6404 - val_mae: 263.6404\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 283.6970 - mae: 283.6970 - val_loss: 235.9379 - val_mae: 235.9379\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 277.9241 - mae: 277.9241 - val_loss: 258.3922 - val_mae: 258.3922\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 274.0790 - mae: 274.0790 - val_loss: 186.7096 - val_mae: 186.7096\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 319.7712 - mae: 319.7712 - val_loss: 302.3244 - val_mae: 302.3244\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 310.0781 - mae: 310.0781 - val_loss: 268.6696 - val_mae: 268.6696\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 281.9485 - mae: 281.9485 - val_loss: 180.2285 - val_mae: 180.2285\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 293.6541 - mae: 293.6541 - val_loss: 267.5230 - val_mae: 267.5230\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 261.1690 - mae: 261.1690\n",
    "    --- Starting trial: run-52\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1695.8501 - mae: 1695.8501 - val_loss: 1663.2485 - val_mae: 1663.2485\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1626.2513 - mae: 1626.2513 - val_loss: 1596.1555 - val_mae: 1596.1555\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1561.0544 - mae: 1561.0544 - val_loss: 1534.0005 - val_mae: 1534.0005\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1495.6290 - mae: 1495.6290 - val_loss: 1467.7705 - val_mae: 1467.7705\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1432.6783 - mae: 1432.6783 - val_loss: 1407.7566 - val_mae: 1407.7566\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1373.1915 - mae: 1373.1915 - val_loss: 1348.9343 - val_mae: 1348.9343\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1315.6998 - mae: 1315.6998 - val_loss: 1294.7925 - val_mae: 1294.7925\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1259.6613 - mae: 1259.6613 - val_loss: 1240.6241 - val_mae: 1240.6241\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1205.4904 - mae: 1205.4904 - val_loss: 1187.2410 - val_mae: 1187.2410\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1154.2507 - mae: 1154.2507 - val_loss: 1137.1331 - val_mae: 1137.1331\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1103.9575 - mae: 1103.9575 - val_loss: 1088.1261 - val_mae: 1088.1261\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1056.1140 - mae: 1056.1140 - val_loss: 1042.2157 - val_mae: 1042.2157\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1009.7570 - mae: 1009.7570 - val_loss: 997.8799 - val_mae: 997.8799\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 964.6146 - mae: 964.6146 - val_loss: 952.8818 - val_mae: 952.8818\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 920.4008 - mae: 920.4008 - val_loss: 909.0135 - val_mae: 909.0135\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 877.5481 - mae: 877.5481 - val_loss: 868.9973 - val_mae: 868.9973\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 854.8322 - mae: 854.8322\n",
    "    --- Starting trial: run-53\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1502.4844 - mae: 1502.4844 - val_loss: 1354.7896 - val_mae: 1354.7896\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1271.2701 - mae: 1271.2701 - val_loss: 1244.1079 - val_mae: 1244.1079\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1202.0615 - mae: 1202.0615 - val_loss: 1208.1639 - val_mae: 1208.1639\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1249.0465 - mae: 1249.0465 - val_loss: 1388.5922 - val_mae: 1388.5922\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1322.0587 - mae: 1322.0587 - val_loss: 1294.1733 - val_mae: 1294.1733\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1257.0309 - mae: 1257.0309 - val_loss: 1255.7302 - val_mae: 1255.7302\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1223.9701 - mae: 1223.9701 - val_loss: 1230.4979 - val_mae: 1230.4979\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1202.8103 - mae: 1202.8103 - val_loss: 1215.9810 - val_mae: 1215.9810\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1189.9863 - mae: 1189.9863 - val_loss: 1207.2212 - val_mae: 1207.2212\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1183.5328 - mae: 1183.5328 - val_loss: 1202.7959 - val_mae: 1202.7959\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1178.9757 - mae: 1178.9757 - val_loss: 1198.9844 - val_mae: 1198.9844\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1180.2412 - mae: 1180.2412 - val_loss: 1201.9044 - val_mae: 1201.9044\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1170.0922 - mae: 1170.0922 - val_loss: 1058.1254 - val_mae: 1058.1254\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 852.0829 - mae: 852.0829 - val_loss: 759.8188 - val_mae: 759.8188\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 821.8002 - mae: 821.8002 - val_loss: 580.1573 - val_mae: 580.1573\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 701.4338 - mae: 701.4338 - val_loss: 858.2007 - val_mae: 858.2007\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 852.1119 - mae: 852.1119\n",
    "    --- Starting trial: run-54\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1694.4193 - mae: 1694.4193 - val_loss: 1661.6246 - val_mae: 1661.6246\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1623.7301 - mae: 1623.7301 - val_loss: 1593.1334 - val_mae: 1593.1334\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1556.1296 - mae: 1556.1296 - val_loss: 1526.8265 - val_mae: 1526.8265\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1490.9556 - mae: 1490.9556 - val_loss: 1460.1119 - val_mae: 1460.1119\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1427.8076 - mae: 1427.8076 - val_loss: 1400.5409 - val_mae: 1400.5409\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1367.5930 - mae: 1367.5930 - val_loss: 1345.0804 - val_mae: 1345.0804\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1310.0184 - mae: 1310.0184 - val_loss: 1285.4628 - val_mae: 1285.4628\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1253.3188 - mae: 1253.3188 - val_loss: 1230.3567 - val_mae: 1230.3567\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1199.4473 - mae: 1199.4473 - val_loss: 1178.5710 - val_mae: 1178.5710\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1148.0098 - mae: 1148.0098 - val_loss: 1125.5715 - val_mae: 1125.5715\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1098.2299 - mae: 1098.2299 - val_loss: 1078.4414 - val_mae: 1078.4414\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1049.9579 - mae: 1049.9579 - val_loss: 1031.6129 - val_mae: 1031.6129\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1003.3875 - mae: 1003.3875 - val_loss: 985.7200 - val_mae: 985.7200\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 957.4155 - mae: 957.4155 - val_loss: 937.9058 - val_mae: 937.9058\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 913.8922 - mae: 913.8922 - val_loss: 894.8328 - val_mae: 894.8328\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 870.7062 - mae: 870.7062 - val_loss: 852.1217 - val_mae: 852.1217\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 838.9028 - mae: 838.9028\n",
    "    --- Starting trial: run-55\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1430.4091 - mae: 1430.4091 - val_loss: 1173.6382 - val_mae: 1173.6382\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 963.4750 - mae: 963.4750 - val_loss: 850.0145 - val_mae: 850.0145\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 619.6481 - mae: 619.6481 - val_loss: 448.4393 - val_mae: 448.4393\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 350.6143 - mae: 350.6143 - val_loss: 263.8476 - val_mae: 263.8476\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 324.3431 - mae: 324.3431 - val_loss: 259.9662 - val_mae: 259.9662\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 303.6106 - mae: 303.6106 - val_loss: 291.3770 - val_mae: 291.3770\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 289.1208 - mae: 289.1208 - val_loss: 246.4542 - val_mae: 246.4542\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 298.6770 - mae: 298.6770 - val_loss: 261.8775 - val_mae: 261.8775\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 284.3426 - mae: 284.3426 - val_loss: 248.1265 - val_mae: 248.1265\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 287.6434 - mae: 287.6434 - val_loss: 270.9297 - val_mae: 270.9297\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 282.9439 - mae: 282.9439 - val_loss: 219.0473 - val_mae: 219.0473\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 334.6888 - mae: 334.6888 - val_loss: 278.2180 - val_mae: 278.2180\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 335.8440 - mae: 335.8440 - val_loss: 332.3330 - val_mae: 332.3330\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 307.0844 - mae: 307.0844 - val_loss: 268.7868 - val_mae: 268.7868\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 285.3783 - mae: 285.3783 - val_loss: 276.0867 - val_mae: 276.0867\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 349.4684 - mae: 349.4684 - val_loss: 333.5100 - val_mae: 333.5100\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 327.2126 - mae: 327.2126\n",
    "    --- Starting trial: run-56\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1658.2850 - mae: 1658.2850 - val_loss: 1591.9558 - val_mae: 1591.9558\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1521.5365 - mae: 1521.5365 - val_loss: 1461.7190 - val_mae: 1461.7190\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1397.4489 - mae: 1397.4489 - val_loss: 1345.9897 - val_mae: 1345.9897\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1285.4169 - mae: 1285.4169 - val_loss: 1243.4918 - val_mae: 1243.4918\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1180.4349 - mae: 1180.4349 - val_loss: 1136.4615 - val_mae: 1136.4615\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1082.0282 - mae: 1082.0282 - val_loss: 1045.4163 - val_mae: 1045.4163\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 991.0557 - mae: 991.0557 - val_loss: 957.9186 - val_mae: 957.9186\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 904.5130 - mae: 904.5130 - val_loss: 872.7933 - val_mae: 872.7933\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 820.6494 - mae: 820.6494 - val_loss: 794.7302 - val_mae: 794.7302\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 742.6188 - mae: 742.6188 - val_loss: 716.9299 - val_mae: 716.9299\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 665.4910 - mae: 665.4910 - val_loss: 640.5123 - val_mae: 640.5123\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 589.0759 - mae: 589.0759 - val_loss: 566.1230 - val_mae: 566.1230\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 516.3492 - mae: 516.3492 - val_loss: 494.2713 - val_mae: 494.2713\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 443.9258 - mae: 443.9258 - val_loss: 428.8368 - val_mae: 428.8368\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 375.5524 - mae: 375.5524 - val_loss: 360.6385 - val_mae: 360.6385\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.4577 - mae: 313.4577 - val_loss: 297.9229 - val_mae: 297.9229\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 288.8310 - mae: 288.8310\n",
    "    --- Starting trial: run-57\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1393.8712 - mae: 1393.8712 - val_loss: 1165.2119 - val_mae: 1165.2119\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 913.7104 - mae: 913.7104 - val_loss: 701.9658 - val_mae: 701.9658\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 479.5779 - mae: 479.5779 - val_loss: 383.4006 - val_mae: 383.4006\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 290.8652 - mae: 290.8652 - val_loss: 266.3282 - val_mae: 266.3282\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 253.8491 - mae: 253.8491 - val_loss: 307.6183 - val_mae: 307.6183\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 293.0007 - mae: 293.0007 - val_loss: 260.0633 - val_mae: 260.0633\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 241.4846 - mae: 241.4846 - val_loss: 252.0758 - val_mae: 252.0758\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 236.6730 - mae: 236.6730 - val_loss: 253.3675 - val_mae: 253.3675\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 229.5158 - mae: 229.5158 - val_loss: 227.3736 - val_mae: 227.3736\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 219.7806 - mae: 219.7806 - val_loss: 227.4309 - val_mae: 227.4309\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 236.8613 - mae: 236.8613 - val_loss: 259.1495 - val_mae: 259.1495\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 247.9173 - mae: 247.9173 - val_loss: 247.2944 - val_mae: 247.2944\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 230.1233 - mae: 230.1233 - val_loss: 237.6126 - val_mae: 237.6126\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 234.4413 - mae: 234.4413 - val_loss: 245.7092 - val_mae: 245.7092\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 247.9096 - mae: 247.9096 - val_loss: 275.5294 - val_mae: 275.5294\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 279.2746 - mae: 279.2746 - val_loss: 264.5772 - val_mae: 264.5772\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 258.7753 - mae: 258.7753\n",
    "    --- Starting trial: run-58\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1654.1593 - mae: 1654.1593 - val_loss: 1588.5820 - val_mae: 1588.5820\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1520.9879 - mae: 1520.9879 - val_loss: 1462.1177 - val_mae: 1462.1177\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1397.7701 - mae: 1397.7701 - val_loss: 1342.3485 - val_mae: 1342.3485\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1284.4426 - mae: 1284.4426 - val_loss: 1234.8505 - val_mae: 1234.8505\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1178.6564 - mae: 1178.6564 - val_loss: 1134.4463 - val_mae: 1134.4463\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1081.3783 - mae: 1081.3783 - val_loss: 1043.1702 - val_mae: 1043.1702\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 990.3057 - mae: 990.3057 - val_loss: 951.9045 - val_mae: 951.9045\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 902.6321 - mae: 902.6321 - val_loss: 866.2329 - val_mae: 866.2329\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 820.3498 - mae: 820.3498 - val_loss: 786.3863 - val_mae: 786.3863\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 741.0311 - mae: 741.0311 - val_loss: 707.8280 - val_mae: 707.8280\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 663.2288 - mae: 663.2288 - val_loss: 630.3914 - val_mae: 630.3914\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 586.4191 - mae: 586.4191 - val_loss: 554.6180 - val_mae: 554.6180\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 512.5125 - mae: 512.5125 - val_loss: 481.3751 - val_mae: 481.3751\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 440.0503 - mae: 440.0503 - val_loss: 409.3201 - val_mae: 409.3201\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 374.1540 - mae: 374.1540 - val_loss: 344.3039 - val_mae: 344.3039\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 316.0261 - mae: 316.0261 - val_loss: 286.0162 - val_mae: 286.0162\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 277.9122 - mae: 277.9122\n",
    "    --- Starting trial: run-59\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1392.4385 - mae: 1392.4385 - val_loss: 1250.3171 - val_mae: 1250.3171\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1197.3577 - mae: 1197.3577 - val_loss: 1201.2832 - val_mae: 1201.2832\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1174.1700 - mae: 1174.1700 - val_loss: 1192.3412 - val_mae: 1192.3412\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1168.2170 - mae: 1168.2170 - val_loss: 1189.7781 - val_mae: 1189.7781\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1166.9709 - mae: 1166.9709 - val_loss: 1190.3672 - val_mae: 1190.3672\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1166.1785 - mae: 1166.1785 - val_loss: 1190.2751 - val_mae: 1190.2751\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.4803 - mae: 1166.4803 - val_loss: 1190.5962 - val_mae: 1190.5962\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1166.8518 - mae: 1166.8518 - val_loss: 1190.4844 - val_mae: 1190.4844\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.3948 - mae: 1166.3948 - val_loss: 1189.2499 - val_mae: 1189.2499\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.2703 - mae: 1166.2703 - val_loss: 1191.2209 - val_mae: 1191.2209\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1046.9111 - mae: 1046.9111 - val_loss: 830.7166 - val_mae: 830.7166\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 658.5361 - mae: 658.5361 - val_loss: 480.1817 - val_mae: 480.1817\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 381.9342 - mae: 381.9342 - val_loss: 744.7347 - val_mae: 744.7347\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 320.2578 - mae: 320.2578 - val_loss: 249.4001 - val_mae: 249.4001\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 287.5827 - mae: 287.5827 - val_loss: 250.1878 - val_mae: 250.1878\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 280.8719 - mae: 280.8719 - val_loss: 294.1183 - val_mae: 294.1183\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 287.3031 - mae: 287.3031\n",
    "    --- Starting trial: run-60\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1715.2748 - mae: 1715.2748 - val_loss: 1700.8225 - val_mae: 1700.8225\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1681.0942 - mae: 1681.0942 - val_loss: 1669.6406 - val_mae: 1669.6406\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1651.2910 - mae: 1651.2910 - val_loss: 1642.4690 - val_mae: 1642.4690\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1623.7871 - mae: 1623.7871 - val_loss: 1614.1747 - val_mae: 1614.1747\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1596.3390 - mae: 1596.3390 - val_loss: 1589.2986 - val_mae: 1589.2986\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1568.7782 - mae: 1568.7782 - val_loss: 1561.4919 - val_mae: 1561.4919\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1541.7036 - mae: 1541.7036 - val_loss: 1500.9530 - val_mae: 1500.9530\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1477.8937 - mae: 1477.8937 - val_loss: 1462.9747 - val_mae: 1462.9747\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1443.2484 - mae: 1443.2484 - val_loss: 1431.3762 - val_mae: 1431.3762\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1410.7377 - mae: 1410.7377 - val_loss: 1401.5405 - val_mae: 1401.5405\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1379.5552 - mae: 1379.5552 - val_loss: 1370.0154 - val_mae: 1370.0154\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1349.6385 - mae: 1349.6385 - val_loss: 1341.7344 - val_mae: 1341.7344\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1357.4172 - mae: 1357.4172 - val_loss: 1354.0046 - val_mae: 1354.0046\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1332.7262 - mae: 1332.7262 - val_loss: 1323.8184 - val_mae: 1323.8184\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1303.0415 - mae: 1303.0415 - val_loss: 1296.4604 - val_mae: 1296.4604\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1271.9946 - mae: 1271.9946 - val_loss: 1259.4896 - val_mae: 1259.4896\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1244.1625 - mae: 1244.1625\n",
    "    --- Starting trial: run-61\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1591.4929 - mae: 1591.4929 - val_loss: 1480.5995 - val_mae: 1480.5995\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1394.2026 - mae: 1394.2026 - val_loss: 1341.5234 - val_mae: 1341.5234\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1285.2896 - mae: 1285.2896 - val_loss: 1270.2513 - val_mae: 1270.2513\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1231.3619 - mae: 1231.3619 - val_loss: 1233.0593 - val_mae: 1233.0593\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1201.1576 - mae: 1201.1576 - val_loss: 1212.6584 - val_mae: 1212.6584\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1186.1936 - mae: 1186.1936 - val_loss: 1203.3052 - val_mae: 1203.3052\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1179.8710 - mae: 1179.8710 - val_loss: 1198.6248 - val_mae: 1198.6248\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1175.8376 - mae: 1175.8376 - val_loss: 1196.5792 - val_mae: 1196.5792\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1173.0054 - mae: 1173.0054 - val_loss: 1194.4427 - val_mae: 1194.4427\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1171.1752 - mae: 1171.1752 - val_loss: 1193.2313 - val_mae: 1193.2313\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1168.9563 - mae: 1168.9563 - val_loss: 1192.1876 - val_mae: 1192.1876\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1167.9323 - mae: 1167.9323 - val_loss: 1190.2078 - val_mae: 1190.2078\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1167.0031 - mae: 1167.0031 - val_loss: 1190.0613 - val_mae: 1190.0613\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.2233 - mae: 1166.2233 - val_loss: 1190.4324 - val_mae: 1190.4324\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1165.9617 - mae: 1165.9617 - val_loss: 1189.8519 - val_mae: 1189.8519\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1165.9121 - mae: 1165.9121 - val_loss: 1189.7178 - val_mae: 1189.7178\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1181.1914 - mae: 1181.1914\n",
    "    --- Starting trial: run-62\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1715.9768 - mae: 1715.9768 - val_loss: 1700.1365 - val_mae: 1700.1365\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1682.1923 - mae: 1682.1923 - val_loss: 1668.7361 - val_mae: 1668.7361\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1645.0662 - mae: 1645.0662 - val_loss: 1629.2726 - val_mae: 1629.2726\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1608.1929 - mae: 1608.1929 - val_loss: 1594.8641 - val_mae: 1594.8641\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1573.3793 - mae: 1573.3793 - val_loss: 1558.7391 - val_mae: 1558.7391\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1538.6969 - mae: 1538.6969 - val_loss: 1527.7262 - val_mae: 1527.7262\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1505.5537 - mae: 1505.5537 - val_loss: 1491.9905 - val_mae: 1491.9905\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1472.1675 - mae: 1472.1675 - val_loss: 1460.4608 - val_mae: 1460.4608\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1440.3983 - mae: 1440.3983 - val_loss: 1429.3096 - val_mae: 1429.3094\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1408.9263 - mae: 1408.9263 - val_loss: 1397.1091 - val_mae: 1397.1091\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1378.8611 - mae: 1378.8611 - val_loss: 1366.4390 - val_mae: 1366.4390\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1347.7401 - mae: 1347.7401 - val_loss: 1338.5714 - val_mae: 1338.5714\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1318.4534 - mae: 1318.4534 - val_loss: 1307.1971 - val_mae: 1307.1971\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1289.7780 - mae: 1289.7780 - val_loss: 1281.5234 - val_mae: 1281.5234\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1260.6885 - mae: 1260.6885 - val_loss: 1252.1787 - val_mae: 1252.1787\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1232.7468 - mae: 1232.7468 - val_loss: 1222.5785 - val_mae: 1222.5785\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1206.2313 - mae: 1206.2313\n",
    "    --- Starting trial: run-63\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1595.6200 - mae: 1595.6200 - val_loss: 1487.7579 - val_mae: 1487.7579\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1400.6389 - mae: 1400.6389 - val_loss: 1343.9597 - val_mae: 1343.9596\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1288.3634 - mae: 1288.3634 - val_loss: 1269.7526 - val_mae: 1269.7526\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1233.0900 - mae: 1233.0900 - val_loss: 1232.9026 - val_mae: 1232.9026\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1203.7236 - mae: 1203.7236 - val_loss: 1213.1016 - val_mae: 1213.1016\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1289.3771 - mae: 1289.3771 - val_loss: 1215.4357 - val_mae: 1215.4357\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1191.6283 - mae: 1191.6283 - val_loss: 1206.5406 - val_mae: 1206.5406\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1174.5946 - mae: 1174.5946 - val_loss: 1176.1923 - val_mae: 1176.1923\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 878.6800 - mae: 878.6800 - val_loss: 616.5504 - val_mae: 616.5504\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 505.5520 - mae: 505.5520 - val_loss: 408.8690 - val_mae: 408.8690\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 478.2428 - mae: 478.2428 - val_loss: 446.8481 - val_mae: 446.8481\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 472.9128 - mae: 472.9128 - val_loss: 337.6688 - val_mae: 337.6688\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 344.9160 - mae: 344.9160 - val_loss: 285.9080 - val_mae: 285.9080\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 332.0251 - mae: 332.0251 - val_loss: 276.5626 - val_mae: 276.5626\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 368.0942 - mae: 368.0942 - val_loss: 338.9963 - val_mae: 338.9963\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 382.4677 - mae: 382.4677 - val_loss: 347.5350 - val_mae: 347.5350\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 340.6219 - mae: 340.6219\n",
    "    --- Starting trial: run-64\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1697.7865 - mae: 1697.7865 - val_loss: 1669.7599 - val_mae: 1669.7599\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1635.1888 - mae: 1635.1888 - val_loss: 1597.5497 - val_mae: 1597.5497\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1559.0917 - mae: 1559.0917 - val_loss: 1526.9058 - val_mae: 1526.9058\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.9906 - mae: 1490.9906 - val_loss: 1462.7157 - val_mae: 1462.7157\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1427.6451 - mae: 1427.6451 - val_loss: 1404.5392 - val_mae: 1404.5392\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1367.3839 - mae: 1367.3839 - val_loss: 1345.6179 - val_mae: 1345.6179\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1309.5031 - mae: 1309.5031 - val_loss: 1286.0879 - val_mae: 1286.0879\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1251.9056 - mae: 1251.9056 - val_loss: 1230.3121 - val_mae: 1230.3121\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1197.0972 - mae: 1197.0972 - val_loss: 1179.2968 - val_mae: 1179.2968\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1144.1387 - mae: 1144.1387 - val_loss: 1126.5173 - val_mae: 1126.5173\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1094.0553 - mae: 1094.0553 - val_loss: 1079.6848 - val_mae: 1079.6848\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1045.9073 - mae: 1045.9073 - val_loss: 1031.7556 - val_mae: 1031.7556\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 999.1041 - mae: 999.1041 - val_loss: 987.5872 - val_mae: 987.5872\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 953.2366 - mae: 953.2366 - val_loss: 942.5939 - val_mae: 942.5939\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 908.8012 - mae: 908.8012 - val_loss: 896.9649 - val_mae: 896.9649\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 865.8039 - mae: 865.8039 - val_loss: 854.4534 - val_mae: 854.4534\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 841.1807 - mae: 841.1807\n",
    "    --- Starting trial: run-65\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1500.3896 - mae: 1500.3896 - val_loss: 1353.6429 - val_mae: 1353.6429\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1269.3514 - mae: 1269.3514 - val_loss: 1242.7375 - val_mae: 1242.7375\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1201.1636 - mae: 1201.1636 - val_loss: 1208.4069 - val_mae: 1208.4069\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1296.0450 - mae: 1296.0450 - val_loss: 1272.9860 - val_mae: 1272.9860\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1065.6682 - mae: 1065.6682 - val_loss: 937.1535 - val_mae: 937.1535\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 863.2352 - mae: 863.2352 - val_loss: 825.2983 - val_mae: 825.2983\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 781.0197 - mae: 781.0197 - val_loss: 761.8275 - val_mae: 761.8275\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 548.8966 - mae: 548.8966 - val_loss: 922.2382 - val_mae: 922.2382\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 721.9518 - mae: 721.9518 - val_loss: 674.2233 - val_mae: 674.2233\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 428.9033 - mae: 428.9033 - val_loss: 310.6823 - val_mae: 310.6823\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 228.6779 - mae: 228.6779 - val_loss: 232.3949 - val_mae: 232.3949\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 319.3440 - mae: 319.3440 - val_loss: 268.2849 - val_mae: 268.2849\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 245.7328 - mae: 245.7328 - val_loss: 249.9334 - val_mae: 249.9334\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 309.2064 - mae: 309.2064 - val_loss: 295.4243 - val_mae: 295.4243\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 409.6797 - mae: 409.6797 - val_loss: 697.2623 - val_mae: 697.2623\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 595.1288 - mae: 595.1288 - val_loss: 505.8657 - val_mae: 505.8657\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 496.2920 - mae: 496.2920\n",
    "    --- Starting trial: run-66\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1696.4565 - mae: 1696.4565 - val_loss: 1669.0969 - val_mae: 1669.0969\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1637.5795 - mae: 1637.5795 - val_loss: 1617.6250 - val_mae: 1617.6250\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1586.1687 - mae: 1586.1687 - val_loss: 1565.0692 - val_mae: 1565.0692\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1536.2133 - mae: 1536.2133 - val_loss: 1520.1093 - val_mae: 1520.1093\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1462.5615 - mae: 1462.5615 - val_loss: 1415.0160 - val_mae: 1415.0160\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1378.6464 - mae: 1378.6464 - val_loss: 1353.4991 - val_mae: 1353.4991\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1320.3718 - mae: 1320.3718 - val_loss: 1295.7448 - val_mae: 1295.7448\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1262.8053 - mae: 1262.8053 - val_loss: 1240.6733 - val_mae: 1240.6733\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1208.9750 - mae: 1208.9750 - val_loss: 1188.1603 - val_mae: 1188.1603\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1157.9777 - mae: 1157.9777 - val_loss: 1138.0925 - val_mae: 1138.0925\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1108.5146 - mae: 1108.5146 - val_loss: 1089.1493 - val_mae: 1089.1493\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1061.1853 - mae: 1061.1853 - val_loss: 1044.2592 - val_mae: 1044.2592\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1014.6068 - mae: 1014.6068 - val_loss: 996.2401 - val_mae: 996.2401\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 969.8047 - mae: 969.8047 - val_loss: 951.8912 - val_mae: 951.8912\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 925.5235 - mae: 925.5235 - val_loss: 908.1586 - val_mae: 908.1586\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 882.4880 - mae: 882.4880 - val_loss: 864.4131 - val_mae: 864.4131\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 851.2615 - mae: 851.2615\n",
    "    --- Starting trial: run-67\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1500.5712 - mae: 1500.5712 - val_loss: 1352.8546 - val_mae: 1352.8546\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1291.4684 - mae: 1291.4684 - val_loss: 1489.7858 - val_mae: 1489.7858\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.3368 - mae: 1407.3368 - val_loss: 1360.9995 - val_mae: 1360.9995\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1302.7561 - mae: 1302.7561 - val_loss: 1276.9496 - val_mae: 1276.9496\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1122.5923 - mae: 1122.5923 - val_loss: 975.3730 - val_mae: 975.3730\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 880.1927 - mae: 880.1927 - val_loss: 797.9919 - val_mae: 797.9919\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 733.1785 - mae: 733.1785 - val_loss: 678.7579 - val_mae: 678.7579\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 612.8644 - mae: 612.8644 - val_loss: 610.0171 - val_mae: 610.0171\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 460.0597 - mae: 460.0597 - val_loss: 343.3179 - val_mae: 343.3179\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 379.8155 - mae: 379.8155 - val_loss: 320.0001 - val_mae: 320.0001\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 373.2209 - mae: 373.2209 - val_loss: 310.9376 - val_mae: 310.9376\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 366.4509 - mae: 366.4509 - val_loss: 327.1859 - val_mae: 327.1859\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 365.6005 - mae: 365.6005 - val_loss: 460.3434 - val_mae: 460.3434\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 390.1715 - mae: 390.1715 - val_loss: 334.4388 - val_mae: 334.4388\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 376.8595 - mae: 376.8595 - val_loss: 335.8213 - val_mae: 335.8213\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 381.7434 - mae: 381.7434 - val_loss: 339.1835 - val_mae: 339.1835\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 332.5205 - mae: 332.5205\n",
    "    --- Starting trial: run-68\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 60s 46ms/step - loss: 1663.6064 - mae: 1663.6064 - val_loss: 1614.4357 - val_mae: 1614.4357\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1559.5764 - mae: 1559.5764 - val_loss: 1519.3787 - val_mae: 1519.3787\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1467.7078 - mae: 1467.7078 - val_loss: 1398.7457 - val_mae: 1398.7457\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1328.1588 - mae: 1328.1588 - val_loss: 1276.2462 - val_mae: 1276.2462\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1214.9052 - mae: 1214.9052 - val_loss: 1171.7322 - val_mae: 1171.7322\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1118.0210 - mae: 1118.0210 - val_loss: 1081.5664 - val_mae: 1081.5664\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1029.3809 - mae: 1029.3809 - val_loss: 998.2254 - val_mae: 998.2254\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 946.2430 - mae: 946.2430 - val_loss: 918.2692 - val_mae: 918.2692\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 866.0884 - mae: 866.0884 - val_loss: 838.9724 - val_mae: 838.9724\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 791.7746 - mae: 791.7746 - val_loss: 768.1389 - val_mae: 768.1389\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 718.3478 - mae: 718.3478 - val_loss: 695.4339 - val_mae: 695.4339\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 647.2088 - mae: 647.2088 - val_loss: 628.5600 - val_mae: 628.5600\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 576.3525 - mae: 576.3525 - val_loss: 553.3524 - val_mae: 553.3524\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 507.9065 - mae: 507.9065 - val_loss: 491.3889 - val_mae: 491.3889\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 441.0811 - mae: 441.0811 - val_loss: 421.3066 - val_mae: 421.3066\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 378.0339 - mae: 378.0339 - val_loss: 362.7409 - val_mae: 362.7409\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 353.1181 - mae: 353.1181\n",
    "    --- Starting trial: run-69\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 60s 46ms/step - loss: 1386.4313 - mae: 1386.4313 - val_loss: 1243.6243 - val_mae: 1243.6243\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1193.9862 - mae: 1193.9862 - val_loss: 1201.1508 - val_mae: 1201.1508\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1173.7668 - mae: 1173.7668 - val_loss: 1193.4762 - val_mae: 1193.4762\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1167.6324 - mae: 1167.6324 - val_loss: 1189.1149 - val_mae: 1189.1149\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.1710 - mae: 1166.1710 - val_loss: 1188.8170 - val_mae: 1188.8170\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1166.0034 - mae: 1166.0034 - val_loss: 1189.7418 - val_mae: 1189.7418\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1165.9618 - mae: 1165.9618 - val_loss: 1190.1847 - val_mae: 1190.1847\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.0416 - mae: 1166.0416 - val_loss: 1190.1637 - val_mae: 1190.1637\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.8226 - mae: 1165.8226 - val_loss: 1190.2715 - val_mae: 1190.2715\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1166.2115 - mae: 1166.2115 - val_loss: 1191.0272 - val_mae: 1191.0272\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1165.6149 - mae: 1165.6149 - val_loss: 1189.4729 - val_mae: 1189.4729\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.9915 - mae: 1165.9915 - val_loss: 1190.9021 - val_mae: 1190.9021\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.8752 - mae: 1165.8752 - val_loss: 1188.5698 - val_mae: 1188.5698\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.0256 - mae: 1166.0256 - val_loss: 1189.5872 - val_mae: 1189.5872\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.8643 - mae: 1165.8643 - val_loss: 1189.8784 - val_mae: 1189.8784\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.1276 - mae: 1166.1276 - val_loss: 1190.4354 - val_mae: 1190.4354\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1183.3245 - mae: 1183.3245\n",
    "    --- Starting trial: run-70\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 63s 48ms/step - loss: 1664.3086 - mae: 1664.3086 - val_loss: 1599.2139 - val_mae: 1599.2139\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1526.7667 - mae: 1526.7668 - val_loss: 1463.8284 - val_mae: 1463.8284\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1398.5803 - mae: 1398.5803 - val_loss: 1343.9360 - val_mae: 1343.9360\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1283.9706 - mae: 1283.9706 - val_loss: 1237.0625 - val_mae: 1237.0625\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1178.6718 - mae: 1178.6718 - val_loss: 1135.2367 - val_mae: 1135.2367\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1081.6058 - mae: 1081.6058 - val_loss: 1041.8905 - val_mae: 1041.8905\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 990.2513 - mae: 990.2513 - val_loss: 952.1376 - val_mae: 952.1376\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 903.1245 - mae: 903.1245 - val_loss: 865.3026 - val_mae: 865.3026\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 820.2087 - mae: 820.2087 - val_loss: 784.1815 - val_mae: 784.1815\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 741.3391 - mae: 741.3391 - val_loss: 707.8594 - val_mae: 707.8594\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 663.3409 - mae: 663.3409 - val_loss: 630.2474 - val_mae: 630.2474\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 585.9130 - mae: 585.9130 - val_loss: 553.8346 - val_mae: 553.8346\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 512.3198 - mae: 512.3198 - val_loss: 479.7880 - val_mae: 479.7880\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 440.6092 - mae: 440.6092 - val_loss: 410.9050 - val_mae: 410.9050\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 374.1082 - mae: 374.1082 - val_loss: 348.0106 - val_mae: 348.0106\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 315.7215 - mae: 315.7215 - val_loss: 286.8600 - val_mae: 286.8600\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 277.8535 - mae: 277.8535\n",
    "    --- Starting trial: run-71\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 63s 48ms/step - loss: 1392.1804 - mae: 1392.1804 - val_loss: 1252.0527 - val_mae: 1252.0527\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1197.3865 - mae: 1197.3865 - val_loss: 1199.4036 - val_mae: 1199.4036\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1173.7020 - mae: 1173.7020 - val_loss: 1193.5513 - val_mae: 1193.5513\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1168.0837 - mae: 1168.0837 - val_loss: 1190.2594 - val_mae: 1190.2594\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.6333 - mae: 1166.6333 - val_loss: 1190.3444 - val_mae: 1190.3444\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.7308 - mae: 1166.7308 - val_loss: 1189.4094 - val_mae: 1189.4094\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5526 - mae: 1166.5526 - val_loss: 1190.6384 - val_mae: 1190.6384\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.2701 - mae: 1166.2701 - val_loss: 1189.3081 - val_mae: 1189.3081\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.2324 - mae: 1166.2324 - val_loss: 1190.3262 - val_mae: 1190.3262\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.1544 - mae: 1166.1544 - val_loss: 1190.9987 - val_mae: 1190.9987\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.4553 - mae: 1166.4553 - val_loss: 1190.5316 - val_mae: 1190.5316\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.8123 - mae: 1166.8123 - val_loss: 1189.6770 - val_mae: 1189.6770\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5917 - mae: 1166.5917 - val_loss: 1188.7397 - val_mae: 1188.7397\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.4796 - mae: 1166.4796 - val_loss: 1189.9938 - val_mae: 1189.9938\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5144 - mae: 1166.5144 - val_loss: 1190.9899 - val_mae: 1190.9899\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5955 - mae: 1166.5955 - val_loss: 1190.0695 - val_mae: 1190.0695\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1183.2845 - mae: 1183.2845\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f591b3b-a116-4223-9a66-b549b75a1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-3')(dropout2)\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(1, name='Output'))(lstm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28a747ed-ff54-40f4-85bc-96b35165648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model multi dense\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-3')(dropout2)\n",
    "dense = keras.layers.Dense(128, name='Dense')(lstm3)\n",
    "output = keras.layers.Dense(1, name='Output')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d996daba-5af0-40cc-a2b2-faa36f00f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for quick testing\n",
    "\n",
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(32, return_sequences=True)(input)\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm1)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dd42f8d-55a5-4ef6-962f-bf2cdfe25af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Covid-Prediction-30-1-Densex2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 30, 101)]         0         \n",
      "                                                                 \n",
      " LSTM-1 (LSTM)               (None, 30, 256)           366592    \n",
      "                                                                 \n",
      " Dropout-1 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " LSTM-2 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " Dropout-2 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " LSTM-3 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " Dense (Dense)               (None, 30, 128)           32896     \n",
      "                                                                 \n",
      " Output (Dense)              (None, 30, 1)             129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,450,241\n",
      "Trainable params: 1,450,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157095f-0712-4ff5-9559-b7e99ab8e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model: \"Covid-Prediction-30-1\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " Input (InputLayer)          [(None, 30, 101)]         0         \n",
    "                                                                 \n",
    " LSTM-1 (LSTM)               (None, 30, 256)           366592    \n",
    "                                                                 \n",
    " Dropout-1 (Dropout)         (None, 30, 256)           0         \n",
    "                                                                 \n",
    " LSTM-2 (LSTM)               (None, 30, 256)           525312    \n",
    "                                                                 \n",
    " Dropout-2 (Dropout)         (None, 30, 256)           0         \n",
    "                                                                 \n",
    " LSTM-3 (LSTM)               (None, 30, 256)           525312    \n",
    "                                                                 \n",
    " Output (Dense)              (None, 30, 1)             257       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 1,417,473\n",
    "Trainable params: 1,417,473\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3280bb06-dae2-4115-9eaa-2c24336ea76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras.utils.plot_model(model, to_file='multilayer_perceptron_graph.png')\n",
    "keras.utils.plot_model(model, to_file='model_3_256.png', show_shapes=False, show_dtype=False, \n",
    "                       show_layer_activations=False, rankdir='LR', dpi=70, show_layer_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b23a23bb-19f0-472c-8f52-963f931b9cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaYAAAIaCAYAAABrkemQAAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzdfVxUZd4/8M8gDOAAziAPrYQgkYAPZEVqLMbdLa5r0IaFZIgk3ZIIaYom+xM1RQE1Y9cVW8xSUF63KQSUuJpgWyr4kK8tNIlUtlKe5GnUIRvk4fr94c65Oc7TAQZmwO/79fKPOedwXde5kPnOmXPO54gYYwyEEEKIabhjZuwREEIIId1RYSKEEGJSzDUtbGlpwd27dwd6LIQQQh4iNjY2kEqlastFms4xvfbaa/jqq68gkUgGZHADoaOjAwqFAjKZzNhDeWi0tLRgxIgRGDZsmLGHYhR37tyBpaUlLC0tjT0Uo2ptbcWwYcNgbW1t7KEQE6JQKDB79mxkZmY+uOqOxiMmAPjggw8we/bs/h3ZALp48SIWLlyI8+fPG3soD40nnngChw4dgpeXl7GHYhSvvfYawsPDh9TfUW+sWrUKjz76KJYuXWrsoRATkpWVhbNnz2pcR+eYCCGEmBQqTIQQQkwKFSZCTMxf/vIX/OlPfzL2MAacn58fRCIRRCIRsrKyeOsOHDiAgwcPGmdgRE1XVxdiYmLQ1NTEW+7g4MD9DktKSnrdPhUmQkwMYwzGuu9906ZNmDp1qlH6BoCkpCQwxrBgwQJu2Z49e1BQUIA5c+agsLCQe+N78cUX1ebJxsaGWx8ZGTnAo9euvr4e77zzDlxdXWFtbY3x48fjww8/VNsuPz8fPj4+sLKywqRJk3DixIle9bd9+3ZYWFjg2rVrGtcL7UdbO2ZmZoiOjsasWbPQ1tbGLW9qaoJCoejVmHnt97kFQohBJSQk4PDhw8Yehkmor69HQkIC0tPTYWZmhtDQUDDGIJVKUVRUhA0bNvC2b21tRXx8PIqLi5GTk2OkUatbuXIlzp49i5MnT6KpqQlxcXFYtGgR/vnPf3LblJaWYu7cuVi1ahVqa2sRHh6OkJAQVFZWCu5HqVQiKioKhw4dQkdHh8ZthPQjpB1/f3/4+PggNTVV8PiEosJEiAlZv349RCIRrKys1F6fOHECnp6ekEqlWLx4MXe0EBsbC5FIBHd3d2RnZ8PV1RXDhw9HREQEWltbAQALFiyASCTijoZaW1u5I4vCwkKunbVr1+LcuXPcOm2fuAfKoUOHMHHiRDz66KO85U5OTggLC0NycrLeIn7v3j0kJCTA2dkZ1tbWeP7553Hx4kUAwuZX9fNOTk5qP98TqampGDNmDCQSCRYvXgxzc3NUVVVx69PS0hAcHIzo6GjY29tj9erV8Pb2xrZt2wT38f3332P+/PnYsmWL1m2E9COkHQAICwtDVlaWwY/wqTARYkLWr1/Pe6NVve7s7MTRo0dx4cIFfPbZZ9i1axf3HX5mZiZ2796NxsZGnD9/HuXl5SgrK8Pp06exYsUKAPcvzd24cSPXro2NDRhjvHsVMzMzsXHjRkyZMoX7OtHT03OA9lyzkpIS+Pj4qC1XnYfy9fVFZGQkrly5orWNlStXoqCgAMeOHUNNTQ0mTJiAoKAg3L59W9D8JiYm4uuvv0ZZWRnq6+vx1FNPITg4GEqlUvB+5OTkYNq0aWCMoaGhARs2bICbmxvvVoKTJ0/C39+f93MBAQE4efKk4H78/PwwY8YMndsI6UdIOwAwbtw4XL9+vUdHdUJQYSJkEOjo6MCyZcsglUoRGBgIb29vXLp0ibfNvXv3sGXLFtjb22PSpElISEjA3r17cefOnT71feDAATg4OOCnn37qUzu9UVNTAwcHB43rJBIJCgsLIRaLERoaqvHchlwux9///nesXr0aTz75JOzt7bF161a0tbXxbuzUNr9yuRw7d+5EcnIyPD09MWLECGzevBlNTU3Iy8vr8f7MmTMHzs7OKCwsRF5eHkaOHAng/s2mCoUCjo6OvO2dnZ1RU1PT4360MXQ/qt9NbW2tQcanQoWJkEHA3Nyc93WWTCZTKzjOzs6wsbHhXk+YMAHt7e19/jpOdfRkjAsy5HI5LCwstK53d3dHXl4erl69igULFqiNsbKyEh0dHZg4cSK3zNraGp6enryv47TNb2VlJdrb2xESEsJ9vSkWi6FUKvHDDz/0eH/y8vLQ0tKCJUuWICAgAMXFxQDAjVskEvG2Z4ypLesLQ/cjFosB3E95MSQqTIQMAg/GOml6E+nq6uK91lRIHlym7cR2dxEREWhuboaHh4eQoRqUTCZDe3u7zm0CAwOxfft25OfnIy0tjbdOVzHtPofa5lf182fPnuUVaMYYUlJSerQvKjKZDAsXLkRQUBA2bdoEALCzs4OtrS0aGhp42zY0NGDUqFG96kcTQ/dz7949AIC9vb1BxqdChYmQIaKhoQG3bt3iXldUVMDCwoI7T2Rra4vbt29z6+vq6niX+gKaC54xubi4qN0ro0lcXBxiYmKwdu1a3rkSHx8fmJub4/vvv+eWKZVKVFVV8Y6itPHx8YGFhUWvLnbozsPDQ+2Dg1gsxq+//sq9njZtGsrKynjblJaW4rnnnutT3w8yZD+q340hiydAhYmQIUMikSApKQktLS0oLy9Heno6oqOjYWdnBwDw9fXFlStXcOrUKdy6dQvp6encVzEqjo6OqK+vR2trK2JiYpCRkWHUc0xBQUGCT6zv3LkT/v7+vHNvMpkMixcvRlpaGsrLyyGXy5GYmAixWIzY2Fi9bcpkMsTHxyMlJQXnzp1DW1sbLly4gLFjx+LMmTMAgGPHjkEsFus8smtpacHSpUu5uc3JyUFhYSFCQ0O5bRITE3HkyBFkZ2dDLpdjy5YtqKioQEJCAreNkL70EdKPUBUVFRg9ejS8vb17PR6NmAZz585l+fn5mlYNWuXl5eyZZ54x9jAeKr6+vqyystLYwzCa3vwdvfvuuwwA9y8+Pp73Wi6XMy8vL+7166+/zhhjbPfu3czNzY3l5uYyNzc3Zm1tzebOncsUCgWv/bfeeotJpVI2ZswYVlxczCQSCQPA3n33XcYYY42NjWzKlCnMysqK+fv7s+bmZnbw4EFmb2/PqqqqejUP77zzDtu+fbve7Z5++mmWlJTEW1ZXV8ekUimrrq5mjDFWUFDAm4/i4mLe9jdv3mSurq685Uqlki1fvpw5OjoyS0tLFhgYyL777jvGGGM7duzQO79tbW1sxYoVzMnJiVlZWTFfX1/2ySefcO0vXbqURUZG6ty3M2fOsNmzZ7Pf/e53TCKRsHHjxrH33nuPdXZ28rbLzc1lXl5eTCwWM19fX3b8+HHeen19tbe38/YHABs5cqTadvr6EdpOVFQUW7duHW+ZQqHQ+Lt50N69e9miRYs0rbpNhYn0G6GF6ciRI2zKlCnMzs6O+yOYOXPmAIywfw3k35GqMJminhQm1e9/79693PKPP/6YhYeHq72Jm4JvvvmGhYaGqn0AGOx9CVFaWsr8/PyYUqnklo0cOVLrh4YH6SpMvf4q7/Tp09xVKiKRCAEBAb1tqlf6IzrFFPfJ2GPqb/X19Xj55ZcRHR2N2tpaMMaQmJho0D60/V8REhNTWlqKkJAQ2NrawsHBAcHBwWqXaRPDuHDhAndhQfdIojfeeAMvvfQScnNzjTc4Lfz8/FBQUMC7GnIo9KVPV1cX9uzZg6NHj/KeN9bU1MT9DoOCgnrdfq8LU0BAABhjGDlyJN577z2cPn2614MwFaa4T6Y4JkO6ePEiOjs78eabbw74gymFxMRMnz4dEydOxI0bN1BeXg4AmDFjhsEvj+2L2NhYxMTE4JdffoFIJOJdADFURERE4NVXXzX2MMh/mJmZ4aOPPtJ6j1mf2zdkY/riPQZjdMpARcL0dZ8qKysRGhoKmUwGV1dXrFq1Cu3t7bx+RCIRioqKAACenp7cmAHdsSsPzoG3tzeXW9bXuZ05cyY6OjpgZmYGc3PNz63UFSmja98B/f9X9MXEODs7IzU1FVKpFC4uLkhPT8fNmzdx6tSpPu27IWVmZvIuY9b0qGpCBhODFyZd8R6DMTploCJh+rpPb7/9NmJiYlBfX48vv/wSX375JdLT02FjY4OWlhZYWloiNzcXISEhAIDLly/j6aef5q600hW7opqDjo4O5Obm4vTp08jPzzfY3FpaWoIxpvWeGl2RMrr2HdA9r0JiYlRHISojRowAAC7LjhBieP1yubi++JT+ik7pT8aMhBHiiy++QHBwMCwtLfH4449j3rx5OHbsGID7l7yGhIRgz5493PaFhYXc3exCY1c6OzuRlJQEBwcHhIaGckew/UlIpIyufRdCW0yMJqqj5unTp/d53wghmvVLYdIXn9Jf0Sn9yZiRMEIUFxdj8uTJkEgkEIlESEhI4N2YGBUVhePHj3N5WHv37kVUVBQACI5dsbCwgKura7/vS3dCImX07bs+2mJiHnTr1i0kJydj3759Wr92JIT0Xb/8demLT+mv6JT+ZMxIGH0aGhoQGhqKJUuWoKSkBHZ2dsjIyOCFVM6aNQsymQzZ2dmYP38+2trauIgZ1ZjOnj2LKVOmaO3HzGzg78fWNIcqIpFI0L4LoYqJKSoqwqZNm9SSlRUKBWbPno2UlBQ8++yzgtr89ddfsXbtWmzfvr1HYxlqrl+/jmHDhhnk618ydNTX12u9stooH/tU0Smqk7RDIToF6Pt+9WafvL29kZmZibt37yImJoa7y1+VYaViYWGBuXPnIisrC52dnbyne3aPXdFVmIyhe6SM6j+xKlImPDwcFRUVevdd27x6eHjg2rVrvIL7YEwMcD/hOiIiAmvWrBH0KAAVKysrzJ8/H4GBgYJ/ZijauXMnHB0dER4ebuyhEBNy5MgR3Lx5U+M6oxQmVXTKxo0bcePGDZ3RKRMnTtQbnbJ8+XI88cQTeOutt4yxO5y+7ldv98nLywtisRgZGRlITk7GzZs3kZ2drbbd/PnzkZGRgYyMDFy9epVb3j12xdfXF5MmTcKlS5cQERGB7OxswUcI/aF7pMwzzzyD0aNHY/369VykzN27d/Xuu7Z5VcXErFmzBjY2NigsLERhYSHWrVvH/ezFixexePFifPjhhxg/fjyA+88IKikpwebNm3WOfdiwYRg7dqxRH1VuCvLz8/Hoo48+9PNA+CorK9HY2Kh5pabbboXcsX7q1CleXMXvf/97QfEe/RmdoouQ5Ife7hNjhomE0bRPD47pwX+WlpaMMcYOHjzIPD09ma2tLZs+fTpbtmwZA8AkEgmvf29vbxYREaG277piVx6cgx07duicRxV9yQ8Pxu/88MMPbPr06WoRKLoiZYTsu7b/K/piYn777Tdma2urcd4TExP17v9QTFDpDaHJD+ThYlKRRMaKTunvSCJTjoTpLjY2ln3xxRcD0hdl5VFhYowKE9GsXyKJyOBz+/ZtXLp0qU9RIYT0Fz8/P+6q0KysLN66AwcO4ODBg8YZGFHT1dWFmJgYtatfHRwcuN+h6j7P3hjQwjRUo1NMfb9WrlwJkUiEsWPH4p133jHK1XWk//RHbuRAtt9dUlKSWlbenj17UFBQgDlz5qCwsJB743vxxRfVrtq0sbHh1ne/wMfYhOQyAvfPx/n4+MDKygqTJk3CiRMnetXf9u3bYWFhofVWFaH9aGvHzMwM0dHRmDVrFu8CrqamJo2PuO+pAX2HGqrRKaa+X9u2bQNjDDdv3sRLL71k7OEQIlh9fT0SEhKQnp7OxWCp/saKioqwYcMG3vatra2Ij49HcXExcnJyjDRqdUJyGUtLSzF37lysWrUKtbW1CA8PR0hIiODnUQH3r1iNiorCoUOHtN6KIqQfIe34+/vDx8cHqampgscnFH10JsTIdGUB9jZjcTDmUmpy6NAhTJw4kXdzOwA4OTkhLCwMycnJOHz4sM42dM2vkCxMXTmSPaEvlzEtLQ3BwcGIjo6Gvb09Vq9eDW9vb2zbtk1wH99//z3mz5+PLVu2aN1GSD9C2gGAsLAwZGVl6bzfsDeoMBFiZLqyAHubsTgYcyk1KSkpgY+Pj9py1XkoX19fREZG4sqVK1rb0DW/QrIwdeVICiUkl/HkyZPw9/fn/VxAQADvUfH6+Pn56b3XTkg/QtoBgHHjxuH69es9OqoTggoTIUYkJAuwL/ozv3EgHrleU1Oj9dEKEokEhYWFEIvFCA0N1XhuQ+j8asvCFJojKZS2XEaFQgGFQgFHR0fe9s7OzlyMmCEYuh/V76a2ttYg41OhwkSIEQnJAuyL/sxv7H5etb/I5XJYWFhoXe/u7o68vDxcvXoVCxYsUBuL0PnVloUpNEdSKG25jKpxP5hSwhgzaMqNoftRBQQY+vlkVJgIMSJdb+rd3yge3E5oxmJ/5jdGRESgubmZy1zsDzKZjHu2ljaBgYHYvn078vPzkZaWxlsndH61ZWGqfv7s2bO8QswYQ0pKSo/2RUWVyxgUFIRNmzYBAOzs7GBra4uGhgbetg0NDRg1alSv+tHE0P2o4r/s7e0NMj4VKkyEGFH3LEAVVRag6lN+XzIWVfmNKoMtl9LFxUVQUnxcXBxiYmKwdu1a3rkSIfOrS/ccyb7w8PBQ+5DwYC7jtGnTUFZWxtumtLQUzz33XJ/6fpAh+1H9bgxZPAEqTIQYVfcswPLycsjlciQmJnJZgAA/Y/HWrVs6MxZjYmKQkZHBrVPlN7a0tKC8vFxnfqOmtnW1PxDnmIKCggSfWN+5cyf8/f15z0kTMr+6dM+RPHfuHNra2nDhwgWMHTsWZ86cAQAcO3YMYrFY55GdKpdRNY85OTkoLCzkPQU6MTERR44cQXZ2NuRyObZs2YKKigokJCRw2wjpSx8h/QhVUVGB0aNHw9vbu9fj0UhTHsRQjFLp70gioo4iiYT9HenLAmSs5xmLjBkmv5Ex7VmDBw8eZPb29qyqqkrn/gmNJHr66adZUlISb1ldXR2TSqWsurqaMcZYQUEBL7OwuLiYt/3NmzeZq6srb7mu+RWShakrR5IxxpYuXcoiIyN17pu+XEaV3Nxc5uXlxcRiMfP19WXHjx/nrdfXV3t7u1quoypzsif9CG0nKiqKrVu3jrdMoVBo/N08yKSy8oyFCtPAo8Jk3L8jU8lv7ElhUr0J7t27l1v+8ccfs/DwcLU3cVPwzTffsNDQULViP9j7EqK0tJT5+fkxpVLJLRs5cqTWDw0P0lWY6DGchBCTcOHCBY3L33jjDVhZWSE3NxevvvrqAI9KNz8/PxQUFAy5vvTp6urCnj17cPToUVhaWnLLe/LkaF2oMBEyBMXGxmLXrl0A7l+8IJfLTS4qqyciIiKMPQTSjZmZGT766KP+a7/fWiaEGI2p5zcSogsVJkIIISaFChMhhBCTovUc04kTJwweM2FM1dXVaGxsxMcff2zsoTw0WlpakJeXh0ceecTYQzGKf//73ygpKRlSf0e9cenSJVRXV/PCYQkpKyvTGjclYkw9s2Pv3r1ar5AZrLq6unDv3j1YWVkZeygPDaVSCUtLS6MmBwyUy5cvw9zcHF5eXtyytrY2mJubq8XdPGza29shEolgbk7XWhG+adOmYe7cuQ8uvqOxMBFCembdunWwtbXFO++8Y+yhEDLY3aFzTIQQQkwKFSZCCCEmhQoTIYQQk0KFiRBCiEmhwkQIIcSkUGEihBBiUqgwEUIIMSlUmAghhJgUKkyEEEJMChUmQgghJoUKEyGEEJNChYkQQohJocJECCHEpFBhIoQQYlKoMBFCCDEpVJgIIYSYFCpMhBBCTAoVJkIIISaFChMhhBCTQoWJEEKISaHCRAghxKRQYSKEEGJSqDARQggxKebGHgAhg1F7ezu+/vpr7vVPP/0Ea2trlJSUcMv++7//G2Zm9NmPkJ4SMcaYsQdByGDk5uYGuVwOCwsLqP6MRCIR7t27h9GjR+Py5ctGHiEhg9Id+jhHSC/Nnz8f9+7dQ0tLC+RyOeRyOVpaWtDZ2Yno6GhjD4+QQYuOmAjppWvXrmHSpEn49ddfecuHDx+OK1euwMXFxUgjI2RQoyMmQnrL09MTjzzyiNpyb29vKkqE9AEVJkL64M0334S1tTX3WiKRYNGiRUYcESGDH32VR0gf1NbW4vHHH8fdu3cBANbW1rhx4wZGjhxp5JERMmjRV3mE9MWoUaMwduxY7vWUKVOoKBHSR1SYCOmjRYsWwcbGBra2tvQ1HiEGQF/lEdJHzc3NcHV1hUgkQkNDAyQSibGHRMhgdsegyQ9lZWX47rvvDNkkIYOCi4sLhg8fjuzsbGMPhZAB98wzz+CZZ54xWHsGLUx5eXkoLy/HE088YchmB72ysjKMGDEC48ePN/ZQ+k1TUxNOnjyJl19+2dhDMYrHHnsMp06dwrVr14w9FKPLyspCREQExGKxsYdCBsA333yDhoYGgxYmg36Vl5CQAE9PT8TFxRmqySHhYZiXb7/9FnFxcThz5oyxh2IULS0tGDNmDG7fvm3soRidk5MTrl27Bjs7O2MPhQyAzZs3Q6lUYv369YZqkq7KI8QQ6LwSIYZDhYkQQohJocJEiJH95S9/wZ/+9CdjD8MoDhw4gIMHDxp7GOQ/urq6EBMTg6amJqOOgwoTIUbGGIMx7trYtGkTpk6dOuD9quzZswcFBQWYM2cOCgsLIRKJIBKJ8OKLL6rNh42NDbc+MjLSSCPWrL6+Hu+88w5cXV1hbW2N8ePH48MPP+Rtk5+fDx8fH1hZWWHSpEk4ceJEr/vbvn07LCwstF5oI6QvbW2YmZkhOjoas2bNQltbW6/H2FdUmAgxsoSEBBw+fNjYwxhQ9fX1SEhIQHp6OszMzBAaGgrGGKRSKYqKirBhwwbe9q2trYiPj0dxcTFycnKMNGrNVq5cibNnz+LkyZNoampCXFwcFi1ahH/+858AgNLSUsydOxerVq1CbW0twsPDERISgsrKyh71o1QqERUVhUOHDqGjo0PjNvr6EtKGv78/fHx8kJqa2qPxGRIVJkKMaP369RCJRLCyslJ7feLECXh6ekIqlWLx4sXcUURsbCxEIhHc3d2RnZ0NV1dXDB8+HBEREWhtbcWCBQsgEom4o6HW1lbuaKOwsJBrY+3atTh37hy3biAvdT906BAmTpyIRx99lLfcyckJYWFhSE5O1lus7927h4SEBDg7O8Pa2hrPP/88Ll68CEDYPHZvw8nJSa2NnkhNTcWYMWMgkUiwePFimJubo6qqCgCQlpaG4OBgREdHw97eHqtXr4a3tze2bdvWoz6+//57zJ8/H1u2bNG6jb6+hLQBAGFhYcjKyjLKkTxAhYkQo1q/fj3vDVj1urOzE0ePHsWFCxfw2WefYdeuXdxj2zMzM7F79240Njbi/PnzKC8vR1lZGU6fPo0VK1YgKysLGzdu5Nq0sbEBY4x35WBmZiY2btyIKVOmcF8lenp6Dth+l5SUwMfHR225SCRCVlYWfH19ERkZiStXrmhtY+XKlSgoKMCxY8dQU1ODCRMmICgoCLdv3xY0jwCQmJiIr7/+GmVlZaivr8dTTz2F4OBgKJVKwfuSk5ODadOmgTGGhoYGbNiwAW5ubpg9ezYA4OTJk/D39+f9TEBAAE6ePCm4DwDw8/PDjBkzdG6jry8hbQDAuHHjcP369R4f1RkKFSZCTFBHRweWLVsGqVSKwMBAeHt749KlS7xt7t27hy1btsDe3h6TJk1CQkIC9u7dizt37vSp7wMHDsDBwQE//fRTn9rRpaamBg4ODhrXSSQSFBYWQiwWIzQ0FAqFQm0buVyOv//971i9ejWefPJJ2NvbY+vWrWhra0NmZia3na55lMvl2LlzJ5KTk+Hp6YkRI0Zg8+bNaGpqQl5eXo/3ac6cOXB2dkZhYSHy8vIwcuRIKBQKKBQKODo68rZ1dnZGTU1Nj/vQxZB9qX43tbW1BhtfT1BhIsQEmZub877mkslkagXH2dkZNjY23OsJEyagvb29z1/JqY6g+vNrHLlcDgsLC63r3d3dkZeXh6tXr2LBggVqY6msrERHRwcmTpzILbO2toanpyfvqzhd81hZWYn29naEhIRwX2eKxWIolUr88MMPPd6nvLw8tLS0YMmSJQgICEBxcTE3bpFIxNuWMaa2rK8M2ZcqtaOlpcUwg+shKkyEmKBhw4bxXmt6Y+nq6uK9fvDN+8HX2k52PygiIgLNzc3w8PAQtH1vyGQytLe369wmMDAQ27dvR35+PtLS0njrdBXN7nOlax5VbZw9e5ZXjBljSElJEbwv3clkMixcuBBBQUHYtGkT7OzsYGtri4aGBt52DQ0NGDVqVK/60MaQfd27dw8AYG9vb7Dx9QQVJkIGqYaGBty6dYt7XVFRAQsLC3h6esLW1pYXj1RXV6d2+a+hP7H3hIuLi6B7ZeLi4hATE4O1a9fyzsn4+PjA3Nwc33//PbdMqVSiqqqKdxSli4+PDywsLHp1sUN3Hh4eah8SxGIxfv31VwDAtGnTUFZWxltfWlqK5557rk/9amKovlS/G0MXT6GoMBEySEkkEiQlJaGlpQXl5eVIT09HdHQ07Ozs4OvriytXruDUqVO4desW0tPT1UJVHR0dUV9fj9bWVsTExCAjIwPAwJxjCgoKEnxifefOnfD39+edY5PJZFi8eDHS0tJQXl4OuVyOxMREiMVixMbGCmpXJpMhPj4eKSkpOHfuHNra2nDhwgWMHTuWy3w8duwYxGKxzqO7lpYWLF26lJvLnJwcFBYWIjQ0FMD9CyyOHDmC7OxsyOVybNmyBRUVFUhISODaENKPEEL6EqKiogKjR4+Gt7d3n8bTa8yAli9fznbu3GnIJoeEh2Fe/vWvf7GpU6caexhGo1QqmZ2dXY9/7t1332UAuH/x8fG813K5nHl5eXGvX3/9dcYYY7t372Zubm4sNzeXubm5MWtrazZ37lymUCi4tt966y0mlUrZmDFjWHFxMZNIJAwAe/fddxljjDU2NrIpU6YwKysr5u/vz5qbmxljjB08eJDZ29uzqqqqXs2Fo6Mju337ts5t6urqmFQqZdXV1YwxxgoKCnj7XVxczNv+5s2bzOZ1FL0AACAASURBVNXVlbdcqVSy5cuXM0dHR2ZpackCAwPZd999xxhjbMeOHYLmsa2tja1YsYI5OTkxKysr5uvryz755BOuj6VLl7LIyEid+3LmzBk2e/Zs9rvf/Y5JJBI2btw49t5777HOzk5um9zcXObl5cXEYjHz9fVlx48f57UhpJ/29nbePgFgI0eOVNtOV19C24iKimLr1q3TOR6VtLQ07v+UgdymwjQAHoZ56UlhOnLkCJsyZQqzs7Pj/jhmzpzZzyPsX70tTL2lKkymSEhhYoyxjz/+mIWHh/PewE3JN998w0JDQ3nFfjD3I1RpaSnz8/NjSqVS0Pb9UZgG/Ku806dPc1fAiEQiBAQEqG3T0dGBDRs2YOzYsbC2tsaoUaMQHR2Ns2fPctt4enry2tH0T3WTnUgkwqOPPqr13oS//e1v3HYLFizQOX59cSC9NdjnRaj6+nq8/PLLiI6ORm1tLRhjSExMNEjbKtqidvRFx5SWliIkJAS2trZwcHBAcHCw2iXaxHDeeOMNvPTSS8jNzTX2UDTy8/NDQUEB78rHwdyPEF1dXdizZw+OHj0KS0tLo41jwAtTQEAAGGMYOXIk3nvvPZw+fVptmw0bNuDDDz/E3r17IZfL8e2338LFxQXPPvssWltbAdy/DJT95woa1fey+/fv55bNmzcPISEhYIxx1/F3v79B5bfffsPWrVsBAMXFxcjKytI4biFRHn0xWOelpy5evIjOzk68+eabA/6oCH3RMdOnT8fEiRNx48YNlJeXAwBmzJhhtEtmtYmNjUVMTAx++eUXiEQi3gUQg01ERAReffVVYw+D/IeZmRk++ugjrfeYDdg4jNq7FiUlJZgzZw5+//vfw8rKCs7Ozti0aRPGjh3bq/YcHBzwwgsvYPPmzdyVMioZGRn44x//qLcNoVEe/ckU56Un1q9fj5kzZ6KjowNmZmYwN9f8AGVdUTPA/ftPQkNDIZPJ4OrqilWrVnFFWF/Ujq7oGGdnZ6SmpkIqlcLFxQXp6em4efMmTp06ZdB56KvMzEzepc1SqdTYQyLEoEyyMI0ePRolJSWoq6vjLf/xxx+5w119V/Tk5OTAz8+Pe52WlobGxkbuyiPgfobYvn37sHz5cr1jEhrl0Z9McV56QhUTY2lpCcaY1iNPXVEzAPD2228jJiYG9fX1+PLLL/Hll18iPT0dgO6oHX3RMaojEJURI0YAAJdjRwgZGCZZmFJSUtDV1QV3d3eEhIRg+/btfb501dfXF/PmzcPWrVu5O7//+te/Ii4ubtA8ffRhmBchUTNffPEFgoODYWlpiccffxzz5s3DsWPHBPehKTpGE1X45/Tp0w2yb4QQYTR/l2Jknp6euHTpEkpKSnDkyBHs2rULy5cvx2uvvYa9e/eq3Y8h1MaNG3Ho0CH89a9/xdtvv43PP/8cpaWlBs+s6i8Pw7wIiZopLi5GUlISLl++jLt37wK4H8cjVF5eHuRyOT799FMEBASgoKBA7Wj41q1bSE5Oxr59+7R+5ahJX3PqhgLGmMZ8OzI09cdzm0yyMAH3o0RmzpyJmTNnAgC+/vprzJw5E9OmTRN8A92D3NzcsHjxYqSnp6OxsRErV67UmNf10UcfISYmhnt948YNtXh+YzHmvAwEpidqpqGhAaGhoViyZAlKSkpgZ2eHjIwMjRdw6KKKjikqKsKmTZt4hUmhUGD27NlISUnBs88+K7jNtra2AU3oNlV37tzBpEmTjJosQQbO3bt3sWLFCoO2aXKFydvbG56enkhNTYWvry+3PDAwEKNHj0Z9fX2f2k9KSsKePXtw5swZ/O1vf9O4zcKFC7Fw4cI+9WNopjAvA6F71Izqkm9V1Ex4eDgqKipw9+5dxMTEwM7ODsD/5XqpaHtD9PDwwLVr12Bm9n/fYHePjgHup15HRERgzZo1PT6naGlpqZZT9jBycnLCtWvXuN8PGdo2b97co8eECGGS55gAYOnSpbh48SLa2trQ2NiI9PR0/Pvf/8Yf/vCHPrXr4OCAL7/8EocOHRqUn+iG+rzoi5rx8vKCWCxGRkYGFAoFrl27huzsbF4b2qJ29EXHXLx4EeHh4fjggw+4olRSUoI///nPAzsJhDzsDHm7rpCEg1OnTqlFYnT/Z2lpyaqrq9maNWvYE088wSQSCbO3t2e///3vWWFhoVp7ubm5vJ9/5ZVXtPZ15swZtZ9/MLrk7bff1jhuoVEeD9u8qAhJfngwfueHH35g06dPV5tPXVEzjN2PzPH09GS2trZs+vTpbNmyZQwAk0gkjDHtUTu6omN+++03Zmtrq3HuExMTde6XaswDmfxgyoQmP5ChoT+SH0SMGe6hKwkJCfD09ERcXJyhmhwSHoZ5+fbbbxEXF8eFXz5s2tra4OTkxEv0flj15Ku8AwcOwMzMjG6yNRFdXV1YtGgR0tLSBN9kq/oqb/369YYaxh2T/SqPEKKZtsglU2/7QXv27EFBQQHmzJmDwsJC7oboF198Ue0iGBsbG259ZGTkgIxPKH1RVwCQn58PHx8fWFlZYdKkSThx4kSP+xESmZWTk6MxhkyVDAMAVVVVCAsLg0wmg5OTExYtWoTm5mYA95MfoqOjMWvWrH652k4oKkyEkAFXX1+PhIQEpKenw8zMDKGhoVyKRVFRETZs2MDbvrW1FfHx8SguLkZOTo6RRq2Zvqir0tJSzJ07F6tWrUJtbS3Cw8MREhIi+LEfKkIjs7pHkKn+qW7Ar6urQ0BAACZPnowff/wR58+fR2lpKXbs2MH9vL+/P3x8fJCamtqXaekTKkyEDCBdcUsLFiyASCTijlhaW1u5T7yFhYUAtEcuxcbGQiQSwd3dHdnZ2XB1dcXw4cMRERHBfVrW176+OCdDOnToECZOnKh2G4aTkxPCwsKQnJyMw4cP62xD11yqgoqtrKy4G6WlUikWL17MOxpTteHk5KQx/kooXVFXaWlpCA4ORnR0NOzt7bF69Wp4e3tj27ZtPerDEJFZqampeOaZZ7Bq1So4OTnB3d1d47OawsLCkJWVpfP2jf5EhYmQAaQrbikrKwsbN27ktrWxsQFjjJfAoS1yKTMzE7t370ZjYyPOnz+P8vJylJWV4fTp09w9Jvra1xXnZGglJSXw8fFRWy4SiZCVlQVfX19ERkbiypUrWtvQNZeq+KvOzk4cPXoUFy5cwGeffYZdu3ahpKSEayMxMRFff/01ysrKUF9fj6eeegrBwcE9uvxZX9TVyZMn4e/vz/uZgIAA3hN5hRAamfX555/j8ccfh52dHaZOncpLRSkqKsL48ePxyiuvYPjw4fDw8EB1dTXWrFnDa2PcuHG4fv16j4/qDIUKEyEDREjcUl/du3cPW7Zsgb29PSZNmoSEhATs3bu3z4kUhn6qbU1NjdaT6xKJBIWFhRCLxQgNDdWYIiF0Ljs6OrBs2TJIpVIEBgbC29ubOy8jl8uxc+dOJCcnw9PTEyNGjMDmzZvR1NSEvLy8Hu+TpqgrhUIBhUIBR0dH3raqZP++0BaZpVQqUVRUhOvXr2POnDl44YUXcPr0aXR0dODnn3/Gjh07EBYWhoaGBuzatQvbtm3jniSgovrd1NbW9mmMvUWFiZABIiRuqa+cnZ15z/WZMGEC2tvb+/yVXPfzFYYgl8t1pou4u7sjLy8PV69exYIFC9T6FTqX5ubmvK8LZTIZV6QrKyvR3t6OkJAQ7qtLsVgMpVKJH374ocf7lJeXh5aWFixZsgQBAQEoLi7mxv3gvYGMsT7dL6gtMisyMhKff/45vLy8IJVKsWLFCkyePBkZGRncUWBAQABee+012NjYYMaMGYiKilK7WEMVb2asR75QYSJkgOh6U+/+JvXgdj15/ldXV5fePnvTfkREBJqbm+Hh4SF4LLrIZDLuUSXaBAYGYvv27cjPz0daWhpvndC5HDZsmNZ1qjbOnj2rdrFASkqK4H3pThV1FRQUhE2bNsHOzg62trZqiSANDQ0YNWpUr/roaWTWY489hp9++gkSiQRWVlZwdXXlrXd3d0d1dbXauTcAsLe379UY+4oKEyEDpHvckooqbkn1yd/W1pZ3L1RdXZ3aZbu6Pmk3NDTwHhxYUVEBCwsL7lyRvvYHKvXDxcUFTU1NereLi4tDTEwM1q5dyzsnI2Qu9fHx8YGFhUWfj1Y9PDzUPhB0j7qaNm0aysrKeOtLS0vx3HPP9bivmpoahISEYPXq1QgLC1Nb/+STT6KxsZG3rKqqCu7u7hCJRJg8eTJ3UYbKzz//zK1XUf1uels8+4oKEyEDRF/cEnD/MSRXrlzBqVOncOvWLaSnp6ulxmuLXALun59JSkpCS0sLysvLkZ6ejujoaO5mV33ta2vb0OeYgoKCBJ9Y37lzJ/z9/Xn37AiZS31kMhni4+ORkpKCc+fOoa2tDRcuXMDYsWO5G8WPHTsGsVis8+hOX9RVYmIijhw5guzsbMjlcmzZsgUVFRW8q+GE9CMkMosxhvj4eNTU1OD27dt4//33cf78ecTHxwO4f8HIV199hf3796O1tRUnTpzA/v378fbbb/P6qqiowOjRo+Ht7S1oLg3OkDkSQqJ3HkYPw7wIiSQayoRGEumLW2KMsbfeeotJpVI2ZswYVlxczCQSCQPAxb5oi1zavXs3c3NzY7m5uczNzY1ZW1uzuXPnMoVCIbh9bW0fPHiQ2dvbs6qqKr37KCSSqK6ujkmlUlZdXc0YY6ygoIAXA1VcXMzb/ubNm8zV1ZW3XNdcPhipJZfLmZeXF/f69ddfZ4wx1tbWxlasWMGcnJyYlZUV8/X1ZZ988gnXx9KlS1lkZKTOfdEVdaWSm5vLvLy8mFgsZr6+vuz48eO8NvT1IzQy6/Lly2zevHnMxcWF2dnZsalTp7J//OMfvLb27dvHxo4dyywtLZm3tzfLyMhgXV1dvG2ioqLYunXrdO63Sn9EElFhGgAPw7xQYTJ+Vp6qMBmb0Ky8jz/+mIWHh/PewE3JN998w0JDQ9UK+2DtR6jS0lLm5+fHlEqloO37ozCZ3GMvCCEPhzfeeANWVlbIzc01yaw8Pz8/FBQUDJl+hOjq6sKePXtw9OhRWFpaGm0cVJgIGQJiY2Oxa9cuAPcvYJDL5ZBKpUYelX4RERHGHgLpxszMDB999JGxh0EXPxAyFGRmZvIudx4MRYkQbagwEUIIMSlUmAghhJgUKkyEEEJMisEvfti1axe++OILQzc7qP373//GqVOnhvS8tLa2ora2Fi+99JKxh2IU7e3tsLCweGj3/0ERERFqcUBkaLp69SrCw8MN2qZBH63+448/4ueffzZUc4QMGvv27cPw4cM1xsQQMtR5enriscceM1Rzdwx6xOTl5QUvLy9DNknIoFBaWgpbW1vMnDnT2EMhZNCjc0yEEEJMChUmQgghJoUKEyGEEJNChYkQQohJocJECCHEpFBhIoQQYlKoMBFCCDEpVJgIIYSYFCpMhBBCTAoVJkIIISaFChMhhBCTQoWJEEKISaHCRAghxKRQYSKEEGJSqDARQggxKVSYCCGEmBQqTIQQQkwKFSZCCCEmhQoTIYQQk0KFiRBCiEmhwkQIIcSkUGEihBBiUqgwEUIIMSnmxh4AIYNRR0cHkpKS0NbWBgC4dOkSzM3NUVNTAwCQSCTYtGkTRCKRMYdJyKAkYowxYw+CkMHo6aefxr/+9S+N655//nl8+eWXAzwiQoaEO/RVHiG9tGjRItjY2Kgtt7W1RWxsrBFGRMjQQEdMhPSSXC6Hi4sLfvvtN95ya2trNDY2QiKRGGlkhAxqdMRESG/JZDJMnjyZt0wkEmHmzJlUlAjpAypMhPTBokWLYGdnx722s7NDTEyMEUdEyOBHX+UR0gd3796Fo6Mj7t69C+D++aXm5mZYWFgYeWSEDFr0VR4hfTF8+HAEBQVBJBLBzMwMr7zyChUlQvqIChMhfRQTEwM7OzvY2trijTfeMPZwCBn06Ks8Qvqovb0d9vb2sLS0RENDA8zM6PMeIX1wh5f8sGPHDhw5csRYgxmyGGO4c+cORowYYeyhmKzOzk7cvXsXtra2xh5Kr9jZ2cHCwgIvvPBCr9u4desWpFKpAUc1ON25cwe2traUmvEQmTNnDv7nf/6He80rTJcvX8akSZPwxz/+ccAHNpQpFAosWLAAn376qbGHYrJu3LiB9evXIyUlxdhD6ZXy8nJIJBJ4enr2uo0//OEPOH78uAFHNTjNnTsXH3zwAezt7Y09FDIA8vPz8eOPP/KWqWXleXt747/+678GakwPBblcDrFYTPOqw48//ggbG5tBO0eBgYF9/oRvZmY2aPffkKysrODv749HHnnE2EMhA+DixYuorq7mLaMvwwkxAPraiRDDocJECCHEpFBhImSQ+8tf/oI//elPxh6GURw4cAAHDx409jDIf3R1dSEmJgZNTU19aocKEyGDHGMMxrjrY9OmTZg6deqA96uyZ88eFBQUYM6cOSgsLIRIJIJIJMKLL76oNh82Njbc+sjISCONWLP6+nq88847cHV1hbW1NcaPH48PP/yQt01+fj58fHxgZWWFSZMm4cSJEz3up7S0FCEhIbC1tYWDgwOCg4Nx6dIl3jY5OTncPHX/19raym1TVVWFsLAwyGQyODk5YdGiRWhubgZw/zxpdHQ0Zs2axT2rrDeoMBEyyCUkJODw4cPGHsaAqq+vR0JCAtLT02FmZobQ0FAwxiCVSlFUVIQNGzbwtm9tbUV8fDyKi4uRk5NjpFFrtnLlSpw9exYnT55EU1MT4uLisGjRIvzzn/8EcL+gzJ07F6tWrUJtbS3Cw8MREhKCysrKHvUzffp0TJw4ETdu3EB5eTkAYMaMGWhpaeFtt3//fu7Djuqf6vEudXV1CAgIwOTJk/Hjjz/i/PnzKC0txY4dO7if9/f3h4+PD1JTU3s9J1SYCBnE1q9fD5FIBCsrK7XXJ06cgKenJ6RSKRYvXswdRcTGxkIkEsHd3R3Z2dlwdXXF8OHDERERwX0yXrBgAUQiEXdE1Nrayn16LiwsRGxsLNauXYtz585xy69duzZg+33o0CFMnDgRjz76KG+5k5MTwsLCkJycrLdY37t3DwkJCXB2doa1tTWef/55XLx4EYCweezehpOTk1obPZGamooxY8ZAIpFg8eLFMDc3R1VVFQAgLS0NwcHBiI6Ohr29PVavXg1vb29s27atR304OzsjNTUVUqkULi4uSE9Px82bN3Hq1KkejfOZZ57BqlWr4OTkBHd3dyQkJKhtFxYWhqysrF4fyVNhImQQW79+Pe8NWPW6s7MTR48exYULF/DZZ59h165dKCkpAQBkZmZi9+7daGxsxPnz51FeXo6ysjKcPn0aK1asAABkZWVh48aNXLs2NjZgjHGP88jMzMTGjRsxZcoU7lN1X+7h6qmSkhL4+PioLReJRMjKyoKvry8iIyNx5coVrW2sXLkSBQUFOHbsGGpqajBhwgQEBQXh9u3bguYRABITE/H111+jrKwM9fX1eOqppxAcHAylUil4X3JycjBt2jQwxtDQ0IANGzbAzc0Ns2fPBgCcPHkS/v7+vJ8JCAjAyZMnBfcBAL/88gvv6lHVDf+qDzUqn3/+OR5//HHY2dlh6tSpOHbsGLeuqKgI48ePxyuvvILhw4fDw8MD1dXVWLNmDa+NcePG4fr16z0+qlOhwkTIENTR0YFly5ZBKpUiMDAQ3t7eaucT7t27hy1btsDe3h6TJk1CQkIC9u7dizt37vSp7wMHDsDBwQE//fRTn9rRpaamBg4ODhrXSSQSFBYWQiwWIzQ0FAqFQm0buVyOv//971i9ejWefPJJ2NvbY+vWrWhra0NmZia3na55lMvl2LlzJ5KTk+Hp6YkRI0Zg8+bNaGpqQl5eXo/3ac6cOXB2dkZhYSHy8vIwcuRIKBQKKBQKODo68rZ1dnZGTU1Nj/voTnUkOH36dN5ypVKJoqIiXL9+HXPmzMELL7yA06dPo6OjAz///DN27NiBsLAwNDQ0YNeuXdi2bRu2bt3Ka0P1u6mtre3V2KgwETIEmZub877mkslkagXH2dmZ92j4CRMmoL29vc9fyXU/N9Ff5HK5zhR3d3d35OXl4erVq1iwYIHaWCorK9HR0YGJEydyy6ytreHp6cn7Kk7XPFZWVqK9vR0hISHc15lisRhKpRI//PBDj/cpLy8PLS0tWLJkCQICAlBcXMyN+8H75Bhjfbp37tatW0hOTsa+fftgbv5/OQuRkZH4/PPP4eXlBalUihUrVmDy5MnIyMjgjgIDAgLw2muvwcbGBjNmzEBUVJTaxRpisRgA1M5fCUWFiZAhaNiwYbzXmt7Eurq6eK81FZIHl3V0dOjtOyIiAs3NzfDw8BAy1F6RyWRob2/XuU1gYCC2b9+O/Px8pKWl8dbpKprd50rXPKraOHv2rNrFAr2N1pLJZFi4cCGCgoKwadMmLrW+oaGBt11DQwNGjRrVqz4UCgVmz56NlJQUPPvss3q3f+yxx/DTTz9BIpHAysoKrq6uvPXu7u6orq5WO/cGoNexUlSYCHlINTQ04NatW9zriooKWFhYcOeKbG1tcfv2bW59XV0d7xJgY6ZduLi4CLpXJi4uDjExMVi7di3vnIyPjw/Mzc3x/fffc8uUSiWqqqp4R1G6+Pj4wMLColcXO3Tn4eGh9iFBLBbj119/BQBMmzYNZWVlvPWlpaV47rnnetxXTU0NQkJCsHr1aoSFhamtf/LJJ9HY2MhbVlVVBXd3d4hEIkyePJm7KEPl559/5tarqH43vS2eVJgIeUhJJBIkJSWhpaUF5eXlSE9PR3R0NPeoeF9fX1y5cgWnTp3CrVu3kJ6ezn1FAwCOjo6or69Ha2srYmJikJGRAWBgzjEFBQUJPrG+c+dO+Pv7886xyWQyLF68GGlpaSgvL4dcLkdiYiLEYjFiY2MFtSuTyRAfH4+UlBScO3cObW1tuHDhAsaOHYszZ84AAI4dOwaxWKzz6K6lpQVLly7l5jInJweFhYUIDQ0FcP8CiyNHjiA7OxtyuRxbtmxBRUUF72o4If1cvHgR4eHh+OCDDzBjxgwA9y8i+fOf/8xtwxhDfHw8ampqcPv2bbz//vs4f/484uPjAdy/YOSrr77C/v370draihMnTmD//v14++23eX1VVFRg9OjR8Pb2FjSXalg3ixYtYnv37mXEsFpaWtgjjzxi7GGYtMrKSubr62vsYRiVpaVlj3/m3XffZQC4f/Hx8bzXcrmceXl5ca9ff/11xhhju3fvZm5ubiw3N5e5ubkxa2trNnfuXKZQKHjtv/XWW0wqlbIxY8aw4uJiJpFIGAD27rvvssbGRjZlyhRmZWXF/P39WXNzM2OMsYMHDzJ7e3tWVVXVq3lwc3NjdXV1Orepq6tjUqmUVVdXM8YYKygo4O13cXExb/ubN28yV1dX3nKlUsmWL1/OHB0dmaWlJQsMDGTfffcdY4yxHTt2CJrHtrY2tmLFCubk5MSsrKyYr68v++STT7g+li5dyiIjI3Xuy5kzZ9js2bPZ7373OyaRSNi4cePYe++9xzo7O7ltcnNzmZeXFxOLxczX15cdP36c14a+fn777Tdma2vL2yfVv8TERG67y5cvs3nz5jEXFxdmZ2fHpk6dyv7xj3/w2tq3bx8bO3Yss7S0ZN7e3iwjI4N1dXXxtomKimLr1q3Tud8q27dvZ++88073RbepMA0AKkz69aQwHTlyhE2ZMoXZ2dlxf1wzZ87s5xH2v94Upt5SFSZTJKQwMcbYxx9/zMLDw3lv4Kbkm2++YaGhoWrFfrD2I1RpaSnz8/NjSqVS0PaaClOPv8o7ffq0WlyFra0tnnjiCSQlJfU5I2mg9TRWZfv27bCwsDD4zYQP+7wKVV9fj5dffhnR0dGora0FYwyJiYkG7UPb2PVFxwiJfCGG88Ybb+Cll15Cbm6usYeikZ+fHwoKCnhXPg7mfoTo6urCnj17cPToUVhaWva6nR4XpoCAADDGMHLkSLz33ntgjOHGjRvYvHkzjh8/Dl9f315dKmnqlEoloqKicOjQIUFXJvXUwzqvPXXx4kV0dnbizTff5G72HCj6omOERr4YW2xsLGJiYrgbLrtfADHYRERE4NVXXzX2MMh/mJmZ4aOPPtJ6j5ngdgwxGKlUilmzZuHkyZMYOXIkXn31VXR1danFenh7e3O5VrriQIREpgC6I0X0Raqo+hEaq/L9999j/vz52LJliyGmTJDezCuge16EzO1AzmtPrF+/HjNnzkRHRwfMzMx49190p2v8wP37T0JDQyGTyeDq6opVq1ZxJ431jV1XdIwhIl8GQmZmJu/SZnqcOzE53b/Y68k5ppEjR7L33ntPbfnevXsZAHb27FnGGGOHDx9mw4YNY4sWLWKNjY2soKCAvfTSS2zJkiXM3d2d/etf/2LNzc3srbfeYo6OjuzWrVuMsfvfgQ8fPpzFxcWx5uZm9u233zJXV1f25ptvcn3pa2Pjxo1sypQpvPFJJBJWUFDAvda0jS6nTp1iANjVq1cF/0xPzjH1dV4Z0z8v+ubWGPMq9BzT4cOH1c7FJCYm8s4x6Rv/H/7wB1ZUVMSUSiW7cuUKe/rpp9nmzZsFj72rq4vdvHmTrVu3jj322GOsqalJ43Z1dXUMADt27Jje/WJsYM8xmTKh55jI0GCQc0z6PPHEEwDuXy6o0tnZiaSkJDg4OCA0NBR79+4VFAeiKzJFaKTIUCFkXgsLCwXPi7a5/eWXXwb1vArZ/y+++ALBwcGwtLTE448/jnnz5vHywPTRFB2jibbIF0KIbpq/C+kDpiFCw8LCgne3sNA4EF2RKW1tbYLaGCqEzCvQ97ktKioa1PMqZP+Li4uRlJSEy5cv4+7duwDu779QeXl5kMvl+PTTTxEQEICCggLuvhAVbZEvujDGTO6RDMZw9+5dfPrpp1zIKBnaLly4ACcnJ94ygxemPmk6awAAIABJREFU7777DsD9dFkVMzP+gRkTGAeiKzJFaBsPbifkwoWPPvoIMTEx3OsbN26oxesPNCHzCvR9bvtzXgeCvvE3NDQgNDQUS5YsQUlJCezs7JCRkdHjo0FVdExRURE2bdrEK0w9jXzpPvavvvqqR+MYiu7du4eysjJYW1sbeyhkAFy9elXtYgmDFqbffvsN6enp8PX1hZ+fn9btuseBqE6iq+JAwsPDue1UkSmqk7PdI1M6Ozv1tqEvUgXQHKuycOFCLFy4sJezYHhC5xXo+9y++OKLWL58eb/M60DQt/8VFRW4e/cuYmJiuIQDVa6Xiraxe3h44Nq1a7wPBN2jY4D7kS8RERFYs2aN2lGUPqormh52JSUleP/99/HII48YeyhkAPztb39DdXU1b5lBzjHdvn0bR48exXPPPYfm5mZ88sknGj/NqwiNA9EVmSKkDX2RKoD2WBVT0NN5Bfo+t25uboN6XvXtv5eXF8RiMTIyMqBQKHDt2jVkZ2cLGru+6BghkS+EEAG6Xwoh5Ko81VVp3f8NHz6cTZw4kf2///f/WGNjI7ftg7EeO3bs4NbpigNhTFhkir42GNMdqcIY0xqr8qD29na1/R45cqTOuVIRclWeoebVEHM7kPOqIuSqvAfjd3744Qc2ffp0td+HvvEfPHiQeXp6MltbWzZ9+nS2bNkyBoBJJBKdY9cVHSM08kUXuirvProq7+EyqCKJTDkypadMLZLIFOeWsvKoMKn0pDD97//+Ly+bjhhXZ2cnW7hwIe+DtD4Dcrk4IcS09VdcVH+3/aA9e/agoKAAc+bMQWFhIXdD9Isvvqh2EYyNjQ23PjIyckDGJ5S+qCsAyM/Ph4+PD6ysrDBp0iScOHGix/0IiczKyclRi0YTiUS8YIOqqiqEhYVBJpPByckJixYtQnNzM4D750mjo6Mxa9YstfPOPWGShWkoRaaYGppbMhTU19cjISEB6enpXOoJ+0+KRVFRETZs2MDbvrW1FfHx8SguLja5S/L1RV2VlpZi7ty5WLVqFWpraxEeHo6QkBDBj/1QERqZtX//frUHH6puLamrq0NAQAAmT56MH3/8EefPn0dpaSl27NjB/by/vz98fHyQmpra6zkxycJEkSn9h+Z2cOuvuCghUVX62u+vKCpNDh06hIkTJ6rdxuHk5ISwsDAkJyfj8OHDOtvQNZcPxn55enpCKpVi8eLFak9qTUhIgJOTk8b4K6F0RV2lpaUhODgY0dHRsLe3x+rVq+Ht7Y1t27b1qA9DRGalpqbimWeewapVq+Dk5AR3d3fec6FUwsLCkJWVpfP2DV1MsjARQjRbuXIlCgoKcOzYMdTU1GDChAkICgrC7du3kZWVhY0bN3Lb2tjYgDHGC7vNzMzExo0bMWXKFO7DiaenJzIzM7F79240Njbi/PnzKC8vR1lZGU6fPo0VK1YAgN72tbXdH0pKSuDj46O2XCQSISsrC76+voiMjMSVK1e0tqFrLtevX4/Dhw+js7MTR48exYULF/DZZ59h165dKCkp4dpITEzE119/jbKyMtTX1+Opp55CcHAwlEql4H3JycnBtGnTwBhDQ0MDNmzYADc3N8yePRsAcPLkSfj7+/N+JiAggPdEXiFU35KoqG5gtrKy4m33+eef4/HHH4ednR2mTp3KS0UpKirC+PHj8corr2D48OHw8PBAdXU11qxZw2tj3LhxuH79eo+P6lSoMBEySAxEDJeuGLC+MPRTbWtqarQmWEskEhQWFkIsFiM0NBQKhUJtG6Fz2dHRgWXLlkEqlSIwMBDe3t7ceRm5XI6dO3ciOTkZnp6eGDFiBDZv3oympibk5eX1eJ80RV0pFAooFAo4OjrytnV2dkZNTU2P++hOW2SWUqlEUVERrl+/jjlz5uCFF17A6dOn0dHRgZ9//hk7duxAWFgYGhoasGvXLmzbtg1bt27ltaH63dTW1vZqbFSYCBkkhMZN9YWuGLC+6P71sSHI5XJYWFhoXe/u7o68vDxcvXoVCxYsUOtX6Fyam5vzvi6UyWRcka6srER7eztCQkK4ry7FYjGUSmWvHlGTl5eHlpYWLFmyBAEBASguLtYYRQbcn8++3MSuLTIrMjISn3/+Oby8vCCVSrFixQpMnjwZGRkZ3FFgQEAAXnvtNdjY2GDGjBmIiopSu1hDdV9jbx/5QoWJkEFC15u6oeKidMWA9aX9iIgINDc3w8PDQ/BYdJHJZNyjSrQJDAzE9u3bkZ+fj7S0NN46oXM5bNgwretUbZw9e1btYoGUlBTB+9KdKuoqKCgImzZtgp2dHWxtbdHQ0MDbrqGhAaNGjepVHz2NzHrsscfw008/QSKRwMrKSi2f093dHdXV1Wrn3gDA3t6+V2OkwkTIINE9bklFFbek+uTf17goVVSVSvcYMCHtD1QUlYuLi6CnOsfFxSEmJgZr167lnZMRMpf6+Pj4wMLCos9Hqx4eHmofCLpHXU2bNg1lZWW89aWlpXjuued63FdNTQ1CQkKwevVqhIWFqa1/8skn0djYyFtWVVUFd3d3iEQiTJ48mbsoQ+Xnn3/m1quofje9LZ5UmAgZJAYihktXDJiQ9rW1behzTEFBQYJPrO/cuRP+/v68e3aERnfpIpPJEB8fj5SUFJw7dw5tbW24cOECxo4dizNnzgAAjh07BrFYrPPoTl/UVWJiIo4cOYLs7GzI5XJs2bIFFRUVvKvhhPQjJDKLMYb4+HjU1NTg9u3beP/993H+/HnEx8cDuH/ByFdffYX9+/ejtbUVJ06cwP79+/H222/z+qqoqMDo0aPh7e0taC7VdL/d1pSSH4YSU0t+MEWU/CAs+aE/46KExIDpa19b2wcPHmT29vasqqpK7z4KSX6oq6tjUqmUVVdXM8YYKygo4MVAFRcX87a/efMmc3V15S3XNZcPxn7J5XLm5eXFvX799dcZY4y1tbWxFStWMCcnJ2ZlZcV8fX15SRRLly5lkZGROvdFV9SVSm5uLvPy8mJisZj5+vqy48eP89rQ14/QyKzLly+zefPmMRcXF2ZnZ8emTp3K/vGPf/Da2rdvHxs7diyztLRk3t7eLCMjg3V1dfG2iYqKYuvWrdO53yqDKpJoKKHCpB8VJuNHEplKVJXQSKKPP/6YhYeH897ATck333zDQkND1Qr7YO1HqNLSUubn58eUSqWg7TUVJoM/j4kQQgbCG2+8ASsrK+Tm5uLVV1819nDU+Pn5oaCgYMj0I0RXVxf27NmDo0ePwtLSstftUGEihCA2Nha7du0CcP8CBrlcPihSQSIiIow9BNKNoZ4pRhc/EEIoqoqYFCpMhBBCTAoVJkIIISaFChMhhBCTwrv4wcLC4v+zd+9hUVX7/8DfgzAMDIwzKFAiMiIKqJD2NSQO6ulR8yRUWECGPCp9JbmUJpj085Y3QM0oAwvTFIXnlMERUkwT7KKASj4VdsJLkqYgyG1EODnIZf3+8Mz+sh0YZmBkBvy8noc/Zu/FWmuvgfnM3nutz0ZERITWC8yI9tra2tSy+BK+/jxGrIucZrroz8evT21tbZDL5YbuBulDqgz2KgLG9JRVkZBH2Nq1a2FtbY23337b0F0hpL+7Q5fyCCGEGBUKTIQQQowKBSZCCCFGhQITIYQQo0KBiRBCiFGhwEQIIcSoUGAihBBiVCgwEUIIMSoUmAghhBgVCkyEEEKMCgUmQgghRoUCEyGEEKNCgYkQQohRocBECCHEqFBgIoQQYlQoMBFCCDEqFJgIIYQYFQpMhBBCjAoFJkIIIUaFAhMhhBCjQoGJEEKIUaHARAghxKhQYCKEEGJUKDARQggxKgLGGDN0Jwjpb1paWuDh4YG6ujruNQCYmZkBAIYPH46ffvoJAoHAYH0kpJ+6Q2dMhPSAmZkZxo4di7q6OtTW1qKhoQENDQ2ora1FfX09JkyYQEGJkB6iwERIDy1atAgSiURtu7W1NRYtWmSAHhEyMNClPEJ6qLW1FUOGDMGdO3d424cMGYKamho6YyKkZ+hSHiE9ZWpqipdeegkmJia8bfPnz6egREgvUGAipBdee+01WFtbc68tLS2xcOFCw3WIkAGALuUR0guMMdjZ2aG2thbA/dl4N27cMHCvCOnX6FIeIb0hEAgQGhoKMzMzCIVC/O///q+hu0RIv0dnTIT00i+//IKpU6eCMYZffvkFo0aNMnSXCOnP7ug1MLW1taG1tVVf1RHSb7i4uMDKygq//PKLobtCSJ8zNTXFoEGD9FWdfgNTTEwMPvnkEwiFQn1VOSC0trZCIBDo840zOowxtLS0PLLvfXNzM9ra2mBpaWnorhhcc3MzzM3NDd0N0keam5vxzjvvYN26dfqq8o6pvmpSef/99xEVFaXvavu1mJgYuLi4DOhx+fnnnxEVFYXTp08buisGUVpaCm9vbzQ0NBi6KwZnZ2eHK1eudLr4mAw8mzdvhlKp1GudNPmBED0YNWoUrV0iRE8oMBFCCDEqFJgIMbAPPvgAL7zwgqG7YRCff/45Dhw4YOhukP9qb29HeHg4ty7PUCgwEWJgjDEYYtXGpk2b4O3t3eftquzZswfZ2dkICgpCTk4OBAIBBAIBnn/+ebXxsLKy4vaHhoYaqMedq6qqwttvvw1HR0dYWFhg3Lhx+PTTT3llDh48CHd3d4hEIkyYMAEnTpzQuZ3CwkL4+/vD2toaQ4cOhZ+fH3799VdemYyMDG6cOv40NTVxZcrKyhAYGAiZTAY7OzssXryYe3yLiYkJwsLC8Nxzz6G5ubkHo6EfFJgIMbCYmBgcPnzY0N3oU1VVVYiJiUFSUhJMTEwQEBAAxhikUilyc3Oxfv16XvmmpiZER0cjLy8PGRkZBup155YvX44zZ87g5MmTqK2tRVRUFBYvXozvvvsOwP2AMnfuXKxYsQI3b95EcHAw/P39cfHiRZ3amT59Ojw8PHDjxg2UlJQAAGbOnIn6+npeufT0dO7LjurHysoKAFBZWQlfX194eXnh0qVLKC4uRmFhIZKTk7nf9/Hxgbu7OxISEnozLL1CgYkQA1q3bh0EAgFEIpHa6xMnTsDFxQVSqRSRkZHcWURERAQEAgHkcjn27dsHR0dHWFpaIiQkBE1NTVi4cCEEAgF3NtTU1MR9c87JyeHqWLNmDc6ePcvtu3LlSp8d95dffgkPDw8MHz6ct93Ozg6BgYHYsGFDt8H63r17iImJgb29PSwsLPDMM8/g/PnzALQbx4512NnZqdWhi4SEBIwcORJisRiRkZEwNTVFWVkZACAxMRF+fn4ICwuDjY0NVq5cCTc3N2zbtk2nNuzt7ZGQkACpVAoHBwckJSXh1q1bOHXqlE79fOqpp7BixQrY2dlBLpcjJiZGrVxgYCDS0tIMciYPUGAixKDWrVvH+wBWvW5ra8PRo0dx7tw5fPXVV9i5cyfy8/MBAKmpqdi1axdqampQXFyMkpISFBUVoaCgALGxsUhLS8PGjRu5Oq2srMAYg1gs5ralpqZi48aNmDx5Mvet2sXFpc+OOz8/H+7u7mrbBQIB0tLS4OnpidDQUFy+fLnLOpYvX47s7GwcO3YMFRUVGD9+PGbMmIGGhgatxhEA4uLi8MMPP6CoqAhVVVV48skn4efnp9P054yMDEyZMgWMMVRXV2P9+vVwcnLCnDlzAAAnT56Ej48P73d8fX1x8uRJrdsAgD///JM383Pw4MEAwH2pUTl06BBGjx4NiUQCb29vHDt2jNuXm5uLcePG4eWXX4alpSWcnZ1RXl6O1atX8+oYO3Ysrl+/rvNZnb5QYCLECLW2tuKtt96CVCrFtGnT4ObmpnY/4d69e9iyZQtsbGwwYcIExMTEYO/evWrPh9LV559/jqFDh+Lq1au9qkeTiooKDB06tNN9YrEYOTk5EAqFCAgIQGNjo1oZhUKBTz75BCtXrsTEiRNhY2ODrVu3orm5GampqVw5TeOoUCiwY8cObNiwAS4uLhg8eDA2b96M2tpaZGVl6XxMQUFBsLe3R05ODrKysjBkyBA0NjaisbERtra2vLL29vaoqKjQuY2OVGeC06dP521XKpXIzc3F9evXERQUhNmzZ6OgoACtra24du0akpOTERgYiOrqauzcuRPbtm3D1q1beXWo3pubN2/2qo89RYGJECNkamrKu8wlk8nUAo69vT137wAAxo8fj5aWll5fkut4b+JhUSgUMDMz63K/XC5HVlYWfv/9dyxcuFCtLxcvXkRrays8PDy4bRYWFnBxceFditM0jhcvXkRLSwv8/f25y5lCoRBKpRIXLlzQ+ZiysrJQX1+PN998E76+vsjLy+P6/eAaN8ZYr9a93b59Gxs2bMD+/fthavp/eRJCQ0Nx6NAhuLq6QiqVIjY2Fl5eXkhJSeHOAn19ffHqq6/CysoKM2fOxPz589Uma6gyuDx4/6qvUGAixAg9mL6qsw+x9vZ23usHP7wffK1tHsuQkBDU1dXB2dlZq/I9IZPJ0NLSorHMtGnTsH37dhw8eBCJiYm8fZqCZsex0jSOqjrOnDmjNlkgPj5e62PpSCaTYdGiRZgxYwY2bdoEiUQCa2trVFdX88pVV1dj2LBhPWqjsbERc+bMQXx8PJ5++uluy48aNQpXr16FWCyGSCSCo6Mjb79cLkd5ebnavTcAsLGx6VEfe4sCEyH9VHV1NW7fvs29Li0thZmZGVxcXGBtbc1Lj1RZWak2/deQmSocHBy0WisTFRWF8PBwrFmzhndPxt3dHaampvj3v//NbVMqlSgrK+OdRWni7u4OMzOzHk126MjZ2VntS4JQKMR//vMfAMCUKVNQVFTE219YWIipU6fq3FZFRQX8/f2xcuVKBAYGqu2fOHEiampqeNvKysogl8shEAjg5eXFTcpQuXbtGrdfRfXe9DR49hYFJkL6KbFYjFWrVqG+vh4lJSVISkpCWFgYJBIJPD09cfnyZZw6dQq3b99GUlKSWoJdW1tbVFVVoampCeHh4UhJSQHQN/eYZsyYofWN9R07dsDHx4d3j00mkyEyMhKJiYkoKSmBQqFAXFwchEIhIiIitKpXJpMhOjoa8fHxOHv2LJqbm3Hu3DmMGTOGy/l47NgxCIVCjWd39fX1WLJkCTeWGRkZyMnJQUBAAID7EyyOHDmCffv2QaFQYMuWLSgtLeXNhtOmnfPnzyM4OBgff/wxZs6cCeD+JJJ33nmHK8MYQ3R0NCoqKtDQ0ID3338fxcXFiI6OBnB/wsj333+P9PR0NDU14cSJE0hPT8fSpUt5bZWWlmLEiBFwc3PTaiz1junRsmXL2I4dO/RZ5YDwKIzLTz/9xLy9vQ3dDYNRKpVMIpHo/HvvvvsuA8D9REdH814rFArm6urKvV6wYAFjjLFdu3YxJycnlpmZyZycnJiFhQWbO3cua2xs5Op+4403mFQqZSNHjmR5eXlMLBYzAOzdd99ljDFWU1PDJk+ezEQiEfPx8WF1dXWMMcYOHDjAbGxsWFlZWY/GwtbWljU0NGgsU1lZyaRSKSsvL2eMMZadnc077ry8PF75W7duMUdHR952pVLJli1bxmxtbZm5uTmbNm0a++WXXxhjjCUnJ2s1js3NzSw2NpbZ2dkxkUjEPD092RdffMG1sWTJEhYaGqrxWE6fPs3mzJnDHn/8cSYWi9nYsWPZe++9x9ra2rgymZmZzNXVlQmFQubp6cmOHz/Oq6O7du7evcusra15x6T6iYuL48r99ttvbN68eczBwYFJJBLm7e3Nvv76a15d+/fvZ2PGjGHm5ubMzc2NpaSksPb2dl6Z+fPns7Vr12o8bpXExETub0pPGigw9YFHYVx0CUxHjhxhkydPZhKJhPvnmjVr1kPu4cPV08DUU6rAZIy0CUyMMfbZZ5+x4OBg3ge4Mfnxxx9ZQEAAL9j353a0VVhYyCZNmsSUSqVW5R9GYOrzS3kFBQW8VBm+vr5qZVpbW7F+/XqMGTMGFhYWGDZsGMLCwnDmzBmujIuLS6epNzr+qBbZCQQCDB8+vMu1CR999BFXbuHChZ2W0SYdSG/013HRVVVVFV566SWEhYXh5s2bYIwhLi5OL3WrdJVqp7vUMQ/7PSZ8r732Gl588UVkZmYauiudmjRpErKzs3kzH/tzO9pob2/Hnj17cPToUYM+U6vPA5Ovry8YYxgyZAjee+89FBQUqJVZv349Pv30U+zduxcKhQI///wzHBwc8PTTT3M5n0xNTbkZNKrrsh1TccybNw/+/v5gjHFrBjqub1C5e/cuN4c/Ly8PaWlpnfZb23QgPdVfx0VX58+fR1tbG15//XXegs++0F3qmIf9HutLREQEwsPDuQWXHSdA9DchISF45ZVXDN0N8l8mJibYvXt3l2vM+qwfBm29C/n5+QgKCsLf/vY3iEQi2NvbY9OmTRgzZkyP6hs6dChmz56NzZs3czNlVFJSUvCPf/yj2zr0kQ6kt4xxXHSxbt06zJo1C62trTAxMeGtv+hIU6oZ4P76k4CAAMhkMjg6OmLFihVcEO4u1Y6m1DHG8B5rIzU1lTe1WSqVGrpLhOiVUQamESNGID8/H5WVlbztly5d4k53u5vRk5GRgUmTJnGvExMTUVNTw808Au7nENu/fz+WLVvWbZ+0TQfyMBnjuOhClSbG3NwcjLEu19VoSjUDAEuXLkV4eDiqqqrw7bff4ttvv0VSUhIAzal2uksdYwzvMSHESANTfHw82tvbIZfL4e/vj+3bt/d66qqnpyfmzZuHrVu3ciu/P/zwQ0RFRfXoklJX6UAepv4wLr2lTaqZb775Bn5+fjA3N8fo0aMxb948Xj6w7nSWOqYzhniPCSFGGphcXFzw66+/4tChQ3B2dsbOnTsxatQozJs3j1uR3BMbN27Ef/7zH3z44YdoaGjAoUOHsGjRIp3r6SodyMNm7OOiD9qkmsnLy4OXlxfEYjEEAgFiYmJ0erBZZ6ljHmSo95gQAhjtf9ygQYMwa9YszJo1CwDwww8/YNasWZgyZYrWC+ge5OTkhMjISCQlJaGmpgbLly/vNF/X7t27ER4ezr2+ceMGl29L13Qg+mbIcekLrJtUM9XV1QgICMCbb76J/Px8SCQSpKSkdDqBQxNV6pjc3Fxs2rSJW7AI9Pw9bmlpQXBwsE79GIiampqwYMECg/0Nkb518eJF7nK4vhhdYHJzc4OLiwsSEhLg6enJbZ82bRpGjBiBqqqqXtW/atUq7NmzB6dPn8ZHH33UaZlFixZ1esZQUVGBkJAQrF69mvdB1heMYVz6QsdUM6op36pUM8HBwSgtLcVff/2F8PBwSCQSAFA7W+wq1Y6zszOuXLkCE5P/u1DQMXUM0Lv3eNCgQUb3dFVDOH78OF555RVYWloauiukD/zrX//Se51GF5hUlixZgo8++giurq64c+cO0tPT8ccff+DZZ5/tVb1Dhw7Ft99+C5lMplOusPPnzyMyMhKffvopxo0bB+D+LLn8/Hxs3ry5V33ShbGNi751TDXz1FNPYcSIEVi3bh2Xauavv/6CUChESkoKNmzYgFu3bmHfvn28Ojqm2lm2bBmeeOIJvPHGG1zqmNWrV8PKygo5OTnIycnB2rVrAfT+PTYxMcELL7yg/0HpZ4RCIWbPns19cSADW2lpqU7Pr9KKPpfrapPh4NSpU52m1VD9mJubs/LycrZ69Wr2xBNPMLFYzGxsbNjf/vY3lpOTo1ZfZmYm7/dffvnlLts6ffq02u8/mLpk6dKlamW0TQfyqI1LR9pkfngw/c6FCxfY9OnTuddDhgxhjGlONcPY/ZQ5Li4uzNramk2fPp299dZbDAATi8WMsa5T7WhKHdPb97ivMz8YM20zP5CB4WFkfhAwpr+HrsTExMDFxQVRUVH6qnJAeBTG5eeff0ZUVBSX/PJR09zcDDs7O15G70eVnZ0drly5QmdMj4jNmzdDqVRi3bp1+qryjlHOyiOEPBo+//xzHDhwwNDdIP/V3t6O8PBwnWa5PgwUmAjpZ7rKBWjsdT9oz549yM7ORlBQEHJycrhMHc8//7za7EwrKytuv7FNMOkuByMAHDx4EO7u7hCJRJgwYQJOnDihczva5HLMyMjoND+mKmUZcP/5TIGBgZDJZLCzs8PixYtRV1cH4P590rCwMDz33HNqz+/qSxSYCCF9rqqqCjExMUhKSoKJiQkCAgK49Eq5ublYv349r3xTUxOio6ORl5eHjIwMA/W6c93lYCwsLMTcuXOxYsUK3Lx5E8HBwfD399f6eVQq2uZy7JgbU/WjygxTWVkJX19feHl54dKlSyguLkZhYSGSk5O53/fx8YG7uzsSEhJ6Myy9QoGJkD6kKQ/gwoULIRAIuDOWpqYm7htvTk4OgK5zAUZEREAgEEAul2Pfvn1wdHSEpaUlQkJCuG/L3dXfXZ5Bffryyy/h4eHBrQ9UsbOzQ2BgIDZs2IDDhw9rrEPTWKoy6ItEIi6Dh1QqRWRkpNojxGNiYmBnZ9dpXkZtacrBmJiYCD8/P4SFhcHGxgYrV66Em5sbtm3bplMb+sjlmJCQgKeeegorVqyAnZ0d5HI574GFKoGBgUhLS9O4rvBhosBESB/SlAcwLS0NGzdu5MpaWVmBMcZLDdVVLsDU1FTs2rULNTU1KC4uRklJCYqKilBQUIDY2FgA6LZ+TXkG9S0/Px/u7u5q2wUCAdLS0uDp6YnQ0FBcvny5yzo0jaUqL2NbWxuOHj2Kc+fO4auvvsLOnTuRn5/P1REXF4cffvgBRUVFqKqqwpNPPgk/Pz+dpj93l4Px5MmT8PHx4f2Or68v71Hx2tA2l+OhQ4cwevRoSCQSeHt789J15ebmYty4cXj55ZdhaWkJZ2dnlJeXY/Xq1bw6xo4di+vXr+t8VqcvFJgI6SPa5AHsrXv37mHLli2wsbHBhAkTEBMTg71793J5EHtK349br6io6PLRCmKxGDk5ORAKhQgICEBjY6NaGW3HsrVCDr9JAAAgAElEQVS1FW+99RakUimmTZsGNzc37r6MQqHAjh07sGHDBri4uGDw4MHYvHkzamtrkZWVpfMxdZaDsbGxEY2NjbC1teWVVT1ypje6yuWoVCqRm5uL69evIygoCLNnz0ZBQQFaW1tx7do1JCcnIzAwENXV1di5cye2bdvGPeJGRfXe3Lx5s1d97CkKTIT0EW3yAPaWvb0974Fz48ePR0tLS68vyXW8X6EPCoVCY8oiuVyOrKws/P7771i4cKFau9qOpampKe9yoUwm44L0xYsX0dLSAn9/f+7SpVAohFKpxIULF3Q+ps5yMKr6/eCidcZYrxayd5XLMTQ0FIcOHYKrqyukUiliY2Ph5eWFlJQU7izQ19cXr776KqysrDBz5kzMnz9fbbKGUCgEAIM9i4wCEyF9RNOHescPqQfLdfV4kM60t7d322ZP6g8JCUFdXR2cnZ217osmMpmMe4ZWV6ZNm4bt27fj4MGDSExM5O3TdiwHDRrU5T5VHWfOnFGbLBAfH6/1sXSkysE4Y8YMbNq0CRKJBNbW1qiuruaVq66uxrBhw3rUhq65HEeNGoWrV69CLBZDJBLB0dGRt18ul6O8vFzt3hsA2NjY9KiPvUWBiZA+0jEPoIoqD6Dqm7+1tTVvkW5lZaXatF1N37Srq6t5T7QtLS2FmZkZd6+ou/r7Kh2Vg4ODVmtloqKiEB4ejjVr1vDuyWgzlt1xd3eHmZlZr89WnZ2d1b4QdMzBOGXKFBQVFfH2FxYWYurUqTq3VVFRAX9/f6xcuRKBgYFq+ydOnIiamhretrKyMsjlcggEAnh5eXGTMlSuXbvG7VdRvTc9DZ69RYGJkD7SMQ9gSUkJFAoF4uLiuDyAwP3nY12+fBmnTp3C7du3kZSUxF1WUemYCzA8PJz3kEexWIxVq1ahvr4eJSUlSEpKQlhYGJeFobv6u6pb3/eYZsyYofWN9R07dsDHx4e3ZkebseyOTCZDdHQ04uPjcfbsWTQ3N+PcuXMYM2YMl8Hk2LFjEAqFGs/uVDkYVeOWkZGBnJwcBAQEALg/weLIkSPYt28fFAoFtmzZgtLSUt5sOG3aOX/+PIKDg/Hxxx9zCYbz8/PxzjvvcGUYY4iOjkZFRQUaGhrw/vvvo7i4GNHR0QDuTxj5/vvvkZ6ejqamJpw4cQLp6elYunQpr63S0lKMGDECbm5uWo2l3ukzwZE2OeEeRY/CuGiTK28g0zZXXnd5ABlj7I033mBSqZSNHDmS5eXlMbFYzABw+ci6ygW4a9cu5uTkxDIzM5mTkxOzsLBgc+fOZY2NjVrX31XdBw4cYDY2NqysrKzbY9QmV15lZSWTSqWsvLycMcZYdnY2Lz9hXl4er/ytW7eYo6Mjb7umsXww16NCoWCurq7c6wULFjDGGGtubmaxsbHMzs6OiUQi5unpyb744guujSVLlrDQ0FCNx6IpB6NKZmYmc3V1ZUKhkHl6erLjx4/z6uiuHW1zOf72229s3rx5zMHBgUkkEubt7c2+/vprXl379+9nY8aMYebm5szNzY2lpKSw9vZ2Xpn58+eztWvXajxulYeRK48CUx94FMaFApPhk7iqApOhaZvE9bPPPmPBwcG8D3Bj8uOPP7KAgAC1wN5f29FWYWEhmzRpElMqlVqVfxiByWgfe0EIGdhee+01iEQiZGZm4pVXXjF0d9RMmjQJ2dnZA6YdbbS3t2PPnj04evQozM3NDdYPCkyEDAARERHYuXMngPsTGBQKBaRSqYF71b2QkBBDd4F0YGJigt27dxu6GzT5gZCBIDU1lTfduT8EJUK6QoGJEEKIUaHARAghxKjo/R7Tr7/+ikOHDum72n6trKwMf/3114Aelz/++AMKhWJAH6MmLS0taG1tfWSPv6N79+7h66+/hqWlpaG7QvrAhQsXMHLkSL3WqddHq+/fvx+5ubn6qm7AUCqVMDExUVsoOZC0tbXh7t27vDxtj5IbN26gubn5oWXj7k/u3LkDa2vrPssiQQwvKCgIQUFB+qrujl4DEyGPqrVr18La2hpvv/22obtCSH93h+4xEUIIMSoUmAghhBgVCkyEEEKMCgUmQgghRoUCEyGEEKNCgYkQQohRocBECCHEqFBgIoQQYlQoMBFCCDEqFJgIIYQYFQpMhBBCjAoFJkIIIUaFAhMhhBCjQoGJEEKIUaHARAghxKhQYCKEEGJUKDARQggxKhSYCCGEGBUKTIQQQowKBSZCCCFGhQITIYQQo0KBiRBCiFGhwEQIIcSomBq6A4T0R21tbfjzzz+51wqFAi0tLfjjjz+4bSNHjoRAIDBE9wjp1wSMMWboThDS3zDG4ODggMbGRpiZmaG9vR0AYGJignv37uHxxx/H77//buBeEtIv3aFLeYT0gEAgwNy5c9Hc3AyFQoGGhgY0NDRAoVCgtbUV8+fPN3QXCem3KDAR0kMLFiyASCRS225qaorQ0FAD9IiQgYECEyE99MQTT0Aqlaptd3JywsiRIw3QI0IGBgpMhPTCa6+9BnNzc+61paUlXn/9dQP2iJD+jyY/ENILf/zxBzw8PPDXX38BuB+Y/vjjD9jb2xu4Z4T0WzT5gZDecHZ2xogRI7jXHh4eFJQI6SUKTIT00uLFiyEWi2FlZYXFixcbujuE9Ht0KY+QXqqqqoKzszMAoLKyEoMHDzZwjwjp1+7wMj/89NNPKCsrM1RnCOm3hg0bBktLSxw/ftzQXSGk33Fzc4OHhwf3mnfGFBERgV9//ZX79kf0o6WlBd999x2effZZQ3fFaDU1NeHcuXP4+9//buiu9EhZWRnMzMx495t0lZubC39/fz32qn/Kz8/HlClTeLMdycB16dIl/P3vf8fWrVtVm+6oBSZvb28sXLjQIB0cqBQKBcaOHYvKykpDd8VoXbp0CcHBwSgpKTF0V3qkoaEB5ubmnS641ZZIJIJSqdRjr/onuVyOM2fO4LHHHjN0V0gf+Oijj1BeXs4LTJTElRA9oPtKhOgPzcojhBBiVCgwEdLPffDBB3jhhRcM3Q2D+Pzzz3HgwAFDd4P8V3t7O8LDw1FbW9ureigwEdLPMcZgiFUfmzZtgre3d5+3q7Jnzx5kZ2cjKCgIOTk5EAgEEAgEeP7559XGw8rKittvbAl2q6qq8Pbbb8PR0REWFhYYN24cPv30U16ZgwcPwt3dHSKRCBMmTMCJEyd0bqewsBD+/v6wtrbG0KFD4efnh19//ZVXJiMjgxunjj9NTU1cmbKyMgQGBkImk8HOzg6LFy9GXV0dgPuPfQkLC8Nzzz2H5ubmHozGfRSYCOnnYmJicPjwYUN3o09VVVUhJiYGSUlJMDExQUBAABhjkEqlyM3Nxfr163nlm5qaEB0djby8PGRkZBio151bvnw5zpw5g5MnT6K2thZRUVFYvHgxvvvuOwD3A8rcuXOxYsUK3Lx5E8HBwfD398fFixd1amf69Onw8PDAjRs3uElGM2fORH19Pa9ceno692VH9WNlZQXg/jo9X19feHl54dKlSyguLkZhYSGSk5O53/fx8YG7uzsSEhJ6PCYUmAjpx9atWweBQMDNBuz4+sSJE3BxcYFUKkVkZCR3FhEREQGBQAC5XI59+/bB0dERlpaWCAkJ4b4ZL1y4EAKBgDsjampq4r495+TkICIiAmvWrMHZs2e57VeuXOmz4/7yyy/h4eGB4cOH87bb2dkhMDAQGzZs6DZY37t3DzExMbC3t4eFhQWeeeYZnD9/HoB249ixDjs7O7U6dJGQkICRI0dCLBYjMjISpqam3JrSxMRE+Pn5ISwsDDY2Nli5ciXc3Nywbds2ndqwt7dHQkICpFIpHBwckJSUhFu3buHUqVM69fOpp57CihUrYGdnB7lcjpiYGLVygYGBSEtL6/GZPAUmQvqxdevW8T6AVa/b2tpw9OhRnDt3Dl999RV27tyJ/Px8AEBqaip27dqFmpoaFBcXo6SkBEVFRSgoKEBsbCwAIC0tDRs3buTqtbKyAmMMYrGYq2Pjxo2YPHky963axcWlz447Pz8f7u7uatsFAgHS0tLg6emJ0NBQXL58ucs6li9fjuzsbBw7dgwVFRUYP348ZsyYgYaGBq3GEQDi4uLwww8/oKioCFVVVXjyySfh5+en07T/jIwMTJkyBYwxVFdXY/369XBycsKcOXMAACdPnoSPjw/vd3x9fXHy5Emt2wCAP//8EwKBgHutmkn64BKHQ4cOYfTo0ZBIJPD29saxY8e4fbm5uRg3bhxefvllWFpawtnZGeXl5Vi9ejWvjrFjx+L69es6n9WpUGAiZABqbW3FW2+9BalUimnTpsHNzU3tfsK9e/ewZcsW2NjYYMKECYiJicHevXtx586dXrX9+eefY+jQobh69Wqv6tGkoqICQ4cO7XSfWCxGTk4OhEIhAgIC0NjYqFZGoVDgk08+wcqVKzFx4kTY2Nhg69ataG5uRmpqKldO0zgqFArs2LEDGzZsgIuLCwYPHozNmzejtrYWWVlZOh9TUFAQ7O3tkZOTg6ysLAwZMgSNjY1obGyEra0tr6y9vT0qKip0bqMj1Zng9OnTeduVSiVyc3Nx/fp1BAUFYfbs2SgoKEBrayuuXbuG5ORkBAYGorq6Gjt37sS2bds6rkECAO69uXnzZo/6RoGJkAHI1NSUd5lLJpOpBRx7e3vu3gEAjB8/Hi0tLb2+JNfx3sTDolAoYGZm1uV+uVyOrKws/P7771i4cKFaXy5evIjW1lZeGhwLCwu4uLjwLsVpGseLFy+ipaUF/v7+3OVMoVAIpVKJCxcu6HxMWVlZqK+vx5tvvglfX1/k5eVx/e54pgPcH+MHt+ni9u3b2LBhA/bv3w9T0/9bzhoaGopDhw7B1dUVUqkUsbGx8PLyQkpKCncW6Ovri1dffRVWVlaYOXMm5s+frzZZQygUAoDa/SttUWAiZAAaNGgQ73VnH2Lt7e28150Fkge3tba2dtt2SEgI6urqHmpqM5lMhpaWFo1lpk2bhu3bt+PgwYNITEzk7dMUNDuOlaZxVNVx5swZtckC8fHxWh9LRzKZDIsWLcKMGTOwadMmSCQSWFtbo7q6mleuuroaw4YN61EbjY2NmDNnDuLj4/H00093W37UqFG4evUqxGIxRCIRHB0defvlcjnKy8vV7r0BgI2NTY/6SIGJkEdUdXU1bt++zb0uLS2FmZkZd6/I2toaDQ0N3P7KykreFODefGPvLQcHB63WykRFRSE8PBxr1qzh3ZNxd3eHqakp/v3vf3PblEolysrKeGdRmri7u8PMzKxHkx06cnZ2VvuSIBQK8Z///AcAMGXKFBQVFfH2FxYWYurUqTq3VVFRAX9/f6xcuRKBgYFq+ydOnIiamhretrKyMsjlcggEAnh5eakl+r527Rq3X0X13vQ0eFJgIuQRJRaLsWrVKtTX16OkpARJSUkICwuDRCIBAHh6euLy5cs4deoUbt++jaSkJO4SDQDY2tqiqqoKTU1NCA8PR0pKCoC+ucc0Y8YMrW+s79ixAz4+Prx7bDKZDJGRkUhMTERJSQkUCgXi4uIgFAoRERGhVb0ymQzR0dGIj4/H2bNn0dzcjHPnzmHMmDE4ffo0AODYsWMQCoUaz+7q6+uxZMkSbiwzMjKQk5ODgIAAAPcnWBw5cgT79u2DQqHAli1bUFpaypsNp00758+fR3BwMD7++GPMnDkTwP1JJO+88w5XhjGG6OhoVFRUoKGhAe+//z6Ki4sRHR0N4P6Eke+//x7p6eloamrCiRMnkJ6ejqVLl/LaKi0txYgRI+Dm5qbVWKphHSxevJjt3buXEf2qr69njz32mKG7YdQuXrzIPD09Dd0NgzI3N9f5d959910GgPuJjo7mvVYoFMzV1ZV7vWDBAsYYY7t27WJOTk4sMzOTOTk5MQsLCzZ37lzW2NjIq/+NN95gUqmUjRw5kuXl5TGxWMwAsHfffZfV1NSwyZMnM5FIxHx8fFhdXR1jjLEDBw4wGxsbVlZW1qNxcHJyYpWVlRrLVFZWMqlUysrLyxljjGVnZ/OOOy8vj1f+1q1bzNHRkbddqVSyZcuWMVtbW2Zubs6mTZvGfvnlF8YYY8nJyVqNY3NzM4uNjWV2dnZMJBIxT09P9sUXX3BtLFmyhIWGhmo8ltOnT7M5c+awxx9/nInFYjZ27Fj23nvvsba2Nq5MZmYmc3V1ZUKhkHl6erLjx4/z6uiunbt37zJra2veMal+4uLiuHK//fYbmzdvHnNwcGASiYR5e3uzr7/+mlfX/v372ZgxY5i5uTlzc3NjKSkprL29nVdm/vz5bO3atRqPW2X79u3s7bff7ripgQJTH6DA1D1dAtORI0fY5MmTmUQi4f65Zs2a9ZB7+PD1JDD1lCowGSNtAhNjjH322WcsODiY9wFuTH788UcWEBCgFuz7azvaKiwsZJMmTWJKpVKr8p0FJp0v5RUUFKilq7C2tsYTTzyBVatW9TpHUl/TNq2KNuk8euNRHVddVVVV4aWXXkJYWBhu3rwJxhji4uL02kZXfe8udczD/hshfK+99hpefPFFZGZmGrornZo0aRKys7N5Mx/7czvaaG9vx549e3D06NFePU9L58Dk6+sLxhiGDBmC9957D4wx3LhxA5s3b8bx48fh6enZo6mSxk7bdB499aiOq67Onz+PtrY2vP7669xiz77SXeqYh/03oi8REREIDw/nFlx2nADR34SEhOCVV14xdDfIf5mYmGD37t1drjHTWsfzJ10u5Q0ZMoS99957vG1//fUXGz9+PPPw8GBtbW3c9W9zc3OWn5/PXF1dmUAgYC+++CJrbm5my5Yt467N/v3vf2clJSVcPwAwJycnlpaWxoYPH84sLCzYq6++yjtd1VTHggULGAA2efJkxhhjjY2N3GWf7OxsXjsdf37//fdOj3fEiBG866gXL15kAFhOTk63Y6XLpbzejmt346LN2PbluHYcz+4u5T14P2XQoEGMMcbi4uJ4l/I09Z8xxi5cuMBefPFFJpVK2fDhw9nbb7/N7t27123f582bx06ePMnV09bWxkxNTdmuXbsYY737G2Gsby/lGTNtL+WRgUGv95g6+wBljLG9e/cyAOzMmTOMMcYOHz7MBg0axBYvXsxqampYdnY2e/HFF9mbb77J5HI5++mnn1hdXR174403mK2tLbt9+zZj7P41cEtLSxYVFcXq6urYzz//zBwdHdnrr7/OtdVdHRs3buQ+QFXEYjH3AdpVGW1UVlYyAOzYsWPdlu1tYGJM+3FlrPtx6W5sDTGu2t5jOnz4sNoH+IOBqbv+P/vssyw3N5cplUp2+fJl9j//8z9s8+bNWve9vb2d3bp1i61du5aNGjWK1dbWdlpOl78RxigwqVBgerTo5R5Td5544gkA96cLqrS1tWHVqlUYOnQoAgICsHfvXq3SgWhKmaJtSpGHpat0Hg+LNuOak5Oj9bh0NbZ//vmnQce1t7Q5/m+++QZ+fn4wNzfH6NGjMW/ePF4+sO50ljqmM339N0LIQKH3wMQ6SaFhZmbGWy2sbToQTSlTtK3jYegqncfDpM24Ar0f29zcXIONqz5oc/x5eXnw8vKCWCyGQCBATEyMTpNLOksd8yBD/I0QMlDo/T/ml19+AXA/u6yKiQk//jEt04FoSpmibR0PltMmpcru3bsRHh7Ovb5x4waXL0vXdB76os24Ar0f24c5rn2hu/5XV1cjICAAb775JvLz8yGRSJCSkqLz2aAqdUxubi42bdrELVgEev430tbWppbp+VEkEAjg5ORk0MwSpO+0tbVxC3hV9BqY7t69i6SkJHh6emLSpEldluuYDkQ1LVeVDiQ4OJgrp0qZIpVKAfBTprS1tXVbR3cpVYDO06osWrQIixYtUtteUVGBkJAQrF69mvdB9LBpO65A78f2+eefx7Jlyx7KuPaF7o6/tLQUf/31F8LDw7kMB6q8Xipd9d3Z2RlXrlzhfSHomDoG6N3fyKBBg3R6XMJAJZfLcebMGTz22GOG7grpAx999BHKy8t52/RyKa+hoQFHjx7F1KlTUVdXhy+++KLTb/Mq2qYD0ZQyRZs6ukupAnSdVuVB2qTz0DddxxXo/dg6OTn16bjqW3fH7+rqCqFQiJSUFDQ2NuLKlSvYt2+fVn3vLnWMIf5GCBmQOk6F0GZW3qlTp9Sm01paWjIPDw/2//7f/2M1NTVc2QfTeiQnJ3P7NKUDYUy7lCnd1cGY5pQqjLEu06p0pG06j65oMytPX+Oqj7Htq3HtqCfTxS9cuMCmT5/OvR4yZIhW/T9w4ABzcXFh1tbWbPr06eytt95iAJhYLNbYd02pY3r7N8IYzcpToVl5j5Z+lZLImFOm6MrYUhIZ49hSrjwKTCq6BKZ//vOfvNx0xLDa2trYokWLeF+ku9Mn08UJIcbtYaWLeth1P2jPnj3Izs5GUFAQcnJyuFRezz//vNokGCsrK25/aGhon/RPW92lugKAgwcPwt3dHSKRCBMmTMCJEyd0bkeblFkZGRlqqdEEAgGampq4MmVlZQgMDIRMJoOdnR0WL16Muro6APcnZIWFheG5555Tu++sC6MMTAMpZYqxobElA0FVVRViYmKQlJQEExMTBAQEgDEGqVSK3NxcrF+/nle+qakJ0dHRyMvLQ0ZGhoF63bnuUl0VFhZi7ty5WLFiBW7evIng4GD4+/tr/dgPFW1TZqWnp6s9+FC1tKSyshK+vr7w8vLCpUuXUFxcjMLCQiQnJ3O/7+PjA3d3dyQkJPR4TIwyMKWmpvIGRTVzjPQejW3/du/ePcTExMDe3h4WFhZ45plnuPVZCxcuhEAg4M5YmpqauG+8OTk5AO5/MVmzZg3Onj3L7bty5QoiIiIgEAggl8uxb98+ODo6wtLSEiEhIdy35e7q76ruh+HLL7+Eh4cH77HnAGBnZ4fAwEBs2LABhw8f1liHprFct24dBAIBRCIRt1BaKpUiMjJS7UmtMTExsLOzU6tDFwkJCRg5ciTEYjEiIyNhamrKPZAvMTERfn5+CAsLg42NDVauXAk3Nzds27ZNpzbs7e2RkJAAqVQKBwcHJCUl4datWzh16pRO/XzqqaewYsUK2NnZQS6X854LpRIYGIi0tDSNyzc0McrARAjp3PLly5GdnY1jx46hoqIC48ePx4wZM9DQ0IC0tDRs3LiRK2tlZQXGGC/ZbWpqKjZu3IjJkydzX05cXFyQmpqKXbt2oaamBsXFxSgpKUFRUREKCgoQGxsLAN3W31XdD0N+fj7c3d3VtgsEAqSlpcHT0xOhoaG4fPlyl3VoGst169bh8OHDaGtrw9GjR3Hu3Dl89dVX2LlzJ/Lz87k64uLi8MMPP6CoqAhVVVV48skn4efnp9O0/4yMDEyZMgWMMVRXV2P9+vVwcnLCnDlzAAAnT56Ej48P73d8fX15T+TVhuoqicrgwYMBQG3t3KFDhzB69GhIJBJ4e3vzsqLk5uZi3LhxePnll2FpaQlnZ2eUl5dj9erVvDrGjh2L69ev63xWp0KBiZB+oi/ScGlKA9Yb+n6qbUVFRZcZrMViMXJyciAUChEQEIDGxka1MtqOZWtrK9566y1IpVJMmzYNbm5u3H0ZhUKBHTt2YMOGDXBxccHgwYOxefNm1NbWIisrS+dj6izVVWNjIxobG2Fra8sra29vj4qKCp3b6KirlFlKpRK5ubm4fv06goKCMHv2bBQUFKC1tRXXrl1DcnIyAgMDUV1djZ07d2Lbtm3YunUrrw7Ve3Pz5s0e9Y0CEyH9RF+k4dKUBqw3Ol4+1geFQgEzM7Mu98vlcmRlZeH333/HwoUL1drVdixNTU15lwtlMhkXpC9evIiWlhb4+/tzly6FQiGUSmWPHlHTWaorVb8fXPTNGOvVIvauUmaFhobi0KFDcHV1hVQqRWxsLLy8vJCSksKdBfr6+uLVV1+FlZUVZs6cifnz56tN1lCta+zpI18oMBHST2j6UNdXuihNacB6U39ISAjq6urg7OysdV80kclkaGlp0Vhm2rRp2L59Ow4ePIjExETePm3HctCgQV3uU9Vx5swZtckC8fHxWh9LR6pUVzNmzMCmTZsgkUhgbW2N6upqXrnq6moMGzasR23omjJr1KhRuHr1KsRiMUQikVp+TrlcjvLycrV7bwBgY2PToz5SYCKkn+iYbklFlW5J9c2/t+miVKmqVDqmAdOm/r5KReXg4KBV4t2oqCiEh4djzZo1vHsy2oxld9zd3WFmZtbrs1VnZ2e1LwQdU11NmTIFRUVFvP2FhYWYOnWqzm1VVFTA398fK1euRGBgoNr+iRMnoqamhretrKwMcrkcAoEAXl5e3KQMlWvXrnH7VVTvTU+DJwUmQvqJvkjDpSkNmDb1d1W3vu8xzZgxQ+sb6zt27ICPjw9vzY62qbs0kclkiI6ORnx8PM6ePYvm5macO3cOY8aMwenTpwEAx44dg1Ao1Hh2112qq7i4OBw5cgT79u2DQqHAli1bUFpaypsNp0072qTMYowhOjoaFRUVaGhowPvvv4/i4mIuyery5cvx/fffIz09HU1NTThx4gTS09OxdOlSXlulpaUYMWIE3NzctBpLNR2X2xpT5oeBxNgyPxgjyvygXeaHh5kuSps0YN3V31XdBw4cYDY2NqysrKzbY9Qm80NlZSWTSqWsvLycMcZYdnY2Lw1UXl4er/ytW7eYo6Mjb7umsXww7ZdCoWCurq7c6wULFjDG7j8tOTY2lntasqenJy8TxZIlS1hoaKjGY9GU6kolMzOTubq6MqFQyDw9Pdnx48d5dXTXjrYps3777Tc2b9485uDgwCQSCfP29mZff/01r679+/ezMWPGMHNzc+bm5sZSUlJ4T25mjLH58+eztWvXajxulX6VkmggocDUPQpMhk9JZCypqrRNSfTZZ5+x4OBg3ge4Mfnxxx9ZQECAWmDvr+1oq7CwkE2aNPstPZQAACAASURBVIkplUqtyncWmOgJZoSQfum1116DSCRCZmYmXnnlFUN3R82kSZOQnZ09YNrRRnt7O/bs2YOjR4/C3Ny8x/VQYCKEICIiAjt37gRwfwKDQqHoF1lBQkJCDN0F0oGJiQl2797d+3r00BdCSD9HqaqIMaHARAghxKhQYCKEEGJUKDARQggxKmqTH/bu3YuCggJD9GXAamlpQVtbGxYtWmTorhitxsZG1NfXP9JjJBKJHunjV1EqlVi+fLla1msyMJWWlsLX15e3TcDY/yU4Ki4u1pgmnhDSuX/9618QiUTw8/MzdFcI6XfGjRuHiRMnql7e4Z0xeXl5wcvLq+97RUg/d/nyZVhbWxvdY7sJ6Y/oHhMhhBCjQoGJEEKIUaHARAghxKhQYCKEEGJUKDARQggxKhSYCCGEGBUKTIQQQowKBSZCCCFGhQITIYQQo0KBiRBCiFGhwEQIIcSoUGAihBBiVCgwEUIIMSoUmAghhBgVCkyEEEKMCgUmQgghRoUCEyGEEKNCgYkQQohRocBECCHEqFBgIoQQYlQoMBFCCDEqFJgIIYQYFQpMhBBCjIqpoTtASH/U0tKCDz74AIwxAEBxcTHMzc3R3t4OADA1NUVMTAwEAoEhu0lIvyRgqv8sQohOxo0bh9LS0k73Pf300ygqKurjHhEyINyhS3mE9NDixYshFovVtltZWSEiIsIAPSJkYKAzJkJ6qKamBk5OTrh79y5vu4WFBSorKzF48GAD9YyQfo3OmAjpKVtbW3h6eqptnzp1KgUlQnqBAhMhvRAREQFra2vu9eDBg7F48WID9oiQ/o8u5RHSC3fu3MFjjz3GXc4Ti8Wora2FSCQycM8I6bfoUh4hvSGRSDBlyhQAgEAggJ+fHwUlQnqJAhMhvfT6669DIpFAIpEgPDzc0N0hpN+jS3mE9JJSqcSQIUNgZmaGuro6DBo0yNBdIqQ/u6PXzA///Oc/cfToUX1WOSAolUqYmJhAKBQauisPTVtbG5RKZafreh4FQ4cOhUAgwMKFCw3dFYNrbGyElZUVZb14hLz00kuYM2eO3urTa2A6d+4cRCIRnn32WX1W2+/t27cPjz32GGbNmmXorjw0V69exe7duxEfH2/orhjE8OHD8cEHH+CFF14wdFcMbtGiRUhOToaFhYWhu0L6QE5ODkpKSow3MAHAxIkTERQUpO9q+7XTp0/DxcVlQI/Lzz//jOzs7AF9jJr4+fnh448/fmSPv6Po6GgEBARAIpEYuiukD5SVlUGpVOq1Tpr8QIge0H0lQvSHAhMhhBCjQoGJEAN7lO9Nff755zhw4IChu0H+q729HeHh4aitrTVoPygwEWJgjDEYYtXGpk2b4O3t3eftquzZs4e7L5mTkwOBQACBQIDnn39ebTxUs/wEAgFCQ0MN1OPOVVVV4e2334ajoyMsLCwwbtw4fPrpp7wyBw8ehLu7O0QiESZMmIATJ07o3E5hYSH8/f1hbW2NoUOHws/PD7/++iuvTEZGBjdOHX+ampq4MmVlZQgMDIRMJoOdnR0WL16Muro6AICJiQnCwsLw3HPPobm5uQejoR8UmAgxsJiYGBw+fNjQ3ehTVVVViImJQVJSEkxMTBAQEADGGKRSKXJzc7F+/Xpe+aamJkRHRyMvLw8ZGRkG6nXnli9fjjNnzuDkyZOora1FVFQUFi9ejO+++w7A/YAyd+5crFixAjdv3kRwcDD8/f1x8eJFndqZPn06PDw8cOPGDZSUlAAAZs6cifr6el659PR07suO6sfKygoAUFlZCV9fX3h5eeHSpUsoLi5GYWEhkpOTud/38fGBu7s7EhISejMsvUKBiRADWrduHQQCAZfGqOPrEydOwMXFBVKpFJGRkdxZREREBAQCAeRyOfbt2wdHR0dYWloiJCQETU1NWLhwIQQCAXc21NTUxH1zzsnJ4epYs2YNzp49y+27cuVKnx33l19+CQ8PDwwfPpy33c7ODoGBgdiwYUO3wfrevXuIiYmBvb09LCws8Mwzz+D8+fMAtBvHjnXY2dmp1aGLhIQEjBw5EmKxGJGRkTA1NUVZWRkAIDExEX5+fggLC4ONjQ1WrlwJNzc3bNu2Tac27O3tkZCQAKlUCgcHByQlJeHWrVs4deqUTv186qmnsGLFCtjZ2UEulyMmJkatXGBgINLS0gxyJg9QYCLEoNatW8f7AFa9bmtrw9GjR3Hu3Dl89dVX2LlzJ/Lz8wEAqamp2LVrF2pqalBcXIySkhIUFRWhoKAAsbGxSEtLw8aNG7k6rayswBjjLX5OTU3Fxo0bMXnyZO5btYuLS58dd35+Ptzd3dW2CwQCpKWlwdPTE6Ghobh8+XKXdSxfvhzZ2dk4duwYKioqMH78eMyYMQMNDQ1ajSMAxMXF4YcffkBRURGqqqrw5JNPws/PT6fpzxkZGZgyZQoYY6iursb69evh5OTEres5efIkfHx8eL/j6+uLkydPat0GAPz555+8RcuqR6s8mJvx0KFDGD16NCQSCby9vXHs2DFuX25uLsaNG4eXX34ZlpaWcHZ2Rnl5OVavXs2rY+zYsbh+/brOZ3X6QoGJECPU2tqKt956C1KpFNOmTYObm5va/YR79+5hy5YtsLGxwYQJExATE4O9e/fizp07vWr7888/x9ChQ3H16tVe1aNJRUUFhg4d2uk+sViMnJwcCIVCBAQEoLGxUa2MQqHAJ598gpUrV2LixImwsbHB1q1b0dzcjNTUVK6cpnFUKBTYsWMHNmzYABcXFwwePBibN29GbW0tsrKydD6moKAg2NvbIycnB1lZWRgyZAgaGxvR2NgIW1tbXll7e3tUVFTo3EZHqjPB6dOn87YrlUrk5ubi+vXrCAoKwuzZs1FQUIDW1lZcu3YNycnJCAwMRHV1NXbu3Ilt27Zh69atvDpU783Nmzd71ceeosBEiBEyNTXlXeaSyWRqAcfe3p67dwAA48ePR0tLS68vyXW8N/GwKBQKmJmZdblfLpcjKysLv//+OxYuXKjWl4sXL6K1tRUeHh7cNgsLC7i4uPAuxWkax4sXL6KlpQX+/v7c5UyhUAilUokLFy7ofExZWVmor6/Hm2++CV9fX+Tl5XH9fjA9E2OsVymbbt++jQ0bNmD//v0wNf2/PAmhoaE4dOgQXF1dIZVKERsbCy8vL6SkpHBngb6+vnj11VdhZWWFmTNnYv78+WqTNVTp0x68f9VXKDARYoQeXLDb2YdYe3s77/WDH94Pvm5tbdWq7ZCQENTV1cHZ2Vmr8j0hk8nQ0tKiscy0adOwfft2HDx4EImJibx9moJmx7HSNI6qOs6cOaM2WaCnqbVkMhkWLVqEGTNmYNOmTZBIJLC2tkZ1dTWvXHV1NYYNG9ajNhobGzFnzhzEx8fj6aef7rb8qFGjcPXqVYjFYohEIjg6OvL2y+VylJeXq917AwAbG5se9bG3KDAR0k9VV1fj9u3b3OvS0lKYmZnBxcUF1tbWaGho4PZVVlaqTf81ZJJVBwcHrdbKREVFITw8HGvWrOHdk3F3d4epqSn+/e9/c9uUSiXKysp4Z1GauLu7w8zMrEeTHTpydnZW+5IgFArxn//8BwAwZcoUFBUV8fYXFhZi6tSpOrdVUVEBf39/rFy5EoGBgWr7J06ciJqaGt62srIyyOVyCAQCeHl5cZMyVK5du8btV1G9Nz0Nnr1FgYmQfkosFmPVqlWor69HSUkJkpKSEBYWBolEAk9PT1y+fBmnTp3C7du3kZSUpJbd3tbWFlVVVWhqakJ4eDhSUlIA9M09phkzZmh9Y33Hjh3w8fHh3WOTyWSIjIxEYmIiSkpKoFAoEBcXB6FQiIiICK3qlclkiI6ORnx8PM6ePYvm5macO3cOY8aMwenTpwEAx44dg1Ao1Hh2V19fjyVLlnBjmZGRgZycHAQEBAC4P8HiyJEj2LdvHxQKBbZs2YLS0lLebDht2jl//jyCg4Px8ccfY+bMmQDuTyJ55513uDKMMURHR6OiogINDQ14//33UVxcjOjoaAD3J4x8//33SE9PR1NTE06cOIH09HQsXbqU11ZpaSlGjBgBNzc3rcZS75geLVu2jO3YsUOfVQ4Ij8K4/PTTT8zb29vQ3TAYpVLJJBKJzr/37rvvMgDcT3R0NO+1QqFgrq6u3OsFCxYwxhjbtWsXc3JyYpmZmczJyYlZWFiwuXPnssbGRq7uN954g0mlUjZy5EiWl5fHxGIxA8DeffddxhhjNTU1bPLkyUwkEjEfHx9WV1fHGGPswIEDzMbGhpWVlfVoLGxtbVlDQ4PGMpWVlUwqlbLy8nLGGGPZ2dm8487Ly+OVv3XrFnN0dORtVyqVbNmyZczW1paZm5uzadOmsV9++YUxxlhycrJW49jc3MxiY2OZnZ0dE4lEzNPTk33xxRdcG0uWLGGhoaEaj+X06dNszpw57PHHH2disZiNHTuWvffee6ytrY0rk5mZyVxdXZlQKGSenp7s+PHjvDq6a+fu3bvM2tqad0yqn7i4OK7cb7/9xubNm8ccHByYRCJh3t7e7Ouvv+bVtX//fjZmzBhmbm7O3NzcWEpKCmtvb+eVmT9/Plu7dq3G41ZJTEzk/qb0pIECUx94FMZFl8B05MgRNnnyZCaRSLh/rlmzZj3kHj5cPQ1MPaUKTMZIm8DEGGOfffYZCw4O5n2AG5Mff/yRBQQE8IJ9f25HW4WFhWzSpElMqVRqVf5hBKY+v5RXUFDAS5Xh6+urVqa1tRXr16/HmDFjYGFhgWHDhiEsLAxnzpzhyri4uHSaeqPjj2qRnUAgwPDhw7tcm/DRRx9x5bp60Js2aUd6o7+Oi66qqqrw0ksvISwsDDdv3gRjDHFxcXqpW6WrVDvdvYfapHwh+vPaa6/hxRdfRGZmpqG70qlJkyYhOzubN/OxP7ejjfb2duzZswdHjx6Fubm5wfrR54HJ19cXjDEMGTIE7733HgoKCtTKrF+/Hp9++in27t0LhUKBn3/+GQ4ODnj66ae5nE+mpqbcDBrVddmOqTjmzZsHf39/MMa4NQMd1zeo3L17l5vDn5eXh7S0tE773V3akUd1XHR1/vx5tLW14fXXX+/zp9129x5qm/LF0CIiIhAeHs4tuOw4AaK/CQkJwSuvvGLobpD/MjExwe7du7tcY9Zn/TBo613Iz89HUFAQ/va3v0EkEsHe3h6bNm3CmDFjelTf0KFDMXv2bGzevJmbKaOSkpKCf/zjH1rVoyntSF8w1nHR1rp16zBr1iy0trbCxMSEt/6iI02pZoD7608CAgIgk8ng6OiIFStWcEG4u1Q7mt5DfaR86Qupqam8qc1SqdTQXSJEr4wyMI0YMQL5+fmorKzkbb906RJ3utvdjJ6MjAxMmjSJe52YmIiamhpu5hFwP4fY/v37sWzZsm771F3akb5gjOOiC1WaGHNzczDGulxXoynVDAAsXboU4eHhqKqqwrfffotvv/0WSUlJADSn2unuPdQ25Qsh5OEyysAUHx+P9vZ2yOVy+Pv7Y/v27b2euurp6Yl58+Zh69at3MrvDz/8EFFRUTpdUuos7UhfMeZx0RdtUs1888038PPzg7m5OUaPHo158+bx8oF1R9v3sKuUL4SQh6vzaykG5uLigl9//RX5+fk4cuQIdu7ciWXLluHVV1/F3r171dZjaGvjxo348ssv8eGHH2Lp0qU4dOgQCgsLdcpZlZWVBYVCgX/961/w9fVFdnY2t6bgYTPmcdEXbVLN5OXlYdWqVfjtt9/w119/Abifjkdb2ryHXaV80aS9vR3ffPON1v0YqFpaWnDixAlYWloauiukD1y+fFktm0RvGWVgAu6nEpk1axZmzZoFAPjhhx8wa9YsTJkyResFdA9ycnJCZGQkkpKSUFNTg+XLl3ear2v37t0IDw/nXt+4cUMt39aiRYuQm5uLTZs29VlgAgw7Ln2BdZNqprq6GgEBAXjzzTeRn58PiUSClJSUTidwaKLpPfz/7d19VBNX+gfwbyIkQABDEFi1KFIUsIovpeqyqGeP2FahW1qRWkqptFJfUGvByq5vVVHR1rLrii2sVrHSYyssoNBiBXetCiL1bKvb4itbV0HeCQgqQcj9/cEvWcaYZIBAAjyfc/iDmZs7dybKk8zc+U5nI19U2tra8Omnn3ZqHP2RQqHA/v37eRd00rfduHGj/xcmDw8PuLm5Yfv27fDy8lIvnzlzJkaMGIGKiopu9b9u3TocOHAA58+fx1//+tcntlm0aBEWLVrEWebq6oqbN29CKPzf2c+OsSM9zRSOS2/oGDWjmvKtipoJDg5GcXExHjx4gIiICNja2gL4X66XiraoHT7vYVlZGUJCQrB+/fpOf+AwNzfHsWPHOvWa/sjR0RFHjhxRvz+kf9uxY0enHhPCh0leYwKAlStX4vLly1AoFKiurkZ8fDz+85//4Pnnn+9Wv0OGDME//vEPHD16tFNZYfpiR3qLqR0XQ9MXNePu7g6RSISEhAQ0Njbi5s2bOHToEKcPbVE7+t5DPpEvhJBeYMjbdfkkHJw9e/aJsRqqH7FYzEpLS9n69evZhAkTmEQiYTKZjP3ud79jmZmZGv2lpqZyXj9v3jyt2zp//rzG6x+PLnnvvfeeOG4+sSMD8bio8El+eDx+58qVK2zWrFnq3+3t7RljuqNmGGuPzHFzc2M2NjZs1qxZbNWqVQwAk0gkjDHtUTu63kO+kS/a9Hbygynjm/xA+oeeSH4QMGa4h65ERUXBzc0Ny5YtM1SX/cJAOC4//vgjli1bpg6/HGgUCgUcHR05id4DlaOjI27evMnrVN6RI0cgFArpJlsToVQqsXjxYsTFxfG+yVZ1Km/Tpk2GGsY9kz2VRwh5Mm2RS6be9+MOHDiAjIwMzJ8/H5mZmeobol966SWNSTDW1tbq9aGhob0yPr74xJWlp6fD09MTFhYWmDhxIk6dOtXp7fCJzEpJSXliDJkqGQZofwxGUFAQ7Ozs4OjoiMWLF6O2thZAe/JDeHg45syZo/GYlN5EhYkQ0usqKioQFRWF+Ph4CIVCBAYGqlMssrOzsXnzZk77pqYmREZGIjc3FykpKUYa9ZPpi7rKz8/HggULsGbNGty9exfBwcEICAjg/dgPFb6RWR0jyFQ/qhvwy8vL4evriylTpuDatWsoKipCfn4+9uzZo369j48PPD09sX379u4clm6hwkRIL9IVt7Rw4UIIBAL1N5ampib1J97MzEwA2iOXlixZAoFAABcXFxw6dAjOzs6wsrJCSEiI+tOyvv71xTkZ0tGjRzF+/HjObRhA+2nAoKAgbNmyBVlZWTr70HUsVUHFFhYW6hulpVIpli5dqvGk1qioKDg6Oj4x/oovXVFXcXFx8Pf3R3h4OGQyGdauXQsPDw/s2rWrU9swRGTW9u3b8dxzz2HNmjVwdHSEi4sL57lQKkFBQUhOTtZ5+0ZPosJESC/SFbeUnJyM2NhYdVtra2swxjgJHNoilxITE7Fv3z5UV1ejqKgIly5dQkFBAc6dO4fo6GgA0Nu/rjgnQ8vLy4Onp6fGcoFAgOTkZHh5eSE0NBTXr1/X2oeuY6mKv2pra0NOTg4uXryIY8eOISkpCXl5eeo+YmJi8P3336OgoAAVFRWYPHky/P39OzX9WV/U1ZkzZ+Dj48N5ja+vL+eJvHzwjcw6fvw4Ro8eDVtbW0ybNo2TipKdnY1nnnkG8+bNg5WVFVxdXVFaWor169dz+hg7dixu377d6W91hkKFiZBewiduqbtaWlqwc+dOyGQyTJw4EVFRUTh48KA6bqqrDP1U27KyMq0X1yUSCTIzMyESiRAYGIjGxkaNNnyPZWtrK1atWgWpVIqZM2fCw8NDfV1GLpdj79692LJlC9zc3DB48GDs2LEDNTU1SEtL6/Q+PSnqqrGxEY2NjXBwcOC0VSX7d4e2yKzm5mZkZ2fj9u3bmD9/PubOnYtz586htbUVt27dwp49exAUFISqqiokJSVh165d6icJqKjem7t373ZrjF1FhYmQXsInbqm7nJycOM/1GTduHB49etTtU3Idr1cYglwu15ku4uLigrS0NNy4cQMLFy7U2C7fY2lmZqaR2qIq0levXsWjR48QEBCgPnUpEonQ3NyMK1eudHqf0tLSUFdXhxUrVsDX1xe5ubnqcT9+byBjrFv3C2qLzAoNDcXx48fh7u4OqVSK6OhoTJkyBQkJCepvgb6+vnj99ddhbW2N2bNnIywsTGOyhirezFiPfKHCREgv0fVHveMfqcfbaUthfxKlUql3m13pPyQkBLW1tXB1deU9Fl3s7OzUjyrRZubMmdi9ezfS09MRFxfHWcf3WA4aNEjrOlUfhYWFGpMFtm3bxntfOlJFXfn5+WHr1q2wtbWFjY0NqqqqOO2qqqowbNiwLm2js5FZTz/9NH799VdIJBJYWFhoxAe5uLigtLRU49obAMhksi6NsbuoMBHSSzrGLamo4pZUn/xtbGw490KVl5drTNvV9Um7qqqK8+DA4uJimJubq68V6eu/t1I/hg8fjpqaGr3tli1bhoiICGzYsIFzTYbPsdTH09MT5ubm3f626urqqvGBoGPU1fTp01FQUMBZn5+fjxkzZnR6W2VlZQgICMDatWsRFBSksX7SpEmorq7mLCspKYGLiwsEAgGmTJmi8Qy5W7duqderqN6brhbP7qLCREgv0Re3BLQ/huT69es4e/Ys6uvrER8fr5Eary1yCWi/PrNu3TrU1dXh0qVLiI+PR3h4uPpmV339a+vb0NeY/Pz8eF9Y37t3L3x8fDj37PA5lvrY2dkhMjIS27Ztw4ULF6BQKHDx4kWMGTNGfaP4iRMnIBKJdH670xd1FRMTg2+++QaHDh2CXC7Hzp07UVxczJkNx2c7fCKzGGOIjIxEWVkZGhoa8Mknn6CoqAiRkZEA2ieMnD59GocPH0ZTUxNOnTqFw4cP47333uNsq7i4GCNGjICHhwevY2lwhsyR4BO9MxANhOPCJ5KoP+MbSaQvbokxxpYvX86kUikbNWoUy83NZRKJhAFQx75oi1zat28fGzlyJEtNTWUjR45klpaWbMGCBayxsZF3/9r6/vrrr5lMJmMlJSV695FPJFF5eTmTSqWstLSUMcZYRkYGJwYqNzeX076yspI5Oztzlus6lo9Hasnlcubu7q7+/a233mKMMaZQKFh0dDRzdHRkFhYWzMvLi3311VfqbaxcuZKFhobq3Bc+cWWpqanM3d2diUQi5uXlxU6ePMnpQ992+EZm/fLLL+yNN95gw4cPZ7a2tmzatGns22+/5fT1xRdfsDFjxjCxWMw8PDxYQkICUyqVnDZhYWFs48aNOvdbpSciiagw9YKBcFyoMBk/K09VmIyNb1be559/zoKDg3nlTRrDDz/8wAIDAzUKe1/dDl/5+fnM29ubNTc382rfE4XJ5B57QQgZGN5++21YWFggNTXVJLPyvL29kZGR0W+2w4dSqcSBAweQk5MDsVhstHFQYSKkH1iyZAmSkpIAtE9gkMvlkEqlRh6VfiEhIcYeAulAKBRi//79xh4GTX4gpD9ITEzkTHfuC0WJEG2oMBFCCDEpVJgIIYSYFCpMhBBCTIrBn2C7b98+WFpaGqrLfkGhUEAoFOrMBuvrlEolFArFgH3vW1tb8fDhQ9jY2Bh7KEZ3//59WFlZ9VqKBDGuBw8eYPXq1QZ9gq1BC5NCoTDqUw8JMZZt27bB2tpa4w56QgYCsVhsyOnl9ww6XdzAgyOkzxCLxbCwsFBH/xBCuo6uMRFCCDEpVJgIIYSYFCpMhBBCTAoVJkIIISaFChMhhBCTQoWJEEKISaHCRAghxKRQYSKEEGJSqDARQggxKVSYCCGEmBQqTIQQQkwKFSZCCCEmhQoTIYQQk0KFiRBCiEmhwkQIIcSkUGEihBBiUqgwEUIIMSlUmAghhJgUKkyEEEJMChUmQgghJoUKEyGEEJNChYkQQohJocJECCHEpAgYY8zYgyCkr2lpacH06dMhl8sBAPfv34dAIICVlRUA4De/+Q1Onz4NoZA++xHSSffMjD0CQvoikUgEmUyGoqIijXUCgQDe3t5UlAjpIvqfQ0gXvfvuuxg8eLDGcltbWyxatMgIIyKkf6BTeYR0kUKhgL29Pe7fv89ZLpVKUVNTg0GDBhlpZIT0affoGxMhXSQWixEQEACBQKBeNmjQICxYsICKEiHdQIWJkG545513YGtrq/5dIpEgPDzciCMipO+jU3mEdINSqYS9vT3q6+sBAE5OTigvL+d8iyKEdAqdyiOkO4RCIYKDgzFo0CCYm5tj4cKFVJQI6SYqTIR0U3h4OCQSCcRiMcLCwow9HEL6PDqVR4gBDB06FFZWVigpKTH2UAjp6+4ZvDBVVVVh9OjRhuyyT2ppaYFIJDL2MHpFa2srhELhgL2htK2tDS0tLRg0aNCAec+fRKlUQqlUwsyM7tsfSCoqKmBpaWnILg2f/KBUKmFnZ4dr164Zuus+RSaToby8fEBMGw4LC8O8efPw8ssvG3soRhEXF4fq6mq8//77cHZ2NvZwjObw4cO4cOECEhISjD0U0kuGDh2Knjjp1mMfbcRicU913WeIxeIBUZiEQiHMzc0H7HtuZmaGoUOHws3NzdhDMSozMzMMGjRowP47IIYzMM+9EEIIMVlUmAgxkj//+c/4wx/+YOxhGM2RI0fw9ddfG3sY5P8plUpERESgpqbG2EOhwkSIsTDGeuT8PB9bt27FtGnTjLJtADhw4AAyMjIwf/58ZGZmQiAQQCAQ4KWXXtI4JtbW1ur1oaGhRhqxpoqKCnzwwQdwdnaGpaUlnnnmGfztb3/TaJeeng5PT09YWFhg4sSJOHXqVKe2k5+fj4CAANjY2GDIkCHw9/fHv//9b06blJQU9THq+NPU1MRpV1JSgqCgINjZ2cHR0RGLFy9GbW0tgPZT8uHh4ZgztbVjSAAAFllJREFUZw4UCkUnj4ZhUWEixEiioqKQlZVl7GH0uoqKCkRFRSE+Ph5CoRCBgYFgjEEqlSI7OxubN2/mtG9qakJkZCRyc3ORkpJipFFrWr16NQoLC3HmzBnU1NRg2bJlWLx4Mf75z3+q2+Tn52PBggVYs2YN7t69i+DgYAQEBODq1au8tzNr1iyMHz8ed+7cwaVLlwAAs2fPRl1dHafd4cOH1R92VD/W1tbq9eXl5fD19cWUKVNw7do1FBUVIT8/H3v27FG38fHxgaenJ7Zv397Vw2IQVJgIMYJNmzZBIBDAwsLiictOnToFNzc3SKVSLF26FIwxLFmyBAKBAC4uLjh06BCcnZ1hZWWFkJAQ9SdjVfKE6ttQU1OT+tNzZmYmAGDJkiXYsGEDLly4oF538+bNXtv3o0ePYvz48Xjqqac4yx0dHREUFIQtW7boLdgtLS2IioqCk5MTLC0t8fvf/x6XL18GoP84dny9o6Ojxus7Y/v27Rg1ahQkEgmWLl0KMzMzzr1scXFx8Pf3R3h4OGQyGdauXQsPDw/s2rWL9zacnJywfft2SKVSDB8+HPHx8aisrMTZs2c7PdbnnnsOa9asgaOjI1xcXBAVFaXRLigoCMnJyUb7Ng9QYSLEKDZt2qTxx1e1rK2tDTk5Obh48SKOHTuGpKQk5OXlITExEfv27UN1dTWKiopw6dIlFBQU4Ny5c4iOjgYAJCcnIzY2Vt2ntbU1GGOQSCTqZYmJiYiNjcXUqVPVn6x7c0ZhXl4ePD09NZYLBAIkJyfDy8sLoaGhuH79utY+Vq9ejYyMDJw4cQJlZWUYN24c/Pz80NDQoPc4AkBMTAy+//57FBQUoKKiApMnT4a/vz+am5t570dKSgqmT58OxhiqqqqwefNmjBw5Eq+88oq6zZkzZ+Dj48N5na+vL86cOcN7O//97385MVeqZ4B1/FADAMePH8fo0aNha2uLadOm4cSJE5z12dnZeOaZZzBv3jxYWVnB1dUVpaWlWL9+Pafd2LFjcfv27U59qzM0KkyEmJjW1lasWrUKUqkUM2fOhIeHB+eaQktLC3bu3AmZTIaJEyciKioKBw8exL1797q97SNHjmDIkCH49ddfu92XNmVlZRgyZMgT10kkEmRmZkIkEiEwMBCNjY0abeRyOT777DOsXbsWkyZNgkwmw0cffQSFQoHExER1O23HUS6XY+/evdiyZQvc3NwwePBg7NixAzU1NUhLS+v0/syfPx9OTk7IzMxEWloa7O3tAQCNjY1obGyEg4MDp72TkxPKyso6vR0V1bfAWbNmcZY3NzcjOzsbt2/fxvz58zF37lycO3cOQPuxuHXrFvbs2YOgoCBUVVUhKSkJu3btwkcffcTpR/Xe3L17t8tj7C4qTISYGDMzM85pLjs7O07RcXJy4lw7GDduHB49emSQ03Edr0/0FLlcDnNzc63rXVxckJaWhhs3bmDhwoUaY7l69SpaW1sxfvx49TJLS0u4ublxTsdpO45Xr17Fo0eP1M/SEggEEIlEaG5uxpUrVzq9P2lpaairq8OKFSvg6+uL3NxcAFCP+/FQX8ZYl4N+6+vrsWXLFnzxxRechI3Q0FAcP34c7u7ukEqliI6OxpQpU9Q3O6u+Cfr6+uL111+HtbU1Zs+ejbCwMI0JG6r0ksevYfUmKkyEmJjHb8p+/I+YUqnk/P6kIvL4stbWVl7bDgkJQW1tLVxdXXm17wo7Ozs8evRIZ5uZM2di9+7dSE9PR1xcHGedrqL5+EMbn7RO9frCwkKNyQLbtm3r1L6o2NnZYdGiRfDz88PWrVsBALa2trCxsUFVVRWnbVVVFYYNG9bpbTQ2NuKVV17Btm3b8Nvf/lZv+6efflr9zVcikcDCwkIjmcTFxQWlpaWcY9rS0gKgPb3GWKgwEdLHVFVVqZ//BADFxcUwNzdXXyeysbFBQ0ODen15ebnG9F9jPppj+PDhvO6VWbZsGSIiIrBhwwbONRlPT0+YmZnh559/Vi9rbm5GSUkJ51uUNp6enjA3N+/SZIeOXF1dNT4kiEQi3L9/X/379OnTUVBQwGmTn5+PGTNmdGpbZWVlCAgIwNq1axEUFKSxftKkSaiuruYsKykpgYuLC4D293vKlCkaIcO3bt2Ci4sL59+D6r3pSvE0FCpMhPQxEokE69atQ11dHS5duoT4+HiEh4ern6Tr5eWF69ev4+zZs6ivr0d8fLxGuKyDgwMqKirQ1NSEiIgI9Smf3rjG5Ofnx/vC+t69e+Hj48O5xmZnZ4elS5ciLi4Oly5dglwuR0xMDEQiEZYsWaK3Tzs7O0RGRmLbtm24cOECFAoFLl68iDFjxuD8+fMAgBMnTkAkEun8ZldXV4eVK1eqj2NKSgoyMzMRGBiobhMTE4NvvvkGhw4dglwux86dO1FcXMyZDadvW5cvX0ZwcDA+/fRTzJ49G0D7BJI//vGP6jaMMURGRqKsrAwNDQ345JNPUFRUhMjISHWb1atX4/Tp0zh8+DCamppw6tQpHD58GO+99x5ne8XFxRgxYgQ8PDz0HssewwysvLycjRw50tDd9jlWVlastbXV2MPoFQsWLGDp6enGHobRxMbGstjY2E695sMPP2QA1D9Xrlxhe/bs4SyTy+XM3d1d/ftbb73F9u3bx0aOHMlSU1PZyJEjmaWlJVuwYAFrbGzk9L98+XImlUrZqFGjWG5uLpNIJAwA+/DDDxljjFVXV7OpU6cyCwsL5uPjw2praxljjH399ddMJpOxkpKSTh+HgwcPssWLF+ttV15ezqRSKSstLWWMMZaRkcHZ79zcXE77yspK5uzszFne3NzM3n//febg4MDEYjGbOXMm++mnnxhjjNdxVCgULDo6mjk6OjILCwvm5eXFvvrqK3X/K1euZKGhoTr34/z58+yVV15hQ4cOZRKJhI0dO5Z9/PHHrK2tjdMuNTWVubu7M5FIxLy8vNjJkyc563Vt6+HDh8zGxoazP6qfmJgYdbtffvmFvfHGG2z48OHM1taWTZs2jX377bca/X3xxRdszJgxTCwWMw8PD5aQkMCUSiWnTVhYGNu4caPOfVexs7Nj9+/f59W2ExqoMPUQKkwDR1cKU1epCpMp4luYGGPs888/Z8HBwRp/xE3BDz/8wAIDAzWKfV/fFh/5+fnM29ubNTc382rfU4WJTuWRXvXtt99i2rRpGDx4sHpG1IsvvmjsYZFe9vbbb+Pll19GamqqsYeiwdvbGxkZGZyZj/1hW/oolUocOHAAOTk5Rk+IN2phampqwsaNG+Hh4QGxWAyZTIY5c+ZwIj346snsr57o+9y5cxq5VjY2NpgwYQLWrVtnEkGKhlZRUYFXX30V4eHhuHv3LhhjiImJMVj/2t4nPplmfPLIjG3JkiWIiIhQ33DZcQJEXxQSEoLXXnvN2MMg/08oFGL//v1a7zHr1bEYa8P19fXw8fFBfn4+vvzyS9y7dw/Xrl3D3LlzMXfuXCQlJRlraL3C19cXjDHY29vj448/BmMMd+7cwY4dO3Dy5El4eXl16Z4KU3b58mW0tbXh3Xff5SQR9DQ+mWZ888iMKTExkTO1WSqVGntIhPQIoxWm6OhoVFZW4vjx43j22WchFovh4OCAFStWYN26dVi+fDlKSkq6lf2lL1vM1HLFpFIp5syZgzNnzsDe3h6vvfaaejqqrmwvPtlgQHskyfjx42FpaYlJkyYhPT1dvc5Q2WHabNq0CS+88IL6MexPevy2rvwzoP3GysDAQNjZ2cHZ2Rlr1qxRz2TS9z7pyzQzVB4ZIcQADH3Vis/kh/v37zOxWMyioqKeuL6mpoYzgyg2NpZNnTqV00YikbCMjAz1709qw1j7xWIrKyu2bNkyVltby3788Ufm7OzM3n333W73rQvfyQ/29vbs448/1lh+8OBBBoAVFhYyxhhbtWoVmzx5Mrtx4warr69nUVFR7KmnnmIPHz5kjDGWlZXFzMzMWHR0NJPL5ez06dNMIBCoZwA1NDQwS0tLdvLkSaZQKNjVq1eZm5sbq66u5tW/LnwnP2RlZTGxWMxZFhMTw1544QXGGGMrVqxgLi4u7F//+herra1ly5cvZw4ODqy+vp4xxtjzzz/PsrOzWXNzM7t+/Tp79tln2Y4dO9R96XuflEolq6ysZBs3bmRPP/00q6mp0dq2vLycAWAnTpzQu1+9OfnBlHVm8gPpH/rV5IeSkhIoFAqtd5fb29tj8ODBBjuV1ZPZYj1lwoQJANrvKeCb7aUrY628vBwPHz6EUqmESCSCu7s7bty4gSFDhhg8O6wr+OSffffdd/D394dYLMbo0aPxxhtvaARV6qIt0+xJtOWREUJ6nub5lF7AtGRIPc5Qd6f3ZLZYT+l4jDpmez2uY/HWlbE2ZswY+Pv748UXX4S7uzveeecdLFq0CHZ2drz770l88s9yc3Oxbt06/PLLL3jw4AGA9veSr7S0NMjlcvz973+Hr68vMjIy1DcsdqQtj0yXrKwslJeX8x5Lf3T16lXU1tZybuok/Zu+aKmuMkphcnNzg1gs1ojHUKmrq0NDQwPGjh2rXsa6mP0F6M8W607fPeWnn34C0B5BrxpPYWEhpk6dqvU1ujLWBAIBsrKykJOTg4SEBKxbt059dzjrkB2mq/+e9Ph70JFAIEBVVRUCAwOxYsUK5OXlwdbWFgkJCZw0aT5UmWbZ2dnYunWrRmHqbB6ZyrBhwzB58uROjaW/efjwIRhjA/44DCSHDx/ukX6NUphUExC+/PJLxMbGwsrKirM+MTERIpEIb775JoDuZ3+pssVUs5g6ZouZYq7Yw4cPER8fDy8vL3h7e6OhoUGd7dWdwiEQCNSzHu/cuYOJEyciKysLISEhBum/Ozrmn6kmo6jyz4KDg1FcXIwHDx4gIiJCHb2jCptU0fY+ubq64ubNmxAK/3fm+vFMM6A9jywkJATr169/4jcpXZ599lm88847nXpNfzNo0CCIRKIBfxwGkg8++KBH+jXarLz4+Hg4ODjAz88PBQUFaGxsxK1bt7Bz507Exsbis88+w6hRowB0L/sL0J0t1t2+DamhoQE5OTmYMWMGamtr8dVXX0EoFPLK9tKnsLAQvr6+uHPnDh49eoTKykq0tLTAw8PDIP13l778M3d3d4hEIiQkJKCxsRE3b97EoUOHOH1oe5/4ZJrxySMjhPQSQ0+n6Ewk0b1799jatWvZ6NGjmbm5OZNKpezFF19kZ86c0Wjb1ewvPtliXe1bF32z8s6ePauRfWVlZcXGjx/P/vSnP6lny6noyvbikw3W2trKEhMT2YQJE5ilpSVzdXVlf/nLX3j1rw+fWXlPyoabNWuW+nd7e3ud+WeMtee4ubm5MRsbGzZr1iy2atUqBoBJJBLGmPb3SV+mGd88Mm1oVl47mpU38PTUrDwBY4Z9IlhFRQWmTZuGW7duGbLbLtu/fz+2bt3a6+ORSCS4d++exnWf/uj1119HcHAw55HSA4nq+TuPP6J6oElOTkZhYWGnr/vpcuTIEQiFQkqI6CalUonFixcjLi7OoMkOMpkMpaWlGpdjuukeZeUR0of0teit7jhw4AAyMjIwf/58ZGZmqm+cfumllzQmy1hbW6vXh4aGGmnEmvjEYQFAeno6PD09YWFhgYkTJ+LUqVNd2t7u3bthbm6uMeNYKBQiPDwcc+bM0biGbor6dWHqb9lihAwUFRUViIqKQnx8PIRCIQIDA9UxTNnZ2di8eTOnfVNTEyIjI5Gbm4uUlBQjjVoTnzis/Px8LFiwAGvWrMHdu3cRHByMgIAA3s+sAtonCoWFheHo0aNaZxX7+PjA09MT27dv7/Z+9bR+XZgoW4yYEl2RS12Nx/Lz89MZu9Wdvo15n9/Ro0cxfvx4zn15AODo6IigoCBs2bIFWVlZWl+v61jzjfAyVEyXvjisuLg4+Pv7Izw8HDKZDGvXroWHhwd27drFexs///wz3nzzTezcuVNnu6CgICQnJ+u8PcMU9OvCRIgpWb16NTIyMnDixAmUlZVh3Lhx8PPzQ0NDA5KTkxEbG6tua21tDcYYJ+w2MTERsbGxmDp1qvrDVl5eHvbt24fq6moUFRXh0qVLKCgowLlz5xAdHQ0AXe5b9ah2Y8jLy4Onp6fGcoFAgOTkZHh5eSE0NBTXr19/4ut1HetNmzYhKysLbW1tyMnJwcWLF3Hs2DEkJSUhLy9P3UdMTAy+//57FBQUoKKiApMnT4a/vz+am5t570dKSgqmT58OxhiqqqqwefNmjBw5knM99syZM/Dx8eG8ztfXl/M4eX28vb153eIwduxY3L59u1PfxoyBChMhvYBP5FJ39GTsVm88bv1xZWVlWi/SSyQSZGZmQiQSITAwEI2NjZz1fI+1rggvQ8d0aYvDamxsRGNjIxwcHDjtnZycUFZW1unt6KM6pnfv3jV434ZEhYmQXsAncqk7ejJ2q+Pp8N4il8thbm6udb2LiwvS0tJw48YNLFy4kDM2vsdaV4RXx5gu1alNkUiE5ubmLsV0paWloa6uDitWrICvry9yc3MBaI9nY4z1yI39qns0TelxLk9ChYmQXqDrj3rHP0CPt+Mbj6Uvdqs7fYeEhKC2tlZr6HJPsLOz05vDNnPmTOzevRvp6emIi4tTL+d7rHVFeLEOMV0dCzNjDNu2bevUvqio4rD8/PzUtxjY2trCxsYGVVVVnLZVVVUYNmxYl7ajiyotRSaTGbxvQ6LCREgv6Bi5pKKKXFJ9su9OPJYqdkulY+xWd/s2huHDh/N6ivOyZcsQERGBDRs2qK/J8DnW+nh6eqpjurrD1dVV40PD43FY06dPR0FBAadNfn4+ZsyY0a1tP4nqmPZE0TMkKkyE9AJ9kUtA96K3dMVudbdvY1xj8vPz432Bfu/evfDx8VFfH+JzrPXhE9N14sQJiEQind/s+MRhxcTE4JtvvsGhQ4cgl8uxc+dOFBcXIyoqivd2+CouLsaIESPg4eHR7b56lKGzJDoTSdSf8X1QYH/A90GB/RXfSCJ9kUuMdS0ei0/sVlf7Zqw9Ckomk7GSkhKd+2fISKLy8nImlUpZaWkpY4yxjIwMTlRUbm4up31lZSVzdnZWL9d1rPlEeDGmP6Zr5cqVLDQ0VOd+6IvDUklNTWXu7u5MJBIxLy8v9QM++W7n0aNHGnFa9vb2Gu3CwsLYxo0bdfbVGT0VSUSFqYdQYRo4jJ2VpypMxmborLzPP/+cBQcHa/wRNwU//PADCwwM1Cj+pryd/Px85u3tzZqbmw0wsnY9VZiM8tgLQgjR5+2334aFhQVSU1NNLivP29sbGRkZfWY7SqUSBw4cQE5ODsRisQFG1rOoMBHShy1ZsgRJSUkA2icvyOXyfpVwEhISYuwh9AtCoRD79+839jB4o8kPhPRhFLtF+iMqTIQQQkwKFSZCCCEmpUeuMSkUChQWFvZE132GUqnEhQsXIBT2/9pfW1uL69evD9j3/M6dOwAwYPdfpaSkBJWVlQP+OAwkbW1tPdKvwZ9gW1dXh1dffdWQXfZJ9fX1A+Z8//379yESiXRmm/VnqrRpCwsLI4/EuFpaWtDa2mrop5kSE/fdd98ZeqbfPYMXJkIIIaQb6NHqhBBCTAsVJkIIISbFDECqsQdBCCGE/L+H/wc91Ag8F5wXcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, to_file='model_3_256_detailed.png', show_shapes=True, show_dtype=True, \n",
    "                       show_layer_activations=False, rankdir='TB', dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f692656d-ff4e-43ce-b6b5-93301c71c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "1258/1258 [==============================] - 166s 130ms/step - loss: 286.3877 - mse: 487170.2500 - mae: 286.3899 - val_loss: 74.3627 - val_mse: 57117.0859 - val_mae: 74.3627\n",
      "Epoch 2/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 70.3007 - mse: 55672.8555 - mae: 70.3012 - val_loss: 76.6159 - val_mse: 66050.9375 - val_mae: 76.6159\n",
      "Epoch 3/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 55.9375 - mse: 49312.1602 - mae: 55.9379 - val_loss: 44.8772 - val_mse: 42978.7578 - val_mae: 44.8772\n",
      "Epoch 4/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 50.7956 - mse: 47694.0664 - mae: 50.7960 - val_loss: 51.5948 - val_mse: 45373.2539 - val_mae: 51.5948\n",
      "Epoch 5/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 49.3243 - mse: 47096.4023 - mae: 49.3247 - val_loss: 38.8320 - val_mse: 40160.5117 - val_mae: 38.8319\n",
      "Epoch 6/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 47.8274 - mse: 45827.5508 - mae: 47.8277 - val_loss: 34.8236 - val_mse: 35936.5938 - val_mae: 34.8236\n",
      "Epoch 7/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 45.8665 - mse: 43083.7617 - mae: 45.8668 - val_loss: 34.7309 - val_mse: 35949.5664 - val_mae: 34.7310\n",
      "Epoch 8/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 44.9054 - mse: 44235.2070 - mae: 44.9057 - val_loss: 65.8994 - val_mse: 54000.8281 - val_mae: 65.8994\n",
      "Epoch 9/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 42.5084 - mse: 40557.2305 - mae: 42.5086 - val_loss: 29.7882 - val_mse: 29846.1973 - val_mae: 29.7883\n",
      "Epoch 10/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 70.7472 - mse: 64323.6914 - mae: 70.7478 - val_loss: 63.9832 - val_mse: 58571.6484 - val_mae: 63.9832\n",
      "Epoch 11/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 65.4846 - mse: 60474.4375 - mae: 65.4851 - val_loss: 55.5972 - val_mse: 48399.9102 - val_mae: 55.5972\n",
      "Epoch 12/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 55.8394 - mse: 51823.3633 - mae: 55.8398 - val_loss: 57.5342 - val_mse: 53135.8711 - val_mae: 57.5343\n",
      "Epoch 13/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 65.5142 - mse: 63821.4805 - mae: 65.5146 - val_loss: 187.3329 - val_mse: 211270.3281 - val_mae: 187.3328\n",
      "Epoch 14/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 111.4268 - mse: 103653.1719 - mae: 111.4276 - val_loss: 53.0927 - val_mse: 38854.3867 - val_mae: 53.0927\n",
      "Epoch 15/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 51.2188 - mse: 45258.3438 - mae: 51.2192 - val_loss: 64.4212 - val_mse: 57849.8633 - val_mae: 64.4212\n",
      "Epoch 16/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 55.4977 - mse: 50669.6250 - mae: 55.4981 - val_loss: 44.4396 - val_mse: 35140.4062 - val_mae: 44.4396\n",
      "Epoch 17/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 42.5834 - mse: 40899.3281 - mae: 42.5837 - val_loss: 31.6093 - val_mse: 31910.2949 - val_mae: 31.6092\n",
      "Epoch 18/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 93.8456 - mse: 84471.3672 - mae: 93.8463 - val_loss: 70.6680 - val_mse: 64763.6094 - val_mae: 70.6680\n",
      "Epoch 19/1024\n",
      "1258/1258 [==============================] - 169s 133ms/step - loss: 61.5434 - mse: 57325.9609 - mae: 61.5439 - val_loss: 50.3082 - val_mse: 44349.4922 - val_mae: 50.3082\n",
      "Epoch 20/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 45.6588 - mse: 44977.5430 - mae: 45.6591 - val_loss: 35.7633 - val_mse: 34333.8477 - val_mae: 35.7633\n",
      "Epoch 21/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 37.0286 - mse: 38727.1719 - mae: 37.0289 - val_loss: 28.1117 - val_mse: 31328.1953 - val_mae: 28.1118\n",
      "Epoch 22/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 32.9782 - mse: 37476.7344 - mae: 32.9784 - val_loss: 20.0392 - val_mse: 29347.6895 - val_mae: 20.0392\n",
      "Epoch 23/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 31.3463 - mse: 38154.6523 - mae: 31.3466 - val_loss: 23.5855 - val_mse: 32126.7773 - val_mae: 23.5855\n",
      "Epoch 24/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 27.4750 - mse: 38347.4648 - mae: 27.4752 - val_loss: 35.8940 - val_mse: 39775.2539 - val_mae: 35.8940\n",
      "Epoch 25/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 29.3473 - mse: 38050.7891 - mae: 29.3475 - val_loss: 24.1530 - val_mse: 28986.7891 - val_mae: 24.1530\n",
      "Epoch 26/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 30.2614 - mse: 36627.7891 - mae: 30.2616 - val_loss: 46.8021 - val_mse: 46300.1289 - val_mae: 46.8021\n",
      "Epoch 27/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 30.9480 - mse: 37008.4453 - mae: 30.9482 - val_loss: 52.5113 - val_mse: 52655.2734 - val_mae: 52.5113\n",
      "Epoch 28/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 31.4558 - mse: 35894.3398 - mae: 31.4561 - val_loss: 22.5095 - val_mse: 28889.6582 - val_mae: 22.5095\n",
      "Epoch 29/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 30.5235 - mse: 36181.1367 - mae: 30.5238 - val_loss: 36.8548 - val_mse: 33839.4492 - val_mae: 36.8548\n",
      "Epoch 30/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 31.6589 - mse: 36433.2734 - mae: 31.6592 - val_loss: 45.7987 - val_mse: 46591.5391 - val_mae: 45.7987\n",
      "Epoch 31/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 26.3999 - mse: 33652.9414 - mae: 26.4001 - val_loss: 17.6986 - val_mse: 22137.6855 - val_mae: 17.6986\n",
      "Epoch 32/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 41.6155 - mse: 47944.5664 - mae: 41.6157 - val_loss: 157.6575 - val_mse: 182587.1250 - val_mae: 157.6575\n",
      "Epoch 33/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 97.5787 - mse: 99357.6953 - mae: 97.5795 - val_loss: 55.6536 - val_mse: 50534.8125 - val_mae: 55.6537\n",
      "Epoch 34/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 52.2289 - mse: 48385.7461 - mae: 52.2292 - val_loss: 35.8183 - val_mse: 33335.7891 - val_mae: 35.8182\n",
      "Epoch 35/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 41.9960 - mse: 43081.3281 - mae: 41.9963 - val_loss: 30.7929 - val_mse: 33344.5000 - val_mae: 30.7929\n",
      "Epoch 36/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 38.7424 - mse: 43064.3867 - mae: 38.7427 - val_loss: 28.7423 - val_mse: 30748.4551 - val_mae: 28.7422\n",
      "Epoch 37/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 39.6465 - mse: 45406.2266 - mae: 39.6468 - val_loss: 29.7864 - val_mse: 34955.2773 - val_mae: 29.7864\n",
      "Epoch 38/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 31.5707 - mse: 38223.6016 - mae: 31.5709 - val_loss: 29.2142 - val_mse: 36156.4023 - val_mae: 29.2142\n",
      "Epoch 39/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 36.9991 - mse: 45021.5703 - mae: 36.9994 - val_loss: 31.2429 - val_mse: 38929.3125 - val_mae: 31.2429\n",
      "Epoch 40/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 50.3992 - mse: 51693.8594 - mae: 50.3995 - val_loss: 27.5143 - val_mse: 30066.4980 - val_mae: 27.5143\n",
      "Epoch 41/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 34.0220 - mse: 39384.0078 - mae: 34.0223 - val_loss: 26.3622 - val_mse: 30154.8086 - val_mae: 26.3622\n",
      "Epoch 42/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 31.9557 - mse: 38613.7500 - mae: 31.9559 - val_loss: 38.1539 - val_mse: 38927.3398 - val_mae: 38.1539\n",
      "Epoch 43/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 33.1084 - mse: 41531.9375 - mae: 33.1086 - val_loss: 22.3907 - val_mse: 33512.5156 - val_mae: 22.3907\n",
      "Epoch 44/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 31.5214 - mse: 40089.7617 - mae: 31.5216 - val_loss: 52.5918 - val_mse: 56804.1211 - val_mae: 52.5918\n",
      "Epoch 45/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 27.8141 - mse: 36580.9453 - mae: 27.8143 - val_loss: 33.5642 - val_mse: 30981.6445 - val_mae: 33.5642\n",
      "Epoch 46/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 81.6024 - mse: 89530.9609 - mae: 81.6029 - val_loss: 45.9652 - val_mse: 44001.8281 - val_mae: 45.9652\n",
      "Epoch 47/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 33.4581 - mse: 40275.2969 - mae: 33.4583 - val_loss: 26.0579 - val_mse: 33216.8633 - val_mae: 26.0579\n",
      "Epoch 48/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 27.2071 - mse: 40019.6484 - mae: 27.2072 - val_loss: 27.4900 - val_mse: 38312.8164 - val_mae: 27.4900\n",
      "Epoch 49/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 23.7990 - mse: 36091.7539 - mae: 23.7992 - val_loss: 15.3062 - val_mse: 24706.7285 - val_mae: 15.3062\n",
      "Epoch 50/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 29.3082 - mse: 37736.3320 - mae: 29.3084 - val_loss: 16.3450 - val_mse: 31703.4824 - val_mae: 16.3450\n",
      "Epoch 51/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 20.6122 - mse: 37317.2812 - mae: 20.6124 - val_loss: 15.8688 - val_mse: 31029.0137 - val_mae: 15.8688\n",
      "Epoch 52/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 20.4877 - mse: 37335.0703 - mae: 20.4878 - val_loss: 14.5871 - val_mse: 33260.7734 - val_mae: 14.5871\n",
      "Epoch 53/1024\n",
      "1258/1258 [==============================] - 169s 133ms/step - loss: 27.2752 - mse: 37246.5430 - mae: 27.2754 - val_loss: 21.6188 - val_mse: 29357.8379 - val_mae: 21.6188\n",
      "Epoch 54/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 26.9859 - mse: 36308.8555 - mae: 26.9861 - val_loss: 16.7479 - val_mse: 28051.9707 - val_mae: 16.7479\n",
      "Epoch 55/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 22.1636 - mse: 30803.0215 - mae: 22.1637 - val_loss: 14.3587 - val_mse: 18707.3301 - val_mae: 14.3587\n",
      "Epoch 56/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 25.5587 - mse: 33289.1211 - mae: 25.5589 - val_loss: 17.9951 - val_mse: 21244.1133 - val_mae: 17.9951\n",
      "Epoch 57/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 23.0692 - mse: 29691.2617 - mae: 23.0694 - val_loss: 14.0787 - val_mse: 21061.4805 - val_mae: 14.0787\n",
      "Epoch 58/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 24.5305 - mse: 26864.4570 - mae: 24.5307 - val_loss: 29.2230 - val_mse: 30813.3223 - val_mae: 29.2230\n",
      "Epoch 59/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 33.8759 - mse: 39271.6211 - mae: 33.8761 - val_loss: 16.9061 - val_mse: 27641.9551 - val_mae: 16.9061\n",
      "Epoch 60/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 20.8310 - mse: 33955.7188 - mae: 20.8312 - val_loss: 14.4311 - val_mse: 26904.2480 - val_mae: 14.4311\n",
      "Epoch 61/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 37.7045 - mse: 47491.1250 - mae: 37.7048 - val_loss: 24.8690 - val_mse: 31466.2031 - val_mae: 24.8690\n",
      "Epoch 62/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 27.9114 - mse: 37829.6680 - mae: 27.9116 - val_loss: 23.8422 - val_mse: 29444.8926 - val_mae: 23.8422\n",
      "Epoch 63/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 25.1152 - mse: 36469.1367 - mae: 25.1154 - val_loss: 19.3909 - val_mse: 29336.0781 - val_mae: 19.3909\n",
      "Epoch 64/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 23.7616 - mse: 36707.5078 - mae: 23.7618 - val_loss: 58.8434 - val_mse: 63175.3828 - val_mae: 58.8434\n",
      "Epoch 65/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 71.8286 - mse: 82804.5469 - mae: 71.8292 - val_loss: 39.7504 - val_mse: 37550.0703 - val_mae: 39.7504\n",
      "Epoch 66/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 36.3142 - mse: 37685.2305 - mae: 36.3145 - val_loss: 24.9921 - val_mse: 25450.5195 - val_mae: 24.9921\n",
      "Epoch 67/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 31.4689 - mse: 28905.0137 - mae: 31.4692 - val_loss: 20.6980 - val_mse: 15655.7227 - val_mae: 20.6980\n",
      "Epoch 68/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 25.9882 - mse: 22565.9121 - mae: 25.9884 - val_loss: 36.6413 - val_mse: 24916.4707 - val_mae: 36.6413\n",
      "Epoch 69/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 24.0518 - mse: 21912.1445 - mae: 24.0520 - val_loss: 24.0356 - val_mse: 29504.4316 - val_mae: 24.0356\n",
      "Epoch 70/1024\n",
      "1258/1258 [==============================] - 167s 132ms/step - loss: 29.9410 - mse: 28819.2578 - mae: 29.9413 - val_loss: 51.4978 - val_mse: 41395.6797 - val_mae: 51.4978\n",
      "Epoch 71/1024\n",
      "1258/1258 [==============================] - 154s 122ms/step - loss: 69.5604 - mse: 70629.0781 - mae: 69.5608 - val_loss: 32.7601 - val_mse: 32956.5742 - val_mae: 32.7601\n",
      "Epoch 72/1024\n",
      "1258/1258 [==============================] - 154s 122ms/step - loss: 35.9901 - mse: 42648.8320 - mae: 35.9903 - val_loss: 30.1246 - val_mse: 37229.6133 - val_mae: 30.1246\n",
      "Epoch 73/1024\n",
      "1258/1258 [==============================] - 155s 122ms/step - loss: 27.2143 - mse: 39631.8672 - mae: 27.2145 - val_loss: 18.6264 - val_mse: 31133.0625 - val_mae: 18.6264\n",
      "Epoch 74/1024\n",
      "1258/1258 [==============================] - 154s 122ms/step - loss: 28.5971 - mse: 39819.0273 - mae: 28.5973 - val_loss: 22.3858 - val_mse: 36977.9453 - val_mae: 22.3858\n",
      "Epoch 75/1024\n",
      "1258/1258 [==============================] - 154s 122ms/step - loss: 24.8855 - mse: 38246.6602 - mae: 24.8857 - val_loss: 15.9932 - val_mse: 33129.2617 - val_mae: 15.9932\n",
      "Epoch 76/1024\n",
      "1258/1258 [==============================] - 155s 122ms/step - loss: 21.8414 - mse: 36233.8398 - mae: 21.8415 - val_loss: 57.7317 - val_mse: 64934.0664 - val_mae: 57.7317\n",
      "Epoch 77/1024\n",
      "1258/1258 [==============================] - 163s 129ms/step - loss: 26.3769 - mse: 38027.3398 - mae: 26.3771 - val_loss: 15.5955 - val_mse: 30044.7441 - val_mae: 15.5955\n",
      "Epoch 78/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 34.7072 - mse: 44376.5820 - mae: 34.7074 - val_loss: 20.4076 - val_mse: 30233.4316 - val_mae: 20.4076\n",
      "Epoch 79/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 30.9490 - mse: 40935.2109 - mae: 30.9492 - val_loss: 18.0244 - val_mse: 27636.3145 - val_mae: 18.0244\n",
      "Epoch 80/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 68.4846 - mse: 85193.2031 - mae: 68.4851 - val_loss: 34.1025 - val_mse: 29961.7754 - val_mae: 34.1025\n",
      "Epoch 81/1024\n",
      "1258/1258 [==============================] - 169s 133ms/step - loss: 33.2078 - mse: 28922.0645 - mae: 33.2080 - val_loss: 32.1405 - val_mse: 22823.7090 - val_mae: 32.1405\n",
      "Epoch 82/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 28.7119 - mse: 24499.1016 - mae: 28.7121 - val_loss: 19.9890 - val_mse: 13768.2148 - val_mae: 19.9890\n",
      "Epoch 83/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 25.0481 - mse: 23082.9258 - mae: 25.0483 - val_loss: 23.7747 - val_mse: 21211.8945 - val_mae: 23.7747\n",
      "Epoch 84/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 22.0592 - mse: 20858.0020 - mae: 22.0593 - val_loss: 30.5816 - val_mse: 17102.7344 - val_mae: 30.5816\n",
      "Epoch 85/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 24.8088 - mse: 22892.4590 - mae: 24.8090 - val_loss: 19.7938 - val_mse: 13330.6846 - val_mae: 19.7938\n",
      "Epoch 86/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 34.8572 - mse: 25773.9863 - mae: 34.8575 - val_loss: 23.8370 - val_mse: 18675.6816 - val_mae: 23.8370\n",
      "Epoch 87/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 63.6992 - mse: 64581.1016 - mae: 63.6996 - val_loss: 45.7448 - val_mse: 41831.0117 - val_mae: 45.7448\n",
      "Epoch 88/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 47.7703 - mse: 51806.1680 - mae: 47.7708 - val_loss: 35.0105 - val_mse: 40411.7070 - val_mae: 35.0105\n",
      "Epoch 89/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 37.2908 - mse: 43785.6914 - mae: 37.2911 - val_loss: 46.1969 - val_mse: 53341.1250 - val_mae: 46.1969\n",
      "Epoch 90/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 32.3176 - mse: 38792.3555 - mae: 32.3178 - val_loss: 19.9869 - val_mse: 28228.5195 - val_mae: 19.9869\n",
      "Epoch 91/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 44.4051 - mse: 51928.8164 - mae: 44.4054 - val_loss: 30.0188 - val_mse: 39210.1719 - val_mae: 30.0188\n",
      "Epoch 92/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 53.4306 - mse: 63450.0586 - mae: 53.4310 - val_loss: 51.5735 - val_mse: 55805.9023 - val_mae: 51.5735\n",
      "Epoch 93/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 42.9750 - mse: 49644.9492 - mae: 42.9753 - val_loss: 28.5950 - val_mse: 32781.5039 - val_mae: 28.5950\n",
      "Epoch 94/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 33.9439 - mse: 40281.5898 - mae: 33.9441 - val_loss: 23.1092 - val_mse: 34876.2969 - val_mae: 23.1092\n",
      "Epoch 95/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 30.8539 - mse: 43210.2891 - mae: 30.8542 - val_loss: 25.9473 - val_mse: 37928.7500 - val_mae: 25.9473\n",
      "Epoch 96/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 34.5757 - mse: 44208.0508 - mae: 34.5759 - val_loss: 40.7436 - val_mse: 41842.6055 - val_mae: 40.7436\n",
      "Epoch 97/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 52.1121 - mse: 59357.8906 - mae: 52.1124 - val_loss: 34.2120 - val_mse: 36058.2773 - val_mae: 34.2120\n",
      "Epoch 98/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 33.4885 - mse: 37445.6836 - mae: 33.4887 - val_loss: 26.8394 - val_mse: 28338.3926 - val_mae: 26.8394\n",
      "Epoch 99/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 29.4927 - mse: 33335.2227 - mae: 29.4929 - val_loss: 27.1079 - val_mse: 22480.6953 - val_mae: 27.1079\n",
      "Epoch 100/1024\n",
      "1258/1258 [==============================] - 168s 133ms/step - loss: 29.9906 - mse: 30068.7695 - mae: 29.9908 - val_loss: 20.9712 - val_mse: 17933.0508 - val_mae: 20.9712\n",
      "Epoch 101/1024\n",
      "1258/1258 [==============================] - 169s 134ms/step - loss: 28.5772 - mse: 28830.9688 - mae: 28.5774 - val_loss: 20.4518 - val_mse: 17752.8047 - val_mae: 20.4518\n",
      "Epoch 102/1024\n",
      "1258/1258 [==============================] - 167s 132ms/step - loss: 37.5859 - mse: 37726.6641 - mae: 37.5861 - val_loss: 34.1056 - val_mse: 31133.4336 - val_mae: 34.1056\n"
     ]
    }
   ],
   "source": [
    "NAME='densex2'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_densex2.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=50, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60a52820-5929-47a6-b787-9ab60dd994e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer LSTM-1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM-2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer LSTM-3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/1024\n",
      "1258/1258 [==============================] - 202s 159ms/step - loss: 702.1060 - mse: 3367508.0000 - mae: 702.1113 - val_loss: 2734.8110 - val_mse: 58324080.0000 - val_mae: 2734.8113\n",
      "Epoch 2/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 399.4943 - mse: 617801.5625 - mae: 399.4971 - val_loss: 326.4767 - val_mse: 231288.1562 - val_mae: 326.4768\n",
      "Epoch 3/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 1041.3079 - mse: 31620510.0000 - mae: 1041.3159 - val_loss: 420.3927 - val_mse: 335565.2500 - val_mae: 420.3929\n",
      "Epoch 4/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 392.8354 - mse: 979044.5625 - mae: 392.8384 - val_loss: 595.6715 - val_mse: 611720.5000 - val_mae: 595.6714\n",
      "Epoch 5/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 236.0323 - mse: 169316.1406 - mae: 236.0337 - val_loss: 574.1381 - val_mse: 545858.1875 - val_mae: 574.1381\n",
      "Epoch 6/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 194.1045 - mse: 125677.5312 - mae: 194.1059 - val_loss: 461.4908 - val_mse: 399284.5312 - val_mae: 461.4910\n",
      "Epoch 7/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 173.6244 - mse: 116147.6406 - mae: 173.6258 - val_loss: 503.9532 - val_mse: 462911.7500 - val_mae: 503.9531\n",
      "Epoch 8/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 150.3615 - mse: 100980.7109 - mae: 150.3628 - val_loss: 474.8628 - val_mse: 436406.0625 - val_mae: 474.8629\n",
      "Epoch 9/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 152.6740 - mse: 1826203904.0000 - mae: 152.6754 - val_loss: 452.5268 - val_mse: 427472.1562 - val_mae: 452.5266\n",
      "Epoch 10/1024\n",
      "1258/1258 [==============================] - 207s 164ms/step - loss: 125.0910 - mse: 89242.3906 - mae: 125.0919 - val_loss: 404.9512 - val_mse: 361403.5000 - val_mae: 404.9511\n",
      "Epoch 11/1024\n",
      "1258/1258 [==============================] - 207s 164ms/step - loss: 119.4985 - mse: 87213.8828 - mae: 119.4993 - val_loss: 579.3033 - val_mse: 590516.4375 - val_mae: 579.3036\n",
      "Epoch 12/1024\n",
      "1258/1258 [==============================] - 207s 164ms/step - loss: 121.9873 - mse: 92256.5781 - mae: 121.9883 - val_loss: 478.4431 - val_mse: 444696.5938 - val_mae: 478.4431\n",
      "Epoch 13/1024\n",
      "1258/1258 [==============================] - 207s 164ms/step - loss: 120.3256 - mse: 98885.0547 - mae: 120.3265 - val_loss: 411.2870 - val_mse: 359503.2188 - val_mae: 411.2869\n",
      "Epoch 14/1024\n",
      "1258/1258 [==============================] - 205s 162ms/step - loss: 111.6556 - mse: 84697.2344 - mae: 111.6564 - val_loss: 427.0576 - val_mse: 381443.9375 - val_mae: 427.0576\n",
      "Epoch 15/1024\n",
      "1258/1258 [==============================] - 205s 162ms/step - loss: 108.6486 - mse: 84359.9688 - mae: 108.6493 - val_loss: 500.8966 - val_mse: 482676.6562 - val_mae: 500.8967\n",
      "Epoch 16/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 106.1867 - mse: 82682.6250 - mae: 106.1875 - val_loss: 458.3425 - val_mse: 416042.8125 - val_mae: 458.3422\n",
      "Epoch 17/1024\n",
      "1258/1258 [==============================] - 204s 161ms/step - loss: 102.5645 - mse: 82201.7109 - mae: 102.5652 - val_loss: 438.8376 - val_mse: 379269.2812 - val_mae: 438.8374\n",
      "Epoch 18/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 95.8939 - mse: 77731.5312 - mae: 95.8946 - val_loss: 411.8668 - val_mse: 359475.0625 - val_mae: 411.8665\n",
      "Epoch 19/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 91.9428 - mse: 74804.1172 - mae: 91.9435 - val_loss: 419.7726 - val_mse: 357992.0312 - val_mae: 419.7723\n",
      "Epoch 20/1024\n",
      "1258/1258 [==============================] - 205s 162ms/step - loss: 234.9624 - mse: 9541171.0000 - mae: 234.9641 - val_loss: 441.8236 - val_mse: 338479.1875 - val_mae: 441.8237\n",
      "Epoch 21/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 189.3925 - mse: 675166.1875 - mae: 189.3938 - val_loss: 526.1981 - val_mse: 500470.1562 - val_mae: 526.1982\n",
      "Epoch 22/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 144.0493 - mse: 98182.6250 - mae: 144.0503 - val_loss: 558.5646 - val_mse: 565608.6250 - val_mae: 558.5644\n",
      "Epoch 23/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 133.6394 - mse: 94279.7969 - mae: 133.6402 - val_loss: 553.0085 - val_mse: 547543.8750 - val_mae: 553.0085\n",
      "Epoch 24/1024\n",
      "1258/1258 [==============================] - 206s 163ms/step - loss: 127.3162 - mse: 90922.1562 - mae: 127.3172 - val_loss: 537.9349 - val_mse: 524085.3438 - val_mae: 537.9351\n",
      "Epoch 25/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 389.4811 - mse: 10820564.0000 - mae: 389.4835 - val_loss: 421.5241 - val_mse: 332238.0625 - val_mae: 421.5243\n",
      "Epoch 26/1024\n",
      "1258/1258 [==============================] - 205s 163ms/step - loss: 183.8714 - mse: 120594.2734 - mae: 183.8726 - val_loss: 540.1902 - val_mse: 512633.8750 - val_mae: 540.1902\n",
      "Epoch 27/1024\n",
      "1258/1258 [==============================] - 210s 166ms/step - loss: 184.2410 - mse: 136052.8281 - mae: 184.2421 - val_loss: 522.2015 - val_mse: 471969.5312 - val_mae: 522.2016\n",
      "Epoch 28/1024\n",
      "1258/1258 [==============================] - 223s 177ms/step - loss: 151.2975 - mse: 99728.1797 - mae: 151.2984 - val_loss: 500.8343 - val_mse: 447996.8438 - val_mae: 500.8340\n",
      "Epoch 29/1024\n",
      "1258/1258 [==============================] - 223s 177ms/step - loss: 139.2727 - mse: 95246.2891 - mae: 139.2737 - val_loss: 508.6428 - val_mse: 470489.5938 - val_mae: 508.6429\n",
      "Epoch 30/1024\n",
      "1258/1258 [==============================] - 223s 177ms/step - loss: 134.3721 - mse: 313142.6562 - mae: 134.3732 - val_loss: 531.0477 - val_mse: 508073.6875 - val_mae: 531.0475\n",
      "Epoch 31/1024\n",
      "1258/1258 [==============================] - 223s 177ms/step - loss: 154.9951 - mse: 885950.0000 - mae: 154.9962 - val_loss: 584.3288 - val_mse: 591024.4375 - val_mae: 584.3288\n",
      "Epoch 32/1024\n",
      "1258/1258 [==============================] - 223s 177ms/step - loss: 129.2233 - mse: 91563.5156 - mae: 129.2242 - val_loss: 581.4423 - val_mse: 591690.1875 - val_mae: 581.4423\n",
      "Epoch 33/1024\n",
      "1258/1258 [==============================] - 223s 176ms/step - loss: 121.1577 - mse: 88755.0469 - mae: 121.1587 - val_loss: 585.6787 - val_mse: 597741.9375 - val_mae: 585.6785\n",
      "Epoch 34/1024\n",
      "1258/1258 [==============================] - 222s 176ms/step - loss: 115.9763 - mse: 87512.7031 - mae: 115.9770 - val_loss: 647.1198 - val_mse: 710939.0000 - val_mae: 647.1196\n",
      "Epoch 35/1024\n",
      "1199/1258 [===========================>..] - ETA: 9s - loss: 109.0054 - mse: 85707.4922 - mae: 109.0059"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(NAME))\n\u001b[0;32m---> 19\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py:1221\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1221\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1223\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:436\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 436\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    319\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:354\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    353\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 354\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1032\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1032\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1104\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1103\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:554\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:550\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    549\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 550\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1149\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1149\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1115\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1114\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complex model multi dense\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, activation='relu', name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, activation='relu', name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, activation='relu', name='LSTM-3')(dropout2)\n",
    "dense = keras.layers.Dense(128, name='Dense', activation='relu')(lstm3)\n",
    "output = keras.layers.Dense(1, name='Output')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')\n",
    "\n",
    "\n",
    "NAME='densex2b'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_densex2b.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03cbb93a-133a-4770-8573-bd4d6d30d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "1258/1258 [==============================] - 158s 123ms/step - loss: 458.6237 - mse: 703029.5625 - mae: 417.5659 - val_loss: 211.1664 - val_mse: 111350.6250 - val_mae: 172.6450\n",
      "Epoch 2/1024\n",
      "1258/1258 [==============================] - 159s 126ms/step - loss: 137.3796 - mse: 77218.4453 - mae: 105.6222 - val_loss: 96.7032 - val_mse: 49779.1992 - val_mae: 67.5796\n",
      "Epoch 3/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 90.0491 - mse: 46306.7344 - mae: 61.9814 - val_loss: 89.5690 - val_mse: 45805.0508 - val_mae: 63.5340\n",
      "Epoch 4/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 77.2846 - mse: 41509.6484 - mae: 52.0244 - val_loss: 73.4656 - val_mse: 43733.3164 - val_mae: 49.8165\n",
      "Epoch 5/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 67.1152 - mse: 39423.5898 - mae: 44.1189 - val_loss: 61.5279 - val_mse: 34259.3789 - val_mae: 39.3101\n",
      "Epoch 6/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 167.7074 - mse: 150305.9688 - mae: 146.6620 - val_loss: 197.1490 - val_mse: 202928.4062 - val_mae: 178.2056\n",
      "Epoch 7/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 161.0382 - mse: 121873.9922 - mae: 142.5933 - val_loss: 76.1864 - val_mse: 40255.9375 - val_mae: 57.8389\n",
      "Epoch 8/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 82.1648 - mse: 47783.9805 - mae: 64.5501 - val_loss: 75.5051 - val_mse: 39667.7891 - val_mae: 58.4998\n",
      "Epoch 9/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 68.1851 - mse: 42237.5586 - mae: 52.0832 - val_loss: 53.9493 - val_mse: 31090.5332 - val_mae: 38.2850\n",
      "Epoch 10/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 63.8057 - mse: 40984.9570 - mae: 48.9929 - val_loss: 54.0876 - val_mse: 33147.8047 - val_mae: 39.6629\n",
      "Epoch 11/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 65.6173 - mse: 44670.0898 - mae: 51.5827 - val_loss: 194.8130 - val_mse: 221190.9531 - val_mae: 181.8016\n",
      "Epoch 12/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 64.9652 - mse: 46452.4805 - mae: 51.4962 - val_loss: 46.3002 - val_mse: 29756.1113 - val_mae: 33.2561\n",
      "Epoch 13/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 51.5155 - mse: 39067.4727 - mae: 38.8679 - val_loss: 43.7531 - val_mse: 30722.9629 - val_mae: 31.4217\n",
      "Epoch 14/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 49.3813 - mse: 39046.3477 - mae: 37.3638 - val_loss: 65.6289 - val_mse: 53781.0781 - val_mae: 54.5146\n",
      "Epoch 15/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 103.1297 - mse: 101205.2188 - mae: 92.1609 - val_loss: 193.0530 - val_mse: 218697.6406 - val_mae: 183.5074\n",
      "Epoch 16/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 152.7161 - mse: 131975.4688 - mae: 140.4223 - val_loss: 79.9785 - val_mse: 53600.6523 - val_mae: 67.2947\n",
      "Epoch 17/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 68.0816 - mse: 46989.3008 - mae: 55.7820 - val_loss: 58.9999 - val_mse: 39879.3867 - val_mae: 47.3031\n",
      "Epoch 18/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 108.9854 - mse: 86755.0859 - mae: 97.7748 - val_loss: 74.9717 - val_mse: 52974.6797 - val_mae: 63.1860\n",
      "Epoch 19/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 192.5156 - mse: 186890.6875 - mae: 181.8600 - val_loss: 137.7920 - val_mse: 96915.2500 - val_mae: 127.3783\n",
      "Epoch 20/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 159.3128 - mse: 124986.3672 - mae: 146.4648 - val_loss: 62.8945 - val_mse: 41110.3398 - val_mae: 50.9139\n",
      "Epoch 21/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 70.2750 - mse: 49215.0703 - mae: 59.1096 - val_loss: 65.1445 - val_mse: 45041.0820 - val_mae: 54.6965\n",
      "Epoch 22/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 60.0476 - mse: 46781.6328 - mae: 49.9916 - val_loss: 44.6627 - val_mse: 38433.2773 - val_mae: 34.8383\n",
      "Epoch 23/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 67.9780 - mse: 53368.6250 - mae: 58.4635 - val_loss: 73.3285 - val_mse: 47522.3711 - val_mae: 64.1083\n",
      "Epoch 24/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 68.5140 - mse: 53327.3086 - mae: 59.8942 - val_loss: 60.1887 - val_mse: 49053.9258 - val_mae: 52.0639\n",
      "Epoch 25/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 55.8403 - mse: 48753.1602 - mae: 47.7611 - val_loss: 49.5547 - val_mse: 43511.5430 - val_mae: 41.8434\n",
      "Epoch 26/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 75.7601 - mse: 68793.9375 - mae: 68.4208 - val_loss: 84.8034 - val_mse: 73945.8359 - val_mae: 77.9103\n",
      "Epoch 27/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 107.2459 - mse: 113751.0078 - mae: 100.3689 - val_loss: 176.9364 - val_mse: 231792.1562 - val_mae: 170.6045\n",
      "Epoch 28/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 201.3110 - mse: 230455.0000 - mae: 194.2502 - val_loss: 195.0997 - val_mse: 226249.5625 - val_mae: 187.7045\n",
      "Epoch 29/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 203.4831 - mse: 230877.4062 - mae: 195.4026 - val_loss: 194.3751 - val_mse: 230807.6562 - val_mae: 185.9468\n",
      "Epoch 30/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 162.0951 - mse: 188876.0312 - mae: 153.9852 - val_loss: 64.2642 - val_mse: 48812.3594 - val_mae: 56.1776\n",
      "Epoch 31/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 54.6323 - mse: 41188.3359 - mae: 46.5754 - val_loss: 63.9149 - val_mse: 48176.7852 - val_mae: 56.2476\n",
      "Epoch 32/1024\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 133.4448 - mse: 139667.3125 - mae: 125.7206 - val_loss: 83.1473 - val_mse: 69542.3281 - val_mae: 75.0392\n",
      "Epoch 33/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 200.5334 - mse: 225555.9688 - mae: 192.7490 - val_loss: 184.7267 - val_mse: 218324.5938 - val_mae: 177.6806\n",
      "Epoch 34/1024\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 198.0333 - mse: 229301.1250 - mae: 191.0432 - val_loss: 202.8962 - val_mse: 232055.3125 - val_mae: 195.7824\n"
     ]
    }
   ],
   "source": [
    "# Complex model multi dense\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-3')(dropout2)\n",
    "dense = keras.layers.Dense(128, name='Dense', activation='relu')(lstm3)\n",
    "output = keras.layers.Dense(1, name='Output')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')\n",
    "\n",
    "\n",
    "NAME='densex2c'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_densex2c.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "42ebcbd3-8213-4e55-8ecd-90f7ffdad381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Covid-Prediction-30-1-BatchNorm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 30, 101)]         0         \n",
      "                                                                 \n",
      " LSTM-1 (LSTM)               (None, 30, 256)           366592    \n",
      "                                                                 \n",
      " Dropout-1 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " Batch-Normalization-1 (Batc  (None, 30, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " LSTM-2 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " Dropout-2 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " Batch-Normalization-2 (Batc  (None, 30, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " LSTM-3 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " Dropout-3 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " Batch-Normalization-3 (Batc  (None, 30, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " Output (Dense)              (None, 30, 1)             257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,420,545\n",
      "Trainable params: 1,419,009\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "Epoch 1/1024\n",
      "1258/1258 [==============================] - 170s 133ms/step - loss: 1416.5576 - mse: 3518188.7500 - mae: 1416.5673 - val_loss: 1391.9730 - val_mse: 3298910.5000 - val_mae: 1391.9720\n",
      "Epoch 2/1024\n",
      "1258/1258 [==============================] - 171s 135ms/step - loss: 495.8170 - mse: 546783.5000 - mae: 495.8205 - val_loss: 110.0454 - val_mse: 76804.3359 - val_mae: 110.0454\n",
      "Epoch 3/1024\n",
      "1258/1258 [==============================] - 171s 135ms/step - loss: 346.7640 - mse: 274513.5312 - mae: 346.7660 - val_loss: 104.7246 - val_mse: 52460.4609 - val_mae: 104.7246\n",
      "Epoch 4/1024\n",
      "1258/1258 [==============================] - 171s 135ms/step - loss: 336.0925 - mse: 287103.2812 - mae: 336.0950 - val_loss: 111.3549 - val_mse: 53142.0430 - val_mae: 111.3550\n",
      "Epoch 5/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 341.2858 - mse: 308983.0625 - mae: 341.2883 - val_loss: 128.2324 - val_mse: 58443.6953 - val_mae: 128.2324\n",
      "Epoch 6/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 343.0390 - mse: 309620.8125 - mae: 343.0417 - val_loss: 200.4723 - val_mse: 167199.3594 - val_mae: 200.4723\n",
      "Epoch 7/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 345.6284 - mse: 297552.7812 - mae: 345.6308 - val_loss: 173.9397 - val_mse: 92073.0703 - val_mae: 173.9398\n",
      "Epoch 8/1024\n",
      "1258/1258 [==============================] - 171s 135ms/step - loss: 339.4654 - mse: 306765.1250 - mae: 339.4679 - val_loss: 98.8079 - val_mse: 46640.3594 - val_mae: 98.8079\n",
      "Epoch 9/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 333.5527 - mse: 311333.4688 - mae: 333.5550 - val_loss: 381.4427 - val_mse: 372519.9688 - val_mae: 381.4427\n",
      "Epoch 10/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 329.3329 - mse: 329438.2188 - mae: 329.3353 - val_loss: 147.1796 - val_mse: 62938.4609 - val_mae: 147.1796\n",
      "Epoch 11/1024\n",
      "1258/1258 [==============================] - 172s 137ms/step - loss: 326.9991 - mse: 320584.7188 - mae: 327.0017 - val_loss: 168.0052 - val_mse: 76566.5703 - val_mae: 168.0052\n",
      "Epoch 12/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 332.2729 - mse: 333215.7500 - mae: 332.2755 - val_loss: 159.8363 - val_mse: 78153.0469 - val_mae: 159.8363\n",
      "Epoch 13/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 335.8900 - mse: 341785.0312 - mae: 335.8922 - val_loss: 130.8359 - val_mse: 63724.1328 - val_mae: 130.8358\n",
      "Epoch 14/1024\n",
      "1258/1258 [==============================] - 171s 136ms/step - loss: 337.3808 - mse: 352013.1562 - mae: 337.3829 - val_loss: 144.4527 - val_mse: 71056.7656 - val_mae: 144.4528\n",
      "Epoch 15/1024\n",
      "1258/1258 [==============================] - 172s 136ms/step - loss: 324.4100 - mse: 344003.3750 - mae: 324.4120 - val_loss: 207.7948 - val_mse: 118254.1328 - val_mae: 207.7948\n",
      "Epoch 16/1024\n",
      "1258/1258 [==============================] - 173s 137ms/step - loss: 330.1512 - mse: 369310.8125 - mae: 330.1538 - val_loss: 183.5926 - val_mse: 83925.5391 - val_mae: 183.5925\n",
      "Epoch 17/1024\n",
      "1258/1258 [==============================] - 171s 135ms/step - loss: 332.3453 - mse: 381186.2188 - mae: 332.3475 - val_loss: 234.1611 - val_mse: 210572.4375 - val_mae: 234.1612\n",
      "Epoch 18/1024\n",
      "1258/1258 [==============================] - 172s 136ms/step - loss: 328.2836 - mse: 387999.3438 - mae: 328.2860 - val_loss: 233.0975 - val_mse: 161767.4688 - val_mae: 233.0974\n",
      "Epoch 19/1024\n",
      "1258/1258 [==============================] - 172s 136ms/step - loss: 323.5865 - mse: 374106.4062 - mae: 323.5887 - val_loss: 161.5987 - val_mse: 73138.4062 - val_mae: 161.5986\n",
      "Epoch 20/1024\n",
      " 484/1258 [==========>...................] - ETA: 1:38 - loss: 349.5117 - mse: 432620.4688 - mae: 349.5115"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(NAME))\n\u001b[0;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py:1221\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1221\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1223\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:436\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 436\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    319\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:354\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    353\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 354\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1032\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1032\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1104\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1103\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:554\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:550\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    549\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 550\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1149\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1149\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1115\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1114\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complex model one dense, batch normalization\n",
    "\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "batchnorm1 = keras.layers.BatchNormalization(name='Batch-Normalization-1')(dropout1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-2')(batchnorm1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "batchnorm2 = keras.layers.BatchNormalization(name='Batch-Normalization-2')(dropout2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-3')(batchnorm2)\n",
    "dropout3 = keras.layers.Dropout(0.20, name='Dropout-3')(lstm3)\n",
    "batchnorm3 = keras.layers.BatchNormalization(name='Batch-Normalization-3')(dropout3)\n",
    "output = keras.layers.Dense(1, name='Output')(batchnorm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-BatchNorm')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "NAME='batchnorm'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_batchnorm.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d3f08-9917-4700-aa20-875fcedfa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added this to the data preprocessing, so not required\n",
    "\n",
    "class TimeEncoding(keras.layers.Layer):\n",
    "    \"\"\" Layer to encode cyclical and continuous time.  \n",
    "    Input should an n x 1 array or vector of integers.  \n",
    "    Integers represent number of time units (i.e., days) from the starting point\"\"\"\n",
    "\n",
    "    def __init__(self, cyclical_interval=365, continuous_interval=3650 , **kwargs):\n",
    "        self.cyclical_interval = cyclical_interval\n",
    "        self.continuous_interval = continuous_interval\n",
    "        super(TimeEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        cyclical_sin = np.sin((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        cyclical_cos = np.cos((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        continuous_sin = np.sin((x * 2 * np.pi)/self.continuous_interval)\n",
    "        continuous_cos = np.cos((x * 2 * np.pi)/self.continuous_interval)\n",
    "        \n",
    "        return keras.layers.concatenate([cyclical_sin, cyclical_cos, continuous_sin, continuous_cos], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42a910-0e7c-4bad-b21a-fc9b8a939924",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_encoding = TimeEncoding()\n",
    "x = np.arange(30)/1.0\n",
    "x = x[:, tf.newaxis]\n",
    "time_encoding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fb679-5415-4550-ad4c-e80deaa94f1f",
   "metadata": {},
   "source": [
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(512, return_sequences=True)(input)\n",
    "dropout1 = keras.layers.Dropout(0.20)(lstm1)\n",
    "lstm2 = keras.layers.LSTM(512, return_sequences=True)(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20)(lstm2)\n",
    "lstm3 = keras.layers.LSTM(512, return_sequences=True)(dropout2)\n",
    "dropout3 = keras.layers.Dropout(0.20)(lstm3)\n",
    "lstm4 = keras.layers.LSTM(512, return_sequences=True)(dropout3)\n",
    "dropout4 = keras.layers.Dropout(0.20)(lstm4)\n",
    "lstm5 = keras.layers.LSTM(512)(dropout4)\n",
    "output = keras.layers.Dense(1)(lstm5)\n",
    "model = keras.models.Model(inputs=input, outputs=output)\n",
    "\n",
    "\n",
    "Output with 5 LSTM @512, one dense layer\n",
    "\n",
    "Epoch 1/32\n",
    "1256/1256 [==============================] - 854s 677ms/step - loss: 1525.3854 - mse: 3823870.7500 - mae: 1525.3854 - val_loss: 1395.9611 - val_mse: 3222036.5000 - val_mae: 1395.9611\n",
    "Epoch 2/32\n",
    "1256/1256 [==============================] - 860s 685ms/step - loss: 1306.3831 - mse: 2720509.2500 - mae: 1306.3831 - val_loss: 1268.0597 - val_mse: 2458543.2500 - val_mae: 1268.0597\n",
    "Epoch 3/32\n",
    "1256/1256 [==============================] - 861s 686ms/step - loss: 1224.3491 - mse: 2193218.7500 - mae: 1224.3491 - val_loss: 1221.3195 - val_mse: 2095028.1250 - val_mae: 1221.3195\n",
    "Epoch 4/32\n",
    "1256/1256 [==============================] - 861s 685ms/step - loss: 1190.3538 - mse: 1940674.8750 - mae: 1190.3538 - val_loss: 1205.6909 - val_mse: 1935832.7500 - val_mae: 1205.6909\n",
    "Epoch 5/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1179.1588 - mse: 1829032.5000 - mae: 1179.1588 - val_loss: 1197.5588 - val_mse: 1859381.2500 - val_mae: 1197.5588\n",
    "Epoch 6/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1173.8285 - mse: 1778276.3750 - mae: 1173.8285 - val_loss: 1195.9708 - val_mse: 1827763.8750 - val_mae: 1195.9708\n",
    "Epoch 7/32\n",
    "1256/1256 [==============================] - 858s 683ms/step - loss: 1170.9568 - mse: 1751288.7500 - mae: 1170.9568 - val_loss: 1193.4899 - val_mse: 1804784.8750 - val_mae: 1193.4899\n",
    "Epoch 8/32\n",
    "1256/1256 [==============================] - 859s 684ms/step - loss: 1168.4807 - mse: 1736100.6250 - mae: 1168.4807 - val_loss: 1190.9672 - val_mse: 1791784.7500 - val_mae: 1190.9672\n",
    "Epoch 9/32\n",
    "1256/1256 [==============================] - 856s 682ms/step - loss: 1166.9685 - mse: 1730238.2500 - mae: 1166.9685 - val_loss: 1191.2921 - val_mse: 1792275.2500 - val_mae: 1191.2921\n",
    "Epoch 10/32\n",
    "1256/1256 [==============================] - 855s 681ms/step - loss: 1166.2148 - mse: 1729396.2500 - mae: 1166.2148 - val_loss: 1189.8693 - val_mse: 1789450.0000 - val_mae: 1189.8693\n",
    "Epoch 11/32\n",
    "1256/1256 [==============================] - 856s 681ms/step - loss: 1165.9210 - mse: 1729189.6250 - mae: 1165.9210 - val_loss: 1190.8143 - val_mse: 1792432.6250 - val_mae: 1190.8143\n",
    "Epoch 12/32\n",
    "1256/1256 [==============================] - 856s 682ms/step - loss: 1165.8933 - mse: 1730290.7500 - mae: 1165.8933 - val_loss: 1190.2284 - val_mse: 1791079.6250 - val_mae: 1190.2284\n",
    "Epoch 13/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1165.8029 - mse: 1731005.8750 - mae: 1165.8029 - val_loss: 1189.7284 - val_mse: 1790104.0000 - val_mae: 1189.7284\n",
    "Epoch 14/32\n",
    "1256/1256 [==============================] - 854s 680ms/step - loss: 1165.6666 - mse: 1729771.0000 - mae: 1165.6666 - val_loss: 1190.9232 - val_mse: 1793442.3750 - val_mae: 1190.9232\n",
    "Epoch 15/32\n",
    "1256/1256 [==============================] - 853s 680ms/step - loss: 1166.0372 - mse: 1731329.2500 - mae: 1166.0372 - val_loss: 1190.1742 - val_mse: 1791922.6250 - val_mae: 1190.1742\n",
    "Epoch 16/32\n",
    " 532/1256 [===========>..................] - ETA: 7:34 - loss: 1170.4701 - mse: 1743321.3750 - mae: 1170.4701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d016b9-bb92-4212-b14f-2ff53eb074a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
