{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d0698-a89c-4f60-8092-ad842f06b879",
   "metadata": {},
   "source": [
    "# Experiments Notebook 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8519c3-fa4d-4404-a8c7-9486a7e14861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d96a9b-5d5d-4b5d-b461-746bcc80384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-hypetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f293244-628b-4bee-a316-cf96b7b89bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import bz2\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kerashypetune import KerasGridSearch\n",
    "\n",
    "\n",
    "import preprocessing as pp\n",
    "import dsci592.model as dsci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b46a13d-bfa6-4e51-a90e-8d7fb47d8a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dsci592.model' from '/home/noah/projects/drexel/dsci592/DS-capstone-pt1/code/src/dsci592/model.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dsci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce25521f-bad6-4705-b439-eda844e32615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183fcd9-48c1-4e6d-a197-4c39853316c7",
   "metadata": {},
   "source": [
    "### Load the golden data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c511402-262c-45a0-a9ae-148b6802fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows version\n",
    "golden_dataset_file_name = os.path.join('..', '..', 'data', 'golden', 'feeFiFoFum.pbz2')\n",
    "\n",
    "# data = bz2.BZ2File(golden_dataset_file_name,'rb')\n",
    "with bz2.BZ2File(golden_dataset_file_name,'rb') as data:\n",
    "    df = pd.read_pickle(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a67ff56-fe24-41c7-8942-56c66eb1451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>JHU_ConfirmedCases.data</th>\n",
       "      <th>NYT_ConfirmedCases.data</th>\n",
       "      <th>JHU_ConfirmedDeaths.data</th>\n",
       "      <th>NYT_ConfirmedDeaths.data</th>\n",
       "      <th>NYT_ConfirmedDeaths.missing</th>\n",
       "      <th>TotalPopulation.data</th>\n",
       "      <th>MaleAndFemale_AtLeast65_Population.data</th>\n",
       "      <th>Male_Total_Population.data</th>\n",
       "      <th>Female_Total_Population.data</th>\n",
       "      <th>MaleAndFemale_Under18_Population.data</th>\n",
       "      <th>BLS_EmployedPopulation.data</th>\n",
       "      <th>BLS_EmployedPopulation.missing</th>\n",
       "      <th>BLS_UnemployedPopulation.data</th>\n",
       "      <th>BLS_UnemployedPopulation.missing</th>\n",
       "      <th>BLS_UnemploymentRate.data</th>\n",
       "      <th>BLS_UnemploymentRate.missing</th>\n",
       "      <th>BLS_LaborForcePopulation.data</th>\n",
       "      <th>BLS_LaborForcePopulation.missing</th>\n",
       "      <th>AverageDailyTemperature.data</th>\n",
       "      <th>AverageDailyTemperature.missing</th>\n",
       "      <th>AverageDewPoint.data</th>\n",
       "      <th>AverageDewPoint.missing</th>\n",
       "      <th>AverageRelativeHumidity.data</th>\n",
       "      <th>AverageRelativeHumidity.missing</th>\n",
       "      <th>AverageSurfaceAirPressure.data</th>\n",
       "      <th>AverageSurfaceAirPressure.missing</th>\n",
       "      <th>AveragePrecipitationTotal.data</th>\n",
       "      <th>AveragePrecipitationTotal.missing</th>\n",
       "      <th>AveragePrecipitation.data</th>\n",
       "      <th>AveragePrecipitation.missing</th>\n",
       "      <th>AverageWindDirection.data</th>\n",
       "      <th>AverageWindDirection.missing</th>\n",
       "      <th>AverageWindSpeed.data</th>\n",
       "      <th>AverageWindSpeed.missing</th>\n",
       "      <th>hospitalIcuBeds</th>\n",
       "      <th>hospitalStaffedBeds</th>\n",
       "      <th>hospitalLicensedBeds</th>\n",
       "      <th>latestTotalPopulation</th>\n",
       "      <th>fips</th>\n",
       "      <th>county</th>\n",
       "      <th>jhu_daily_death</th>\n",
       "      <th>jhu_daily_cases</th>\n",
       "      <th>jhu_daily_new_cases</th>\n",
       "      <th>jhu_daily_death_rolling_7</th>\n",
       "      <th>jhu_daily_cases_rolling_7</th>\n",
       "      <th>jhu_daily_new_cases_rolling_7</th>\n",
       "      <th>jhu_daily_death_rolling_30</th>\n",
       "      <th>jhu_daily_cases_rolling_30</th>\n",
       "      <th>jhu_daily_new_cases_rolling_30</th>\n",
       "      <th>LND110210</th>\n",
       "      <th>jhu_death_rate</th>\n",
       "      <th>jhu_case_rate</th>\n",
       "      <th>jhu_new_case_rate</th>\n",
       "      <th>density</th>\n",
       "      <th>icu_beds_per_person</th>\n",
       "      <th>staffed_beds_per_person</th>\n",
       "      <th>licensed_beds_per_person</th>\n",
       "      <th>cold_days</th>\n",
       "      <th>hot_days</th>\n",
       "      <th>moderate_days</th>\n",
       "      <th>gte_65_percent</th>\n",
       "      <th>lt_18_percent</th>\n",
       "      <th>employed_percent</th>\n",
       "      <th>unemployed_percent</th>\n",
       "      <th>totalMoved</th>\n",
       "      <th>movedWithinState</th>\n",
       "      <th>movedWithoutState</th>\n",
       "      <th>movedFromAbroad</th>\n",
       "      <th>publicTrans</th>\n",
       "      <th>totalTrans</th>\n",
       "      <th>householdsTotal</th>\n",
       "      <th>houseWith65</th>\n",
       "      <th>house2+with65</th>\n",
       "      <th>houseFamily65</th>\n",
       "      <th>houseNonfam65</th>\n",
       "      <th>houseNo65</th>\n",
       "      <th>house2+No65</th>\n",
       "      <th>houseFamilyNo65</th>\n",
       "      <th>houseNonfamNo65</th>\n",
       "      <th>householdStructuresTotal</th>\n",
       "      <th>householdIncomeMedian</th>\n",
       "      <th>gini</th>\n",
       "      <th>hoursWorkedMean</th>\n",
       "      <th>unitsInStructure</th>\n",
       "      <th>healthInsTotal</th>\n",
       "      <th>healthInsNativeWith</th>\n",
       "      <th>healthInsForeignNatWith</th>\n",
       "      <th>healthInsForeignNoncitWith</th>\n",
       "      <th>healthInsForeignNatNo</th>\n",
       "      <th>healthInsForeignNoncitNo</th>\n",
       "      <th>healthInsNativeNo</th>\n",
       "      <th>countyStateName</th>\n",
       "      <th>stateFip</th>\n",
       "      <th>countyFip</th>\n",
       "      <th>pm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.659722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.590139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.326389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.732639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.541667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.537708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.520417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.770833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.395833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.642708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.458333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.197917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.548944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.705556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dates  JHU_ConfirmedCases.data  NYT_ConfirmedCases.data  \\\n",
       "0  2020-03-19                      1.0                      1.0   \n",
       "1  2020-03-20                      1.0                      1.0   \n",
       "2  2020-03-21                      1.0                      1.0   \n",
       "3  2020-03-22                      1.0                      1.0   \n",
       "4  2020-03-23                      1.0                      1.0   \n",
       "\n",
       "   JHU_ConfirmedDeaths.data  NYT_ConfirmedDeaths.data  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   NYT_ConfirmedDeaths.missing  TotalPopulation.data  \\\n",
       "0                            0               24527.0   \n",
       "1                            0               24527.0   \n",
       "2                            0               24527.0   \n",
       "3                            0               24527.0   \n",
       "4                            0               24527.0   \n",
       "\n",
       "   MaleAndFemale_AtLeast65_Population.data  Male_Total_Population.data  \\\n",
       "0                                   5343.0                     11868.0   \n",
       "1                                   5343.0                     11868.0   \n",
       "2                                   5343.0                     11868.0   \n",
       "3                                   5343.0                     11868.0   \n",
       "4                                   5343.0                     11868.0   \n",
       "\n",
       "   Female_Total_Population.data  MaleAndFemale_Under18_Population.data  \\\n",
       "0                       12673.0                                 4924.0   \n",
       "1                       12673.0                                 4924.0   \n",
       "2                       12673.0                                 4924.0   \n",
       "3                       12673.0                                 4924.0   \n",
       "4                       12673.0                                 4924.0   \n",
       "\n",
       "   BLS_EmployedPopulation.data  BLS_EmployedPopulation.missing  \\\n",
       "0                       9716.5                             0.0   \n",
       "1                       9716.5                             0.0   \n",
       "2                       9716.5                             0.0   \n",
       "3                       9716.5                             0.0   \n",
       "4                       9716.5                             0.0   \n",
       "\n",
       "   BLS_UnemployedPopulation.data  BLS_UnemployedPopulation.missing  \\\n",
       "0                          386.5                               0.0   \n",
       "1                          386.5                               0.0   \n",
       "2                          386.5                               0.0   \n",
       "3                          386.5                               0.0   \n",
       "4                          386.5                               0.0   \n",
       "\n",
       "   BLS_UnemploymentRate.data  BLS_UnemploymentRate.missing  \\\n",
       "0                   3.825596                           0.0   \n",
       "1                   3.825596                           0.0   \n",
       "2                   3.825596                           0.0   \n",
       "3                   3.825596                           0.0   \n",
       "4                   3.825596                           0.0   \n",
       "\n",
       "   BLS_LaborForcePopulation.data  BLS_LaborForcePopulation.missing  \\\n",
       "0                        10103.0                               0.0   \n",
       "1                        10103.0                               0.0   \n",
       "2                        10103.0                               0.0   \n",
       "3                        10103.0                               0.0   \n",
       "4                        10103.0                               0.0   \n",
       "\n",
       "   AverageDailyTemperature.data  AverageDailyTemperature.missing  \\\n",
       "0                     68.659722                              0.0   \n",
       "1                     72.541667                              0.0   \n",
       "2                     69.208333                              0.0   \n",
       "3                     53.916667                              0.0   \n",
       "4                     49.863889                              0.0   \n",
       "\n",
       "   AverageDewPoint.data  AverageDewPoint.missing  \\\n",
       "0             63.152778                      0.0   \n",
       "1             60.583333                      0.0   \n",
       "2             56.708333                      0.0   \n",
       "3             49.770833                      0.0   \n",
       "4             49.863889                      0.0   \n",
       "\n",
       "   AverageRelativeHumidity.data  AverageRelativeHumidity.missing  \\\n",
       "0                     84.152778                              0.0   \n",
       "1                     68.708333                              0.0   \n",
       "2                     66.583333                              0.0   \n",
       "3                     86.395833                              0.0   \n",
       "4                    100.000000                              0.0   \n",
       "\n",
       "   AverageSurfaceAirPressure.data  AverageSurfaceAirPressure.missing  \\\n",
       "0                       29.590139                                0.0   \n",
       "1                       29.537708                                0.0   \n",
       "2                       29.520417                                0.0   \n",
       "3                       29.642708                                0.0   \n",
       "4                       29.548944                                0.0   \n",
       "\n",
       "   AveragePrecipitationTotal.data  AveragePrecipitationTotal.missing  \\\n",
       "0                        0.000833                                0.0   \n",
       "1                        0.000000                                0.0   \n",
       "2                        0.000000                                0.0   \n",
       "3                        0.000000                                0.0   \n",
       "4                        0.013750                                0.0   \n",
       "\n",
       "   AveragePrecipitation.data  AveragePrecipitation.missing  \\\n",
       "0                   0.000000                           0.0   \n",
       "1                   0.000000                           0.0   \n",
       "2                   0.000000                           0.0   \n",
       "3                   0.011354                           0.0   \n",
       "4                   0.008042                           0.0   \n",
       "\n",
       "   AverageWindDirection.data  AverageWindDirection.missing  \\\n",
       "0                 192.326389                           0.0   \n",
       "1                 208.125000                           0.0   \n",
       "2                 252.916667                           0.0   \n",
       "3                  91.458333                           0.0   \n",
       "4                  83.333333                           0.0   \n",
       "\n",
       "   AverageWindSpeed.data  AverageWindSpeed.missing  hospitalIcuBeds  \\\n",
       "0               6.732639                       0.0              6.0   \n",
       "1              10.833333                       0.0              6.0   \n",
       "2               8.125000                       0.0              6.0   \n",
       "3               7.197917                       0.0              6.0   \n",
       "4               3.705556                       0.0              6.0   \n",
       "\n",
       "   hospitalStaffedBeds  hospitalLicensedBeds  latestTotalPopulation   fips  \\\n",
       "0                 25.0                  25.0                24527.0  45001   \n",
       "1                 25.0                  25.0                24527.0  45001   \n",
       "2                 25.0                  25.0                24527.0  45001   \n",
       "3                 25.0                  25.0                24527.0  45001   \n",
       "4                 25.0                  25.0                24527.0  45001   \n",
       "\n",
       "                                 county  jhu_daily_death  jhu_daily_cases  \\\n",
       "0  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "1  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "2  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "3  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "4  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "\n",
       "   jhu_daily_new_cases  jhu_daily_death_rolling_7  jhu_daily_cases_rolling_7  \\\n",
       "0                  0.0                        0.0                        0.0   \n",
       "1                  0.0                        0.0                        0.0   \n",
       "2                  0.0                        0.0                        0.0   \n",
       "3                  0.0                        0.0                        0.0   \n",
       "4                  0.0                        0.0                        0.0   \n",
       "\n",
       "   jhu_daily_new_cases_rolling_7  jhu_daily_death_rolling_30  \\\n",
       "0                            0.0                         0.0   \n",
       "1                            0.0                         0.0   \n",
       "2                            0.0                         0.0   \n",
       "3                            0.0                         0.0   \n",
       "4                            0.0                         0.0   \n",
       "\n",
       "   jhu_daily_cases_rolling_30  jhu_daily_new_cases_rolling_30  LND110210  \\\n",
       "0                         0.0                             0.0     490.48   \n",
       "1                         0.0                             0.0     490.48   \n",
       "2                         0.0                             0.0     490.48   \n",
       "3                         0.0                             0.0     490.48   \n",
       "4                         0.0                             0.0     490.48   \n",
       "\n",
       "   jhu_death_rate  jhu_case_rate  jhu_new_case_rate    density  \\\n",
       "0             0.0            0.0                0.0  50.006116   \n",
       "1             0.0            0.0                0.0  50.006116   \n",
       "2             0.0            0.0                0.0  50.006116   \n",
       "3             0.0            0.0                0.0  50.006116   \n",
       "4             0.0            0.0                0.0  50.006116   \n",
       "\n",
       "   icu_beds_per_person  staffed_beds_per_person  licensed_beds_per_person  \\\n",
       "0             0.000245                 0.001019                  0.001019   \n",
       "1             0.000245                 0.001019                  0.001019   \n",
       "2             0.000245                 0.001019                  0.001019   \n",
       "3             0.000245                 0.001019                  0.001019   \n",
       "4             0.000245                 0.001019                  0.001019   \n",
       "\n",
       "   cold_days  hot_days  moderate_days  gte_65_percent  lt_18_percent  \\\n",
       "0          0         0              1        0.217842       0.200758   \n",
       "1          0         0              1        0.217842       0.200758   \n",
       "2          0         0              1        0.217842       0.200758   \n",
       "3          0         0              1        0.217842       0.200758   \n",
       "4          1         0              0        0.217842       0.200758   \n",
       "\n",
       "   employed_percent  unemployed_percent  totalMoved  movedWithinState  \\\n",
       "0          0.961744            0.038256     24317.0            1118.0   \n",
       "1          0.961744            0.038256     24317.0            1118.0   \n",
       "2          0.961744            0.038256     24317.0            1118.0   \n",
       "3          0.961744            0.038256     24317.0            1118.0   \n",
       "4          0.961744            0.038256     24317.0            1118.0   \n",
       "\n",
       "   movedWithoutState  movedFromAbroad  publicTrans  totalTrans  \\\n",
       "0              329.0             12.0           44        9791   \n",
       "1              329.0             12.0           44        9791   \n",
       "2              329.0             12.0           44        9791   \n",
       "3              329.0             12.0           44        9791   \n",
       "4              329.0             12.0           44        9791   \n",
       "\n",
       "   householdsTotal  houseWith65  house2+with65  houseFamily65  houseNonfam65  \\\n",
       "0             9660         3645           2081           2042             39   \n",
       "1             9660         3645           2081           2042             39   \n",
       "2             9660         3645           2081           2042             39   \n",
       "3             9660         3645           2081           2042             39   \n",
       "4             9660         3645           2081           2042             39   \n",
       "\n",
       "   houseNo65  house2+No65  houseFamilyNo65  houseNonfamNo65  \\\n",
       "0       6015         4531             4206              325   \n",
       "1       6015         4531             4206              325   \n",
       "2       6015         4531             4206              325   \n",
       "3       6015         4531             4206              325   \n",
       "4       6015         4531             4206              325   \n",
       "\n",
       "   householdStructuresTotal  householdIncomeMedian   gini  hoursWorkedMean  \\\n",
       "0                      9660                  38741  0.479             38.1   \n",
       "1                      9660                  38741  0.479             38.1   \n",
       "2                      9660                  38741  0.479             38.1   \n",
       "3                      9660                  38741  0.479             38.1   \n",
       "4                      9660                  38741  0.479             38.1   \n",
       "\n",
       "   unitsInStructure  healthInsTotal  healthInsNativeWith  \\\n",
       "0             12191           24397                21415   \n",
       "1             12191           24397                21415   \n",
       "2             12191           24397                21415   \n",
       "3             12191           24397                21415   \n",
       "4             12191           24397                21415   \n",
       "\n",
       "   healthInsForeignNatWith  healthInsForeignNoncitWith  healthInsForeignNatNo  \\\n",
       "0                      179                          87                     39   \n",
       "1                      179                          87                     39   \n",
       "2                      179                          87                     39   \n",
       "3                      179                          87                     39   \n",
       "4                      179                          87                     39   \n",
       "\n",
       "   healthInsForeignNoncitNo  healthInsNativeNo  \\\n",
       "0                       122               2555   \n",
       "1                       122               2555   \n",
       "2                       122               2555   \n",
       "3                       122               2555   \n",
       "4                       122               2555   \n",
       "\n",
       "                    countyStateName stateFip countyFip      pm25  \n",
       "0  Abbeville County, South Carolina       45       001  9.618551  \n",
       "1  Abbeville County, South Carolina       45       001  9.618551  \n",
       "2  Abbeville County, South Carolina       45       001  9.618551  \n",
       "3  Abbeville County, South Carolina       45       001  9.618551  \n",
       "4  Abbeville County, South Carolina       45       001  9.618551  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linux version (problems with bz2 file)\n",
    "\n",
    "golden_dataset_file_name = os.path.join('/', 'data', 'projects', 'drexel', 'dsci592', 'feeFiFoFum.pkl')\n",
    "\n",
    "df = pd.read_pickle(golden_dataset_file_name)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fde0d-13e5-4198-9f8e-5689f1130301",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "#### Drop non-numeric and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e761798-4dba-4c6c-96db-04ba85fb6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['NYT_ConfirmedCases.data','NYT_ConfirmedDeaths.data','NYT_ConfirmedDeaths.missing','county','LND110210','countyStateName','stateFip','countyFip']\n",
    "\n",
    "df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf9d9-f105-4376-80ce-65a15ad0bca0",
   "metadata": {},
   "source": [
    "#### Temporarily, replace FIPS codes with latitude and longitude of the centroid of each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d613f0d-2b87-4c84-80d1-a34cb680909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = pd.read_csv('2021_Gaz_counties_national.txt', delimiter='\\t')\n",
    "counties.rename(columns={'INTPTLONG                                                                                                               ': 'longitude',\n",
    "                        'INTPTLAT': 'latitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cce69e-9e11-4ebf-ad6a-7257ea693173",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = counties[['GEOID', 'latitude', 'longitude' ]]\n",
    "df.fips = df.fips.astype('int64')\n",
    "\n",
    "df = df.merge(counties, how='left', left_on='fips', right_on='GEOID')\n",
    "df.drop(['GEOID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324af6e-050d-4195-b5d9-4c9d51dac540",
   "metadata": {},
   "source": [
    "#### Replace dates with monotonically increasing integers starting with the minimum date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d464b3-5904-4671-9ef8-97b2450bfc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dates = pd.to_datetime(df.dates, format='%Y-%m-%d')\n",
    "min_date = min(df.dates)\n",
    "max_date = max(df.dates)\n",
    "min_date, max_date, df.dates.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a52c51-f7f0-4835-a57e-8ec4298e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] =(df.dates - min_date).dt.days\n",
    "df.drop(['dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0581ee-2583-4748-8a3c-e4ccadc6a3ba",
   "metadata": {},
   "source": [
    "#### Replace the integer representation of date with sin and cosine encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517112d-13a1-4685-9f96-22e5ece969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclical_interval = 365\n",
    "continuous_interval = 3650\n",
    "df['cyclical_sin'] = np.sin((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['cyclical_cos'] = np.cos((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['continuous_sin'] = np.sin((df.day * 2 * np.pi)/continuous_interval)\n",
    "df['continuous_cos'] = np.cos((df.day * 2 * np.pi)/continuous_interval)\n",
    "df.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc592ca-b282-47f6-a26c-da7033ac9194",
   "metadata": {},
   "source": [
    "#### Get the feature column for latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743a233-aa53-4a8d-a047-4f05805fac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossed_latlong = pp.get_latlong_fc(df)\n",
    "\n",
    "lat_buckets = list(np.linspace(df.latitude.min(), df.latitude.max(),100))\n",
    "long_buckets = list(np.linspace(df.longitude.min(), df.longitude.max(),100))\n",
    "\n",
    "#make feature columns\n",
    "lat_fc = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'),lat_buckets)\n",
    "long_fc= tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'),long_buckets)\n",
    "    \n",
    "# crossed columns tell the model how the features relate\n",
    "crossed_latlong = tf.feature_column.crossed_column(keys=[lat_fc, long_fc], hash_bucket_size=1000) # No precise rule, maybe 1000 buckets will be good?\n",
    "    \n",
    "embedded_latlong = tf.feature_column.embedding_column(crossed_latlong,9)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(embedded_latlong)\n",
    "\n",
    "df[['geo0', 'geo1', 'geo2','geo3', 'geo4','geo5','geo6','geo7','geo8']] = feature_layer({'latitude': df.latitude, 'longitude': df.longitude})\n",
    "\n",
    "# df.drop(['longitude', 'latitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ad6ca-75b4-4be9-84d7-5deddf14bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f842b-c049-404a-bb19-55f26e8efa25",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513da66f-15f7-4907-bf2d-f136a335f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = [\n",
    "       'TotalPopulation.data', 'MaleAndFemale_AtLeast65_Population.data',\n",
    "       'Male_Total_Population.data', 'Female_Total_Population.data',\n",
    "       'MaleAndFemale_Under18_Population.data', 'BLS_EmployedPopulation.data',\n",
    "       'BLS_EmployedPopulation.missing', 'BLS_UnemployedPopulation.data',\n",
    "       'BLS_UnemployedPopulation.missing', 'BLS_UnemploymentRate.data',\n",
    "       'BLS_UnemploymentRate.missing', 'BLS_LaborForcePopulation.data',\n",
    "       'BLS_LaborForcePopulation.missing', 'AverageDailyTemperature.data',\n",
    "       'AverageDailyTemperature.missing', 'AverageDewPoint.data',\n",
    "       'AverageDewPoint.missing', 'AverageRelativeHumidity.data',\n",
    "       'AverageRelativeHumidity.missing', 'AverageSurfaceAirPressure.data',\n",
    "       'AverageSurfaceAirPressure.missing', 'AveragePrecipitationTotal.data',\n",
    "       'AveragePrecipitationTotal.missing', 'AveragePrecipitation.data',\n",
    "       'AveragePrecipitation.missing', 'AverageWindDirection.data',\n",
    "       'AverageWindDirection.missing', 'AverageWindSpeed.data',\n",
    "       'AverageWindSpeed.missing', 'hospitalIcuBeds', 'hospitalStaffedBeds',\n",
    "       'hospitalLicensedBeds', 'latestTotalPopulation', 'jhu_daily_death',\n",
    "       'jhu_daily_cases', 'jhu_daily_new_cases', \n",
    "    'jhu_daily_death_rolling_7',\n",
    "       'jhu_daily_cases_rolling_7', 'jhu_daily_new_cases_rolling_7',\n",
    "       'jhu_daily_death_rolling_30', 'jhu_daily_cases_rolling_30',\n",
    "       'jhu_daily_new_cases_rolling_30', 'jhu_death_rate', 'jhu_case_rate',\n",
    "       'jhu_new_case_rate', 'density', 'icu_beds_per_person',\n",
    "       'staffed_beds_per_person', 'licensed_beds_per_person', 'cold_days',\n",
    "       'hot_days', 'moderate_days', 'gte_65_percent', 'lt_18_percent',\n",
    "       'employed_percent', 'unemployed_percent', 'totalMoved',\n",
    "       'movedWithinState', 'movedWithoutState', 'movedFromAbroad',\n",
    "       'publicTrans', 'totalTrans', 'householdsTotal', 'houseWith65',\n",
    "       'house2+with65', 'houseFamily65', 'houseNonfam65', 'houseNo65',\n",
    "       'house2+No65', 'houseFamilyNo65', 'houseNonfamNo65',\n",
    "       'householdStructuresTotal', 'householdIncomeMedian', 'gini',\n",
    "       'hoursWorkedMean', 'unitsInStructure', 'healthInsTotal',\n",
    "       'healthInsNativeWith', 'healthInsForeignNatWith',\n",
    "       'healthInsForeignNoncitWith', 'healthInsForeignNatNo',\n",
    "       'healthInsForeignNoncitNo', 'healthInsNativeNo', 'pm25', 'latitude',\n",
    "       'longitude']\n",
    "cols_raw = ['fips','JHU_ConfirmedCases.data', 'JHU_ConfirmedDeaths.data', 'cyclical_sin', 'cyclical_cos', 'continuous_sin',\n",
    "       'continuous_cos', 'geo0', 'geo1', 'geo2','geo3', 'geo4','geo5','geo6','geo7','geo8']\n",
    "df_normalized = df[cols_to_normalize]\n",
    "df_normalized = (df_normalized - df_normalized.mean())/df_normalized.std()\n",
    "df_raw = df[cols_raw]\n",
    "df = pd.concat([df_raw, df_normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bf3ef-a6d5-4f22-b336-f46f2798a363",
   "metadata": {},
   "source": [
    "#### Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7731216-cef5-4d0a-a9d2-51dfbe00172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_history = 30\n",
    "days_to_predict = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3c1f6-ec25-4420-8f70-b8d7f0cef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = df.fips.unique()\n",
    "\n",
    "# def x_generator(data, days_of_history=30, days_to_predict=1):\n",
    "#     for j, fip in enumerate(fips):\n",
    "#         if not j % 100: print(j, end=' ')\n",
    "#         county = data[data.fips == fip]\n",
    "#         for i in range(days_of_history, len(county) - days_to_predict):\n",
    "#             data_matrix = data.iloc[i - days_of_history: i, 1:].to_numpy()\n",
    "#             yield data_matrix\n",
    "            \n",
    "# def y_generator(data, days_of_history=30, days_to_predict=1):\n",
    "#     for fip in fips:\n",
    "#         county = data[data.fips == fip]\n",
    "#         for i in range(days_of_history, len(county) - days_to_predict):\n",
    "#             data_matrix = data.iloc[i: i + days_to_predict, 1:3].to_numpy()\n",
    "#             yield data_matrix\n",
    "    \n",
    "def xy_generator(data, days=31):\n",
    "    for j, fip in enumerate(fips):\n",
    "        if not j % 100: print(j, end=' ')\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days, len(county) + 1):\n",
    "            data_matrix = data.iloc[i - days: i, 1:].to_numpy()\n",
    "            yield data_matrix\n",
    "            \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac163c8-45f9-4a5c-972d-babb0d65816a",
   "metadata": {},
   "source": [
    "##### Save the raw X and Y to files of 50,000 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32add75b-d9ef-432b-a607-6fe85948c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = []\n",
    "j = 0\n",
    "\n",
    "N_SAMPLES = 200\n",
    "\n",
    "for i, x in enumerate(xy_generator(df)):\n",
    "    Xi.append(x)\n",
    "    if i and not i % (N_SAMPLES - 1):\n",
    "        X = np.asarray(Xi)\n",
    "        np.save(os.path.join('.','data', f'x_{j}.npy'), X)\n",
    "        j += 1\n",
    "        Xi = []\n",
    "if Xi:\n",
    "    X = np.asarray(Xi)\n",
    "    np.save(os.path.join('.','data', f'x_{j}.npy'), X)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac1bc8-469d-4412-a243-74e5113661c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('./data/x_0.npy')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664610a-0086-4410-b237-4ae731bce0d7",
   "metadata": {},
   "source": [
    "##### Split into train, test, eval directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabce3f5-5188-408b-8bb1-711fdb8af856",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "def set_seed():\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee865b7d-1f2b-4c1e-b24b-ee56a59ca907",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_files = glob('./data/x_*.npy')\n",
    "random.shuffle(x_files)\n",
    "n_files = len(x_files)\n",
    "print(n_files)\n",
    "n_train = int(n_files * 0.70)\n",
    "print(n_train)\n",
    "n_eval = int(n_files * 0.15)\n",
    "print(n_eval)\n",
    "n_test = n_files - n_train - n_eval\n",
    "print(n_test)\n",
    "train_files = x_files[:n_train]\n",
    "# print(len(train_files))\n",
    "eval_files = x_files[n_train:n_train+n_test]\n",
    "# print(len(eval_files))\n",
    "test_files = x_files[n_train+n_test:]\n",
    "assert n_files == len(train_files) + len(eval_files) + len(test_files)\n",
    "for (subdir, lst) in [['train', train_files], ['eval', eval_files], ['test', test_files]]:\n",
    "    for file in lst:\n",
    "        shutil.move(file, os.path.join('.', 'data', subdir))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72038235-dbcf-47e3-8a70-7b8021b68f7f",
   "metadata": {},
   "source": [
    "##### Create the Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c917f4-f604-48cf-a8dd-d07a1fffed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob('./data/train/x_*.npy')\n",
    "eval_files = glob('./data/eval/x_*.npy')\n",
    "test_files = glob('./data/test/x_*.npy')\n",
    "\n",
    "n_readers = 5\n",
    "n_parse_threads = 5\n",
    "len_array = 995\n",
    "\n",
    "def create_generator(files, cycle_length=5):\n",
    "    set_seed()\n",
    "    random.shuffle(files)\n",
    "    for i in range(0, len(files), cycle_length):\n",
    "        subset = files[i:i+cycle_length]\n",
    "        np_arrays = [np.load(s) for s in subset]\n",
    "        np_array = np.concatenate(np_arrays, axis=0)\n",
    "        np.random.shuffle(np_array)\n",
    "        yield np_array\n",
    "            \n",
    "\n",
    "def split_xy(np_array):\n",
    "    X = np_array[:,:-1,:]\n",
    "    y = np_array[:,-1:,:1]\n",
    "    return X,y\n",
    "        \n",
    "    \n",
    "train_ds = tf.data.Dataset.from_generator(lambda: create_generator(train_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "train_ds = train_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(lambda: create_generator(eval_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "val_ds = val_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(lambda: create_generator(test_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "test_ds = test_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e1b39-b6af-4662-9975-060c06c987ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in train_ds.take(3):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaea1d7-a885-4ca5-9f52-83846c1bd2df",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351ce58-2517-4ec3-9a7f-36e819e7f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'recurrent_layer_type': [keras.layers.LSTM, keras.layers.GRU],\n",
    "    'n_recurrent': [1, 2, 3],\n",
    "    'unit': [32, 64, 128],\n",
    "    'dropout': [0.0, 0.10, 0.20],\n",
    "    'lr': [1e-2, 1e-3],   \n",
    "    'epochs': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc01b2-659b-4d10-b0ba-a2ae967e58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(param, shape=(30,101)):\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(param['n_recurrent']):\n",
    "        last = param['recurrent_layer_type'](param['unit'])(last)\n",
    "        if param['dropout']:\n",
    "            last = keras.layers.Dropout(param['dropout'])(last)\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=param['lr']),  loss='mae',  metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2195ed-28cc-4565-9d05-31d855c1ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "n_trial = 3\n",
    "\n",
    "name = f'trial_{n_trial}'\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(f'./data/model/{name}_{timestamp}.h5', save_best_only=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(name))\n",
    "\n",
    "\n",
    "hypermodel = lambda param: build_model(param=param)\n",
    "\n",
    "kgs = KerasGridSearch(hypermodel, param_grid, monitor='val_loss', greater_is_better=False, tuner_verbose=1)\n",
    "kgs.search(train_ds, validation_data=val_ds, callbacks=[early_stopping_cb, checkpoint_cb, tensorboard], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea660af-bf47-4c55-a9ae-989a7d972e45",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning \n",
    "\n",
    "Using technique from https://medium.com/ml-book/neural-networks-hyperparameter-tuning-in-tensorflow-2-0-a7b4e2b574a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1978f67-3294-44bf-b79e-007a6645b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 08:59:09.245948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.246617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.285750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.285992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.286196: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.286403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.286966: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-23 08:59:09.424768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.424992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.425180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.425348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.425514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.425688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.946557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.946804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.946988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.947161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.947338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.947517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2937 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2022-02-23 08:59:09.947717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 08:59:09.947871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 3743 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([1, 2, 3]))\n",
    "# HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([3]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([32, 64, 128]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-2]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning_full').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb81ad5-9d75-4725-ac82-5d514fcc5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=16\n",
    "\n",
    "def train_test_model(hparams, shape=(30,100)):\n",
    "    # set_seed()\n",
    "    train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "    \n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            batch_size=128,\n",
    "            epochs=EPOCHS)\n",
    "    print(model.summary())\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcc69d-4d33-472c-a9b4-11eff17d74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "      1/Unknown - 2s 2s/step - loss: 8392.6074"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 08:59:21.597683: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6911.4292 - val_loss: 6723.3223\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 27s 22ms/step - loss: 6685.3008 - val_loss: 6530.3672\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 6493.8950 - val_loss: 6342.6016\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 6343.5845 - val_loss: 6192.4019\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 27s 22ms/step - loss: 6255.3521 - val_loss: 6203.6484\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6231.6177 - val_loss: 6096.7539\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6164.8921 - val_loss: 6085.1582\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 27s 22ms/step - loss: 6142.7354 - val_loss: 6069.8369\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6169.3828 - val_loss: 6085.0615\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 6113.9048 - val_loss: 5969.0767\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6004.5107 - val_loss: 5877.4175\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5906.8657 - val_loss: 5785.9136\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5824.1431 - val_loss: 5693.3784\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5839.5093 - val_loss: 5718.2349\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5773.6318 - val_loss: 5629.8169\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 5703.7349 - val_loss: 5592.1777\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                12864     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,897\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 20ms/step - loss: 6781.2651\n",
      "--- Starting trial: run-1\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6789.9219 - val_loss: 6525.9351\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6430.8057 - val_loss: 6262.3799\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6302.4097 - val_loss: 6183.1777\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6177.9653 - val_loss: 6077.1489\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6077.1328 - val_loss: 6025.7544\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 27s 22ms/step - loss: 6031.9253 - val_loss: 5958.0703\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5946.0571 - val_loss: 5764.3369\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5807.7202 - val_loss: 5801.2920\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5832.8550 - val_loss: 5662.1343\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5740.3003 - val_loss: 5608.0020\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5721.9927 - val_loss: 5562.8491\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5652.1909 - val_loss: 5551.9282\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5609.2676 - val_loss: 5461.5557\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5506.3770 - val_loss: 5436.4282\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5579.0010 - val_loss: 5462.7964\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 27s 22ms/step - loss: 5512.3027 - val_loss: 5343.2676\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                31872     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,937\n",
      "Trainable params: 31,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 21ms/step - loss: 6523.8989\n",
      "--- Starting trial: run-2\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6606.2305 - val_loss: 6268.9268\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6185.6362 - val_loss: 5974.8203\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6038.7715 - val_loss: 5888.5444\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5966.0605 - val_loss: 5930.1372\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5906.7725 - val_loss: 5807.2466\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5859.6860 - val_loss: 5762.3667\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5806.1797 - val_loss: 5638.7476\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5694.2100 - val_loss: 5602.2065\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5665.2915 - val_loss: 5518.0903\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5550.9844 - val_loss: 5506.3838\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5483.9473 - val_loss: 5344.6235\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5478.3013 - val_loss: 5526.7573\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5514.0386 - val_loss: 5335.9346\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5428.9414 - val_loss: 5329.0635\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5385.0151 - val_loss: 5232.0024\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5335.8481 - val_loss: 5239.5078\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 128)               88320     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,449\n",
      "Trainable params: 88,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 24ms/step - loss: 6411.8818\n",
      "--- Starting trial: run-3\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 32s 24ms/step - loss: 6911.7148 - val_loss: 6723.3623\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6653.3369 - val_loss: 6496.0732\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6446.1899 - val_loss: 6344.4688\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6281.4385 - val_loss: 6153.8916\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6130.2798 - val_loss: 6010.0522\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6066.6235 - val_loss: 5975.6729\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5984.0566 - val_loss: 6196.0635\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5954.2905 - val_loss: 5832.0029\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5906.0381 - val_loss: 6003.0088\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5809.8608 - val_loss: 5739.8379\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5965.5156 - val_loss: 5794.8350\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5903.4683 - val_loss: 6285.3188\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5849.8271 - val_loss: 6205.2935\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6184.0718 - val_loss: 6232.0200\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6003.2964 - val_loss: 6636.9893\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6137.1118 - val_loss: 6612.4497\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 30, 32)            12864     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 30, 32)            0         \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 32)                6336      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,233\n",
      "Trainable params: 19,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 23ms/step - loss: 7840.6748\n",
      "--- Starting trial: run-4\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 32s 24ms/step - loss: 6783.0430 - val_loss: 6497.8364\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6378.7881 - val_loss: 6224.6118\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6170.2593 - val_loss: 6030.1616\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6009.1270 - val_loss: 6074.7197\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5844.6968 - val_loss: 5713.0864\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5664.1865 - val_loss: 6463.3203\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5582.3574 - val_loss: 6040.0366\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5537.2271 - val_loss: 5832.8384\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5387.6787 - val_loss: 5396.2988\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5355.3018 - val_loss: 5354.9360\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5510.9438 - val_loss: 5685.0249\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5675.0127 - val_loss: 5551.1235\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5780.3267 - val_loss: 5397.0571\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6032.5498 - val_loss: 6660.0571\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5658.4805 - val_loss: 6013.6235\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5424.7739 - val_loss: 5323.6182\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 30, 64)            31872     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 64)                24960     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56,897\n",
      "Trainable params: 56,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 24ms/step - loss: 6504.1870\n",
      "--- Starting trial: run-5\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 41s 31ms/step - loss: 6578.8789 - val_loss: 6184.6895\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 6062.0259 - val_loss: 5845.1890\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5928.2637 - val_loss: 5816.8525\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5776.7749 - val_loss: 5651.8296\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5677.1860 - val_loss: 5659.4595\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5709.8228 - val_loss: 5762.9438\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5745.0332 - val_loss: 5887.0933\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5884.3960 - val_loss: 6563.6919\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5936.3662 - val_loss: 5848.5288\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5499.0459 - val_loss: 5728.5020\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5403.6680 - val_loss: 5186.5347\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5355.8032 - val_loss: 6367.1279\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5385.2612 - val_loss: 5183.4810\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5697.1475 - val_loss: 6562.4849\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5684.9175 - val_loss: 6254.3345\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 39s 31ms/step - loss: 5777.5161 - val_loss: 5951.1235\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 30, 128)           88320     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 187,521\n",
      "Trainable params: 187,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 23ms/step - loss: 7138.1660\n",
      "--- Starting trial: run-6\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 33s 24ms/step - loss: 6916.2812 - val_loss: 6724.4995\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 6650.4038 - val_loss: 6494.3057\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6458.3950 - val_loss: 6363.9648\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 6300.2642 - val_loss: 6217.8262\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6157.7021 - val_loss: 6040.6411\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 6022.0903 - val_loss: 5925.9912\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 6015.8262 - val_loss: 6026.7134\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5864.9082 - val_loss: 5774.6943\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 5790.5288 - val_loss: 6152.3154\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5713.5225 - val_loss: 5656.7896\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 5642.6118 - val_loss: 5551.5454\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 5632.9541 - val_loss: 6271.6299\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5572.4263 - val_loss: 6214.7573\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5690.9556 - val_loss: 5515.2349\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5481.4478 - val_loss: 5594.0361\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5421.3408 - val_loss: 5449.4893\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_9 (GRU)                 (None, 30, 32)            12864     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 30, 32)            0         \n",
      "                                                                 \n",
      " gru_10 (GRU)                (None, 30, 32)            6336      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 30, 32)            0         \n",
      "                                                                 \n",
      " gru_11 (GRU)                (None, 32)                6336      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,569\n",
      "Trainable params: 25,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 23ms/step - loss: 6631.8350\n",
      "--- Starting trial: run-7\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 35s 26ms/step - loss: 6783.8809 - val_loss: 6507.7568\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 6409.4551 - val_loss: 6243.5894\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 6139.7412 - val_loss: 6017.7544\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5943.6616 - val_loss: 5776.7773\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5802.8379 - val_loss: 5671.9160\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5864.5918 - val_loss: 5631.1963\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 31s 24ms/step - loss: 5608.7314 - val_loss: 5595.6519\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5874.6597 - val_loss: 5758.0054\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 31s 25ms/step - loss: 5633.5171 - val_loss: 5671.8281\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5523.2554 - val_loss: 5483.2896\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5549.6128 - val_loss: 5794.3892\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 31s 24ms/step - loss: 5405.4785 - val_loss: 5445.5840\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 31s 25ms/step - loss: 5282.2676 - val_loss: 5533.8008\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5660.4805 - val_loss: 6023.7212\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 32s 25ms/step - loss: 5834.8994 - val_loss: 6143.8374\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 31s 25ms/step - loss: 5630.3022 - val_loss: 5452.4980\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 30, 64)            31872     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 30, 64)            24960     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 64)                24960     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,857\n",
      "Trainable params: 81,857\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 7s 24ms/step - loss: 6628.1543\n",
      "--- Starting trial: run-8\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 54s 41ms/step - loss: 6582.9336 - val_loss: 6326.4683\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 6057.6030 - val_loss: 5914.9360\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5778.7812 - val_loss: 5815.1299\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5574.6978 - val_loss: 5419.3872\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5412.5112 - val_loss: 5371.6040\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5440.0259 - val_loss: 5483.0073\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5445.9785 - val_loss: 5243.1025\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5439.8423 - val_loss: 6729.8369\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5810.5181 - val_loss: 5518.8511\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5385.7314 - val_loss: 6752.7461\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5477.7427 - val_loss: 5452.6577\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5493.7119 - val_loss: 6757.4619\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5480.6958 - val_loss: 5388.3350\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5394.9019 - val_loss: 5569.7632\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5779.6440 - val_loss: 6321.1133\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 51s 40ms/step - loss: 5083.9692 - val_loss: 5631.9111\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_15 (GRU)                (None, 30, 128)           88320     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_16 (GRU)                (None, 30, 128)           99072     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_17 (GRU)                (None, 128)               99072     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 286,593\n",
      "Trainable params: 286,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 7s 25ms/step - loss: 6811.9282\n",
      "--- Starting trial: run-9\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 31s 24ms/step - loss: 6938.8296 - val_loss: 6767.2363\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 6716.5186 - val_loss: 6593.0229\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6558.2109 - val_loss: 6430.3184\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6415.1226 - val_loss: 6315.9624\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6325.7524 - val_loss: 6247.6021\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 6213.3853 - val_loss: 6084.2041\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6100.1973 - val_loss: 6041.0337\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6102.3716 - val_loss: 5986.3755\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6087.4336 - val_loss: 5973.4048\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 6052.8311 - val_loss: 5941.3403\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5970.2090 - val_loss: 5871.2500\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5928.4956 - val_loss: 5870.1421\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5932.2969 - val_loss: 5858.1528\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 5979.6455 - val_loss: 6012.5713\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 27s 21ms/step - loss: 5986.5430 - val_loss: 5899.2939\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5885.6060 - val_loss: 5859.1362\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                17024     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,057\n",
      "Trainable params: 17,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 23ms/step - loss: 7062.1221\n",
      "--- Starting trial: run-10\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 32s 24ms/step - loss: 6831.9858 - val_loss: 6616.6548\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 27s 22ms/step - loss: 6499.2275 - val_loss: 6315.1660\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6271.9185 - val_loss: 6163.8584\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6126.6338 - val_loss: 5980.7764\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6007.3223 - val_loss: 5912.8916\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5902.8149 - val_loss: 5822.8345\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5863.5161 - val_loss: 5752.9380\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5858.9136 - val_loss: 5718.8076\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5864.9058 - val_loss: 5931.5693\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5899.5288 - val_loss: 5741.1748\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5867.1421 - val_loss: 5864.1357\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5881.0903 - val_loss: 5838.4971\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5899.3110 - val_loss: 5937.8857\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5899.9907 - val_loss: 5799.1392\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 27s 22ms/step - loss: 5827.7783 - val_loss: 5739.3325\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5805.5425 - val_loss: 5713.6094\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                42240     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,305\n",
      "Trainable params: 42,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 22ms/step - loss: 6909.8818\n",
      "--- Starting trial: run-11\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 31s 24ms/step - loss: 6694.5586 - val_loss: 6391.0854\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6248.2065 - val_loss: 6018.8623\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5984.8252 - val_loss: 5839.2993\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5790.6899 - val_loss: 5702.1548\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5750.1982 - val_loss: 5630.9614\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5715.0635 - val_loss: 5565.9897\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5645.2656 - val_loss: 5613.9453\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5662.6660 - val_loss: 5644.7437\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5665.7661 - val_loss: 5559.2817\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5589.4854 - val_loss: 5493.3779\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5667.8115 - val_loss: 5595.0181\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5689.3379 - val_loss: 5599.9673\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5643.4351 - val_loss: 5563.4873\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5713.6753 - val_loss: 5592.4536\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5714.6934 - val_loss: 5610.3247\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5730.5938 - val_loss: 5711.0732\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,377\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 7s 24ms/step - loss: 6907.1313\n",
      "--- Starting trial: run-12\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 32s 24ms/step - loss: 6911.3774 - val_loss: 6722.8257\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6648.0913 - val_loss: 6485.1733\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6439.3325 - val_loss: 6302.5298\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 6261.5190 - val_loss: 6158.0908\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 6104.6309 - val_loss: 5982.9746\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5984.3730 - val_loss: 5901.5547\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5873.8506 - val_loss: 5774.5532\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5762.8813 - val_loss: 5638.6465\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5684.2866 - val_loss: 5663.3213\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5690.9243 - val_loss: 5616.1899\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5610.7905 - val_loss: 5588.3784\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5739.3784 - val_loss: 5524.6128\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5510.2422 - val_loss: 5350.1631\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5691.5903 - val_loss: 5804.4136\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5927.1860 - val_loss: 5750.4443\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5738.6519 - val_loss: 5527.8398\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 30, 32)            17024     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 30, 32)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,377\n",
      "Trainable params: 25,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 23ms/step - loss: 6691.8560\n",
      "--- Starting trial: run-13\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 32s 24ms/step - loss: 6787.4590 - val_loss: 6522.4092\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6402.8872 - val_loss: 6198.2046\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6113.0098 - val_loss: 5985.6748\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5901.7378 - val_loss: 5763.4663\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5768.9346 - val_loss: 5621.8003\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5634.6567 - val_loss: 5532.4473\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5721.9458 - val_loss: 5705.6626\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5472.4238 - val_loss: 5718.4678\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5479.7271 - val_loss: 5795.1221\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5402.3584 - val_loss: 5282.2534\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5458.9927 - val_loss: 5336.7686\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5834.5684 - val_loss: 6047.5747\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5826.1152 - val_loss: 5439.6646\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5888.8271 - val_loss: 6205.6807\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5903.7661 - val_loss: 5589.5430\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5718.7212 - val_loss: 5431.4780\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 30, 64)            42240     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 75,329\n",
      "Trainable params: 75,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 24ms/step - loss: 6616.1899\n",
      "--- Starting trial: run-14\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 45s 34ms/step - loss: 6641.4990 - val_loss: 6293.8833\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 6183.8120 - val_loss: 5973.6699\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 5867.9609 - val_loss: 5733.0239\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5592.3394 - val_loss: 5429.1665\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5432.3247 - val_loss: 5327.8208\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5273.0513 - val_loss: 5375.9224\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5169.3735 - val_loss: 5043.9951\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 4987.5981 - val_loss: 5037.4307\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5073.4277 - val_loss: 4994.9282\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5023.9839 - val_loss: 4900.9087\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5111.3276 - val_loss: 5091.3301\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 44s 34ms/step - loss: 4921.9468 - val_loss: 4806.7754\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5449.7573 - val_loss: 4935.5903\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5058.1646 - val_loss: 5056.5557\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5707.2812 - val_loss: 5065.4453\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 43s 34ms/step - loss: 5355.3750 - val_loss: 5835.4565\n",
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 30, 128)           117248    \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 248,961\n",
      "Trainable params: 248,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 6s 24ms/step - loss: 7034.9087\n",
      "--- Starting trial: run-15\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 34s 25ms/step - loss: 6913.8999 - val_loss: 6727.3428\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6660.2026 - val_loss: 6509.0908\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 30s 23ms/step - loss: 6453.6367 - val_loss: 6317.3936\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6283.9526 - val_loss: 6160.8213\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6140.5396 - val_loss: 6018.4102\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6018.3179 - val_loss: 5906.6807\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5892.1807 - val_loss: 5798.5117\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 28s 22ms/step - loss: 5821.3154 - val_loss: 5725.9785\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5698.2773 - val_loss: 5586.5864\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5748.7368 - val_loss: 5981.2065\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 6017.8418 - val_loss: 5987.5869\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 29s 23ms/step - loss: 5761.4023 - val_loss: 5613.7710\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 29s 22ms/step - loss: 5868.7651 - val_loss: 5945.1489\n",
      "Epoch 14/16\n",
      " 847/1258 [===================>..........] - ETA: 7s - loss: 5736.7881"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning_full/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd7ef0-b079-4654-844d-c9c1958e07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96591bc2-d3ce-4f32-a0e5-b2a8323c2b35",
   "metadata": {},
   "source": [
    "### Testing bigger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35663ab1-9789-40d4-b701-5cb51b8daccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "# HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([1, 2, 3]))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([3, 4, 5]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([256, 512, 1024]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-2]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning_big').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03be26e6-7f4c-4bd7-897d-086c77371fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 256, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 146s 114ms/step - loss: 6427.5605 - val_loss: 6109.7310\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 148s 117ms/step - loss: 5798.8784 - val_loss: 5556.3062\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 149s 118ms/step - loss: 5436.5806 - val_loss: 5293.4595\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 150s 118ms/step - loss: 5305.8569 - val_loss: 5279.2510\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 150s 118ms/step - loss: 5450.7954 - val_loss: 5307.6792\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5063.1104 - val_loss: 4944.4219\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 4952.9736 - val_loss: 4900.3438\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5256.9976 - val_loss: 5646.8174\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5216.1440 - val_loss: 5630.3770\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5110.5645 - val_loss: 5521.7910\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5136.2490 - val_loss: 5355.0962\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5390.1982 - val_loss: 4907.1104\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5241.6943 - val_loss: 5586.3057\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 4861.6113 - val_loss: 4652.3833\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 4613.9985 - val_loss: 4411.5630\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 150s 119ms/step - loss: 5012.0176 - val_loss: 5990.5903\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm_18 (LSTM)              (None, 30, 256)           365568    \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 30, 256)           0         \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,416,449\n",
      "Trainable params: 1,416,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 13s 47ms/step - loss: 7255.8384\n",
      "--- Starting trial: run-1\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 512, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n",
      "1258/1258 [==============================] - 494s 390ms/step - loss: 6713.0063 - val_loss: 6608.1577\n",
      "Epoch 2/16\n",
      "1258/1258 [==============================] - 490s 389ms/step - loss: 6645.1465 - val_loss: 6608.6865\n",
      "Epoch 3/16\n",
      "1258/1258 [==============================] - 490s 389ms/step - loss: 6644.8438 - val_loss: 6608.4648\n",
      "Epoch 4/16\n",
      "1258/1258 [==============================] - 490s 389ms/step - loss: 6645.0137 - val_loss: 6608.2866\n",
      "Epoch 5/16\n",
      "1258/1258 [==============================] - 491s 390ms/step - loss: 6645.0479 - val_loss: 6608.1777\n",
      "Epoch 6/16\n",
      "1258/1258 [==============================] - 491s 390ms/step - loss: 6644.9404 - val_loss: 6609.1333\n",
      "Epoch 7/16\n",
      "1258/1258 [==============================] - 492s 391ms/step - loss: 6645.1973 - val_loss: 6608.9287\n",
      "Epoch 8/16\n",
      "1258/1258 [==============================] - 493s 391ms/step - loss: 6645.0376 - val_loss: 6609.0752\n",
      "Epoch 9/16\n",
      "1258/1258 [==============================] - 492s 391ms/step - loss: 6645.1553 - val_loss: 6608.6724\n",
      "Epoch 10/16\n",
      "1258/1258 [==============================] - 490s 389ms/step - loss: 6645.0967 - val_loss: 6608.1953\n",
      "Epoch 11/16\n",
      "1258/1258 [==============================] - 491s 390ms/step - loss: 6645.0308 - val_loss: 6608.2388\n",
      "Epoch 12/16\n",
      "1258/1258 [==============================] - 492s 391ms/step - loss: 6644.6025 - val_loss: 6609.2485\n",
      "Epoch 13/16\n",
      "1258/1258 [==============================] - 492s 391ms/step - loss: 6645.3076 - val_loss: 6608.7217\n",
      "Epoch 14/16\n",
      "1258/1258 [==============================] - 493s 391ms/step - loss: 6645.2729 - val_loss: 6608.5693\n",
      "Epoch 15/16\n",
      "1258/1258 [==============================] - 494s 393ms/step - loss: 6644.9673 - val_loss: 6608.1772\n",
      "Epoch 16/16\n",
      "1258/1258 [==============================] - 493s 392ms/step - loss: 6645.0981 - val_loss: 6608.7759\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_20 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " lstm_21 (LSTM)              (None, 30, 512)           1255424   \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 30, 512)           0         \n",
      "                                                                 \n",
      " lstm_22 (LSTM)              (None, 30, 512)           2099200   \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 30, 512)           0         \n",
      "                                                                 \n",
      " lstm_23 (LSTM)              (None, 512)               2099200   \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,454,337\n",
      "Trainable params: 5,454,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "270/270 [==============================] - 39s 144ms/step - loss: 7812.9487\n",
      "--- Starting trial: run-2\n",
      "{'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 1024, 'dropout': 0.2, 'lr': 0.01}\n",
      "Epoch 1/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 16:13:25.244791: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 529.73MiB (rounded to 555458560)requested by op CudnnRNN\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-02-23 16:13:25.244888: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2022-02-23 16:13:25.244938: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 110, Chunks in use: 108. 27.5KiB allocated for chunks. 27.0KiB in use in bin. 473B client-requested in use in bin.\n",
      "2022-02-23 16:13:25.244964: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-23 16:13:25.244994: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245046: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 4, Chunks in use: 3. 11.8KiB allocated for chunks. 9.5KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245073: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 4, Chunks in use: 4. 16.0KiB allocated for chunks. 16.0KiB in use in bin. 15.9KiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245094: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 10, Chunks in use: 9. 86.8KiB allocated for chunks. 74.8KiB in use in bin. 72.0KiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245116: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 12, Chunks in use: 12. 192.0KiB allocated for chunks. 192.0KiB in use in bin. 192.0KiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245134: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245150: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245166: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245182: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245200: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 3, Chunks in use: 3. 2.34MiB allocated for chunks. 2.34MiB in use in bin. 2.34MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245218: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 2. 4.48MiB allocated for chunks. 3.12MiB in use in bin. 3.12MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245240: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 11, Chunks in use: 11. 41.30MiB allocated for chunks. 41.30MiB in use in bin. 40.43MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245262: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 20, Chunks in use: 20. 89.83MiB allocated for chunks. 89.83MiB in use in bin. 79.43MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245283: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 2, Chunks in use: 1. 23.61MiB allocated for chunks. 11.39MiB in use in bin. 11.39MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245305: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 17, Chunks in use: 16. 283.47MiB allocated for chunks. 257.59MiB in use in bin. 257.59MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245327: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 3, Chunks in use: 2. 100.06MiB allocated for chunks. 64.06MiB in use in bin. 64.06MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245354: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 8, Chunks in use: 8. 932.81MiB allocated for chunks. 932.81MiB in use in bin. 932.81MiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245373: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 179.92MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245392: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 2. 1.25GiB allocated for chunks. 1.25GiB in use in bin. 1.14GiB client-requested in use in bin.\n",
      "2022-02-23 16:13:25.245712: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 529.73MiB was 256.00MiB, Chunk State: \n",
      "2022-02-23 16:13:25.245766: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 3080585216\n",
      "2022-02-23 16:13:25.246467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000000 of size 1280 next 1\n",
      "2022-02-23 16:13:25.246524: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000500 of size 256 next 2\n",
      "2022-02-23 16:13:25.246543: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000600 of size 256 next 3\n",
      "2022-02-23 16:13:25.246557: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000700 of size 256 next 4\n",
      "2022-02-23 16:13:25.246570: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000800 of size 256 next 5\n",
      "2022-02-23 16:13:25.246583: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000900 of size 256 next 6\n",
      "2022-02-23 16:13:25.246596: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000a00 of size 256 next 7\n",
      "2022-02-23 16:13:25.246610: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000b00 of size 256 next 66\n",
      "2022-02-23 16:13:25.246623: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000c00 of size 256 next 30\n",
      "2022-02-23 16:13:25.246636: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000d00 of size 256 next 69\n",
      "2022-02-23 16:13:25.246649: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60000e00 of size 256 next 93\n",
      "2022-02-23 16:13:25.246663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb60000f00 of size 256 next 159\n",
      "2022-02-23 16:13:25.246677: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60001000 of size 256 next 122\n",
      "2022-02-23 16:13:25.246690: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60001100 of size 256 next 98\n",
      "2022-02-23 16:13:25.246703: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60001200 of size 256 next 114\n",
      "2022-02-23 16:13:25.246717: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60001300 of size 3328 next 31\n",
      "2022-02-23 16:13:25.246730: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60002000 of size 256 next 118\n",
      "2022-02-23 16:13:25.246744: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60002100 of size 9472 next 61\n",
      "2022-02-23 16:13:25.246757: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004600 of size 256 next 62\n",
      "2022-02-23 16:13:25.246770: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004700 of size 256 next 58\n",
      "2022-02-23 16:13:25.246783: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004800 of size 256 next 41\n",
      "2022-02-23 16:13:25.246796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004900 of size 256 next 37\n",
      "2022-02-23 16:13:25.246809: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004a00 of size 256 next 76\n",
      "2022-02-23 16:13:25.246822: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004b00 of size 256 next 115\n",
      "2022-02-23 16:13:25.246836: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004c00 of size 256 next 16\n",
      "2022-02-23 16:13:25.246849: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004d00 of size 256 next 54\n",
      "2022-02-23 16:13:25.246869: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004e00 of size 256 next 85\n",
      "2022-02-23 16:13:25.246894: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60004f00 of size 256 next 46\n",
      "2022-02-23 16:13:25.246920: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005000 of size 256 next 57\n",
      "2022-02-23 16:13:25.246941: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005100 of size 256 next 51\n",
      "2022-02-23 16:13:25.246966: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005200 of size 256 next 129\n",
      "2022-02-23 16:13:25.246991: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005300 of size 256 next 88\n",
      "2022-02-23 16:13:25.247017: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005400 of size 256 next 162\n",
      "2022-02-23 16:13:25.247037: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005500 of size 256 next 28\n",
      "2022-02-23 16:13:25.247078: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005600 of size 256 next 48\n",
      "2022-02-23 16:13:25.247106: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005700 of size 256 next 119\n",
      "2022-02-23 16:13:25.247130: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005800 of size 256 next 171\n",
      "2022-02-23 16:13:25.247156: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005900 of size 256 next 120\n",
      "2022-02-23 16:13:25.247182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005a00 of size 256 next 47\n",
      "2022-02-23 16:13:25.247206: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005b00 of size 256 next 133\n",
      "2022-02-23 16:13:25.247231: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005c00 of size 256 next 34\n",
      "2022-02-23 16:13:25.247258: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005d00 of size 256 next 147\n",
      "2022-02-23 16:13:25.247272: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005e00 of size 256 next 59\n",
      "2022-02-23 16:13:25.247285: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60005f00 of size 256 next 136\n",
      "2022-02-23 16:13:25.247298: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60006000 of size 256 next 111\n",
      "2022-02-23 16:13:25.247314: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60006100 of size 256 next 128\n",
      "2022-02-23 16:13:25.247338: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60006200 of size 256 next 135\n",
      "2022-02-23 16:13:25.247364: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60006300 of size 3072 next 75\n",
      "2022-02-23 16:13:25.247390: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60006f00 of size 256 next 38\n",
      "2022-02-23 16:13:25.247416: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007000 of size 256 next 107\n",
      "2022-02-23 16:13:25.247434: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007100 of size 256 next 35\n",
      "2022-02-23 16:13:25.247447: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007200 of size 256 next 80\n",
      "2022-02-23 16:13:25.247460: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007300 of size 256 next 174\n",
      "2022-02-23 16:13:25.247472: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007400 of size 256 next 12\n",
      "2022-02-23 16:13:25.247485: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007500 of size 256 next 56\n",
      "2022-02-23 16:13:25.247497: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007600 of size 256 next 179\n",
      "2022-02-23 16:13:25.247515: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007700 of size 256 next 155\n",
      "2022-02-23 16:13:25.247528: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007800 of size 256 next 45\n",
      "2022-02-23 16:13:25.247541: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007900 of size 256 next 157\n",
      "2022-02-23 16:13:25.247553: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007a00 of size 256 next 55\n",
      "2022-02-23 16:13:25.247566: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007b00 of size 256 next 183\n",
      "2022-02-23 16:13:25.247578: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007c00 of size 256 next 97\n",
      "2022-02-23 16:13:25.247594: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007d00 of size 256 next 40\n",
      "2022-02-23 16:13:25.247618: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007e00 of size 256 next 86\n",
      "2022-02-23 16:13:25.247643: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60007f00 of size 256 next 104\n",
      "2022-02-23 16:13:25.247667: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60008000 of size 256 next 44\n",
      "2022-02-23 16:13:25.247694: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60008100 of size 256 next 101\n",
      "2022-02-23 16:13:25.247721: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60008200 of size 9728 next 138\n",
      "2022-02-23 16:13:25.247735: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000a800 of size 256 next 77\n",
      "2022-02-23 16:13:25.247748: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000a900 of size 256 next 39\n",
      "2022-02-23 16:13:25.247761: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000aa00 of size 256 next 83\n",
      "2022-02-23 16:13:25.247773: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000ab00 of size 256 next 144\n",
      "2022-02-23 16:13:25.247786: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000ac00 of size 256 next 146\n",
      "2022-02-23 16:13:25.247798: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000ad00 of size 256 next 108\n",
      "2022-02-23 16:13:25.247810: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000ae00 of size 256 next 184\n",
      "2022-02-23 16:13:25.247823: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb6000af00 of size 256 next 149\n",
      "2022-02-23 16:13:25.247836: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000b000 of size 256 next 131\n",
      "2022-02-23 16:13:25.247848: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000b100 of size 256 next 172\n",
      "2022-02-23 16:13:25.247862: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000b200 of size 3328 next 99\n",
      "2022-02-23 16:13:25.247874: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000bf00 of size 256 next 71\n",
      "2022-02-23 16:13:25.247886: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c000 of size 256 next 91\n",
      "2022-02-23 16:13:25.247899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c100 of size 256 next 8\n",
      "2022-02-23 16:13:25.247912: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c200 of size 256 next 109\n",
      "2022-02-23 16:13:25.247924: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c300 of size 256 next 72\n",
      "2022-02-23 16:13:25.247937: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c400 of size 256 next 78\n",
      "2022-02-23 16:13:25.247949: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c500 of size 256 next 112\n",
      "2022-02-23 16:13:25.247961: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c600 of size 256 next 151\n",
      "2022-02-23 16:13:25.247978: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c700 of size 256 next 50\n",
      "2022-02-23 16:13:25.248003: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c800 of size 256 next 52\n",
      "2022-02-23 16:13:25.248028: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000c900 of size 256 next 140\n",
      "2022-02-23 16:13:25.248049: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000ca00 of size 256 next 15\n",
      "2022-02-23 16:13:25.248074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000cb00 of size 256 next 10\n",
      "2022-02-23 16:13:25.248100: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000cc00 of size 256 next 19\n",
      "2022-02-23 16:13:25.248119: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000cd00 of size 256 next 63\n",
      "2022-02-23 16:13:25.248132: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000ce00 of size 256 next 13\n",
      "2022-02-23 16:13:25.248145: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000cf00 of size 256 next 11\n",
      "2022-02-23 16:13:25.248159: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d000 of size 256 next 14\n",
      "2022-02-23 16:13:25.248182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d100 of size 256 next 117\n",
      "2022-02-23 16:13:25.248210: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d200 of size 256 next 17\n",
      "2022-02-23 16:13:25.248233: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d300 of size 256 next 18\n",
      "2022-02-23 16:13:25.248257: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d400 of size 256 next 121\n",
      "2022-02-23 16:13:25.248282: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d500 of size 256 next 20\n",
      "2022-02-23 16:13:25.248306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d600 of size 256 next 21\n",
      "2022-02-23 16:13:25.248323: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d700 of size 256 next 22\n",
      "2022-02-23 16:13:25.248336: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d800 of size 256 next 23\n",
      "2022-02-23 16:13:25.248358: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000d900 of size 256 next 24\n",
      "2022-02-23 16:13:25.248384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000da00 of size 256 next 25\n",
      "2022-02-23 16:13:25.248409: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000db00 of size 256 next 26\n",
      "2022-02-23 16:13:25.248433: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000dc00 of size 256 next 27\n",
      "2022-02-23 16:13:25.248462: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000dd00 of size 8192 next 130\n",
      "2022-02-23 16:13:25.248489: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6000fd00 of size 8192 next 134\n",
      "2022-02-23 16:13:25.248513: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60011d00 of size 8192 next 87\n",
      "2022-02-23 16:13:25.248527: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60013d00 of size 8192 next 74\n",
      "2022-02-23 16:13:25.248539: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60015d00 of size 8192 next 70\n",
      "2022-02-23 16:13:25.248552: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60017d00 of size 8192 next 143\n",
      "2022-02-23 16:13:25.248566: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60019d00 of size 4194304 next 139\n",
      "2022-02-23 16:13:25.248579: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60419d00 of size 4194304 next 166\n",
      "2022-02-23 16:13:25.248592: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60819d00 of size 4194304 next 113\n",
      "2022-02-23 16:13:25.248605: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb60c19d00 of size 6802176 next 165\n",
      "2022-02-23 16:13:25.248619: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb61296800 of size 819200 next 178\n",
      "2022-02-23 16:13:25.248636: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6135e800 of size 819200 next 150\n",
      "2022-02-23 16:13:25.248649: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb61426800 of size 819200 next 79\n",
      "2022-02-23 16:13:25.248662: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb614ee800 of size 8192 next 164\n",
      "2022-02-23 16:13:25.248676: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb614f0800 of size 2547712 next 82\n",
      "2022-02-23 16:13:25.248689: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6175e800 of size 4194304 next 141\n",
      "2022-02-23 16:13:25.248701: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb61b5e800 of size 4194304 next 142\n",
      "2022-02-23 16:13:25.248714: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb61f5e800 of size 4194304 next 124\n",
      "2022-02-23 16:13:25.248727: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6235e800 of size 4194304 next 182\n",
      "2022-02-23 16:13:25.248740: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6275e800 of size 4194304 next 106\n",
      "2022-02-23 16:13:25.248756: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb62b5e800 of size 4194304 next 49\n",
      "2022-02-23 16:13:25.248781: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb62f5e800 of size 4194304 next 169\n",
      "2022-02-23 16:13:25.248805: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6335e800 of size 4194304 next 67\n",
      "2022-02-23 16:13:25.248829: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6375e800 of size 4194304 next 43\n",
      "2022-02-23 16:13:25.248854: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb63b5e800 of size 4194304 next 73\n",
      "2022-02-23 16:13:25.248877: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb63f5e800 of size 4194304 next 126\n",
      "2022-02-23 16:13:25.248892: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6435e800 of size 4096 next 137\n",
      "2022-02-23 16:13:25.248909: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb6435f800 of size 12288 next 153\n",
      "2022-02-23 16:13:25.248934: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64362800 of size 4096 next 176\n",
      "2022-02-23 16:13:25.248958: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64363800 of size 256 next 175\n",
      "2022-02-23 16:13:25.248983: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64363900 of size 256 next 125\n",
      "2022-02-23 16:13:25.249009: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64363a00 of size 256 next 173\n",
      "2022-02-23 16:13:25.249056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64363b00 of size 256 next 64\n",
      "2022-02-23 16:13:25.249071: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64363c00 of size 256 next 84\n",
      "2022-02-23 16:13:25.249084: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64363d00 of size 256 next 105\n",
      "2022-02-23 16:13:25.249096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64363e00 of size 256 next 148\n",
      "2022-02-23 16:13:25.249109: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb64363f00 of size 2304 next 102\n",
      "2022-02-23 16:13:25.249122: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64364800 of size 4096 next 168\n",
      "2022-02-23 16:13:25.249135: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64365800 of size 4096 next 65\n",
      "2022-02-23 16:13:25.249149: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64366800 of size 16384 next 177\n",
      "2022-02-23 16:13:25.249162: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6436a800 of size 16384 next 132\n",
      "2022-02-23 16:13:25.249175: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6436e800 of size 16384 next 89\n",
      "2022-02-23 16:13:25.249188: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64372800 of size 16384 next 68\n",
      "2022-02-23 16:13:25.249201: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64376800 of size 16384 next 53\n",
      "2022-02-23 16:13:25.249214: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6437a800 of size 16384 next 160\n",
      "2022-02-23 16:13:25.249226: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6437e800 of size 16384 next 29\n",
      "2022-02-23 16:13:25.249239: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64382800 of size 16384 next 161\n",
      "2022-02-23 16:13:25.249252: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64386800 of size 16384 next 163\n",
      "2022-02-23 16:13:25.249265: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6438a800 of size 16384 next 32\n",
      "2022-02-23 16:13:25.249278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6438e800 of size 16384 next 90\n",
      "2022-02-23 16:13:25.249290: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64392800 of size 16384 next 203\n",
      "2022-02-23 16:13:25.249303: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb64396800 of size 1425408 next 36\n",
      "2022-02-23 16:13:25.249318: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb644f2800 of size 1638400 next 94\n",
      "2022-02-23 16:13:25.249331: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64682800 of size 1638400 next 9\n",
      "2022-02-23 16:13:25.249344: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64812800 of size 7020800 next 116\n",
      "2022-02-23 16:13:25.249358: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb64ec4900 of size 16777216 next 152\n",
      "2022-02-23 16:13:25.249372: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb65ec4900 of size 16777216 next 170\n",
      "2022-02-23 16:13:25.249384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb66ec4900 of size 16777216 next 100\n",
      "2022-02-23 16:13:25.249397: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb67ec4900 of size 16777216 next 180\n",
      "2022-02-23 16:13:25.249410: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb68ec4900 of size 16777216 next 154\n",
      "2022-02-23 16:13:25.249428: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb69ec4900 of size 16777216 next 167\n",
      "2022-02-23 16:13:25.249453: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6aec4900 of size 16777216 next 123\n",
      "2022-02-23 16:13:25.249477: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6bec4900 of size 16777216 next 96\n",
      "2022-02-23 16:13:25.249501: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6cec4900 of size 16777216 next 181\n",
      "2022-02-23 16:13:25.249517: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6dec4900 of size 16777216 next 33\n",
      "2022-02-23 16:13:25.249533: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6eec4900 of size 16777216 next 92\n",
      "2022-02-23 16:13:25.249558: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb6fec4900 of size 16777216 next 127\n",
      "2022-02-23 16:13:25.249582: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb70ec4900 of size 16777216 next 42\n",
      "2022-02-23 16:13:25.249607: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb71ec4900 of size 16777216 next 95\n",
      "2022-02-23 16:13:25.249634: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb72ec4900 of size 16777216 next 110\n",
      "2022-02-23 16:13:25.249657: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb73ec4900 of size 4075520 next 103\n",
      "2022-02-23 16:13:25.249671: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb742a7900 of size 7864576 next 158\n",
      "2022-02-23 16:13:25.249685: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb74a27a00 of size 4075520 next 195\n",
      "2022-02-23 16:13:25.249702: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb74e0aa00 of size 4075520 next 209\n",
      "2022-02-23 16:13:25.249715: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb751eda00 of size 4075520 next 205\n",
      "2022-02-23 16:13:25.249729: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb755d0a00 of size 4993024 next 186\n",
      "2022-02-23 16:13:25.249743: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb75a93a00 of size 11940096 next 187\n",
      "2022-02-23 16:13:25.249758: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb765f6b00 of size 4603904 next 189\n",
      "2022-02-23 16:13:25.249782: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb76a5ab00 of size 18448384 next 60\n",
      "2022-02-23 16:13:25.249810: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb77bf2b00 of size 122265600 next 194\n",
      "2022-02-23 16:13:25.249836: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb7f08cb00 of size 122265600 next 191\n",
      "2022-02-23 16:13:25.249861: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb86526b00 of size 122265600 next 190\n",
      "2022-02-23 16:13:25.249886: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb8d9c0b00 of size 4075520 next 185\n",
      "2022-02-23 16:13:25.249901: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb8dda3b00 of size 4075520 next 81\n",
      "2022-02-23 16:13:25.249914: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb8e186b00 of size 4075520 next 201\n",
      "2022-02-23 16:13:25.249927: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb8e569b00 of size 4075520 next 204\n",
      "2022-02-23 16:13:25.249940: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb8e94cb00 of size 12820480 next 199\n",
      "2022-02-23 16:13:25.249953: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb8f586b00 of size 122265600 next 200\n",
      "2022-02-23 16:13:25.249966: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb96a20b00 of size 4194304 next 202\n",
      "2022-02-23 16:13:25.249980: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb96e20b00 of size 33587200 next 210\n",
      "2022-02-23 16:13:25.249993: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb98e28b00 of size 37748736 next 222\n",
      "2022-02-23 16:13:25.250006: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb9b228b00 of size 33587200 next 223\n",
      "2022-02-23 16:13:25.250019: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb9d230b00 of size 4075520 next 221\n",
      "2022-02-23 16:13:25.250032: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb9d613b00 of size 4075520 next 219\n",
      "2022-02-23 16:13:25.250045: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efb9d9f6b00 of size 27131904 next 193\n",
      "2022-02-23 16:13:25.250059: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efb9f3d6b00 of size 611328256 next 192\n",
      "2022-02-23 16:13:25.250072: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efbc3ad8c00 of size 122265600 next 208\n",
      "2022-02-23 16:13:25.250086: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efbcaf72c00 of size 122265600 next 206\n",
      "2022-02-23 16:13:25.250099: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efbd240cc00 of size 122265600 next 214\n",
      "2022-02-23 16:13:25.250112: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efbd98a6c00 of size 122265600 next 220\n",
      "2022-02-23 16:13:25.250125: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7efbe0d40c00 of size 188661760 next 207\n",
      "2022-02-23 16:13:25.250140: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7efbec12cc00 of size 730543104 next 18446744073709551615\n",
      "2022-02-23 16:13:25.250153: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2022-02-23 16:13:25.250194: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 108 Chunks of size 256 totalling 27.0KiB\n",
      "2022-02-23 16:13:25.250227: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-02-23 16:13:25.250257: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB\n",
      "2022-02-23 16:13:25.250288: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 3328 totalling 6.5KiB\n",
      "2022-02-23 16:13:25.250314: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 4096 totalling 16.0KiB\n",
      "2022-02-23 16:13:25.250331: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 7 Chunks of size 8192 totalling 56.0KiB\n",
      "2022-02-23 16:13:25.250346: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 9472 totalling 9.2KiB\n",
      "2022-02-23 16:13:25.250361: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 9728 totalling 9.5KiB\n",
      "2022-02-23 16:13:25.250379: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 12 Chunks of size 16384 totalling 192.0KiB\n",
      "2022-02-23 16:13:25.250395: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 819200 totalling 2.34MiB\n",
      "2022-02-23 16:13:25.250422: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 1638400 totalling 3.12MiB\n",
      "2022-02-23 16:13:25.250453: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2547712 totalling 2.43MiB\n",
      "2022-02-23 16:13:25.250484: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 10 Chunks of size 4075520 totalling 38.87MiB\n",
      "2022-02-23 16:13:25.250519: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 15 Chunks of size 4194304 totalling 60.00MiB\n",
      "2022-02-23 16:13:25.250551: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4603904 totalling 4.39MiB\n",
      "2022-02-23 16:13:25.250570: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4993024 totalling 4.76MiB\n",
      "2022-02-23 16:13:25.250586: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 6802176 totalling 6.49MiB\n",
      "2022-02-23 16:13:25.250601: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7020800 totalling 6.70MiB\n",
      "2022-02-23 16:13:25.250616: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 7864576 totalling 7.50MiB\n",
      "2022-02-23 16:13:25.250632: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 11940096 totalling 11.39MiB\n",
      "2022-02-23 16:13:25.250649: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 15 Chunks of size 16777216 totalling 240.00MiB\n",
      "2022-02-23 16:13:25.250665: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 18448384 totalling 17.59MiB\n",
      "2022-02-23 16:13:25.250682: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 33587200 totalling 64.06MiB\n",
      "2022-02-23 16:13:25.250698: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 8 Chunks of size 122265600 totalling 932.81MiB\n",
      "2022-02-23 16:13:25.250714: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 611328256 totalling 583.01MiB\n",
      "2022-02-23 16:13:25.250730: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 730543104 totalling 696.70MiB\n",
      "2022-02-23 16:13:25.250746: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 2.62GiB\n",
      "2022-02-23 16:13:25.250760: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 3080585216 memory_limit_: 3080585216 available bytes: 0 curr_region_allocation_bytes_: 6161170432\n",
      "2022-02-23 16:13:25.250787: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                      3080585216\n",
      "InUse:                      2812781824\n",
      "MaxInUse:                   2942469376\n",
      "NumAllocs:                    67240878\n",
      "MaxAllocSize:                730543104\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-02-23 16:13:25.250863: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *******************************_***************************************_____*********************xxx\n",
      "2022-02-23 16:13:25.251656: E tensorflow/stream_executor/dnn.cc:764] OOM when allocating tensor with shape[555458560] and type uint8 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-02-23 16:13:25.252465: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1562 : INTERNAL: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 1024, 1024, 1, 30, 995, 1024] \n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": " RecvAsync is cancelled.\n\t [[node mean_absolute_error/cond/cond/remove_squeezable_dimensions/cond_1\n (defined at /home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py:143)\n]] [Op:__inference_train_function_3128463]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node mean_absolute_error/cond/cond/remove_squeezable_dimensions/cond_1:\nIn[0] mean_absolute_error/cond/cond/remove_squeezable_dimensions/Equal_1 (defined at /home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py:144)\t\nIn[1] mean_absolute_error/cond/cond/remove_squeezable_dimensions/Rank_1/IteratorGetNext (defined at /home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py:134)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2768, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2814, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3251, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_195725/2582481134.py\", line 17, in <module>\n>>>     run('./logs/hparam_tuning_big/' + run_name, hparams)\n>>> \n>>>   File \"/tmp/ipykernel_195725/4010069047.py\", line 35, in run\n>>>     val_loss = train_test_model(hparams)\n>>> \n>>>   File \"/tmp/ipykernel_195725/4010069047.py\", line 22, in train_test_model\n>>>     model.fit(train_ds,\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/losses.py\", line 242, in call\n>>>     y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(y_pred, y_true)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 197, in squeeze_or_expand_dimensions\n>>>     y_true, y_pred = tf.cond(\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 195, in <lambda>\n>>>     maybe_squeeze_dims = lambda: tf.cond(  # pylint: disable=g-long-lambda\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 192, in <lambda>\n>>>     squeeze_dims = lambda: remove_squeezable_dimensions(  # pylint: disable=g-long-lambda\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 143, in remove_squeezable_dimensions\n>>>     labels = tf.cond(\n>>> \n\nFunction call stack:\ntrain_function -> mean_absolute_error_cond_cond_true_3126348_rewritten\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--- Starting trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m({h\u001b[38;5;241m.\u001b[39mname: hparams[h] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hparams})\n\u001b[0;32m---> 17\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs/hparam_tuning_big/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m session_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mcreate_file_writer(run_dir)\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     34\u001b[0m     hp\u001b[38;5;241m.\u001b[39mhparams(hparams)  \u001b[38;5;66;03m# record the values used in this trial\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mscalar(METRIC_MAE, val_loss, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[0;34m(hparams, shape)\u001b[0m\n\u001b[1;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, outputs\u001b[38;5;241m=\u001b[39moutput)\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mhparams[HP_LR]),  loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[1;32m     28\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_ds)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m:  RecvAsync is cancelled.\n\t [[node mean_absolute_error/cond/cond/remove_squeezable_dimensions/cond_1\n (defined at /home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py:143)\n]] [Op:__inference_train_function_3128463]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node mean_absolute_error/cond/cond/remove_squeezable_dimensions/cond_1:\nIn[0] mean_absolute_error/cond/cond/remove_squeezable_dimensions/Equal_1 (defined at /home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py:144)\t\nIn[1] mean_absolute_error/cond/cond/remove_squeezable_dimensions/Rank_1/IteratorGetNext (defined at /home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py:134)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2768, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2814, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3251, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_195725/2582481134.py\", line 17, in <module>\n>>>     run('./logs/hparam_tuning_big/' + run_name, hparams)\n>>> \n>>>   File \"/tmp/ipykernel_195725/4010069047.py\", line 35, in run\n>>>     val_loss = train_test_model(hparams)\n>>> \n>>>   File \"/tmp/ipykernel_195725/4010069047.py\", line 22, in train_test_model\n>>>     model.fit(train_ds,\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py\", line 809, in train_step\n>>>     loss = self.compiled_loss(\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n>>>     loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/losses.py\", line 141, in __call__\n>>>     losses = call_fn(y_true, y_pred)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/losses.py\", line 242, in call\n>>>     y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(y_pred, y_true)\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 197, in squeeze_or_expand_dimensions\n>>>     y_true, y_pred = tf.cond(\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 195, in <lambda>\n>>>     maybe_squeeze_dims = lambda: tf.cond(  # pylint: disable=g-long-lambda\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 192, in <lambda>\n>>>     squeeze_dims = lambda: remove_squeezable_dimensions(  # pylint: disable=g-long-lambda\n>>> \n>>>   File \"/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/losses_utils.py\", line 143, in remove_squeezable_dimensions\n>>>     labels = tf.cond(\n>>> \n\nFunction call stack:\ntrain_function -> mean_absolute_error_cond_cond_true_3126348_rewritten\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning_big/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f3c8b-f7f2-443d-994c-12348b93af8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b18c15-146e-4206-9aea-56ea77ae8532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281eeb3-f816-466e-8ee3-6a101f0ed11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cbdc3-fc87-4f23-8f1b-f51d6aa9805e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf09c66-8a82-4800-b8f0-a9b6cd9774e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ffee6-9cc7-4dae-b81a-40491c65f8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38795671-b9c2-453a-92b0-bec2444c07fd",
   "metadata": {},
   "source": [
    "#### Testing 4, 5, and 6 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00c2e5-4dfa-4637-aaab-35a7ecbe5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([4, 5, 6]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([32, 64, 128]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([256]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning6').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e722ee-b3d4-46dc-bedf-8a4616789a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,101)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6e570-cc34-4cbd-9071-f4a17e802e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning6/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250522c-de3c-426f-a9ce-a8b4c75363d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152bffd-bde8-4e80-a2d9-f2dea66d3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([3]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128, 256, 512]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128, 256, 512]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning5').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b57339-bdb2-44b6-96b3-5ac4f0e3d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,101)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deaee76-e048-404a-9f86-10038aa964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning5/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3619c57-c71e-4046-a136-9c695001ca73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeabaac-292d-4b7e-b16b-9865e57f7ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6563f7-1956-45a4-9d1a-48b1d0123d40",
   "metadata": {},
   "source": [
    "# IGNORE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a22f3-db2b-46cc-b86f-145672a92814",
   "metadata": {},
   "source": [
    "#### Output from first full trial run without geoencoding. From 2/11/22 - 2/12/22.  Logs in hparam_tuning_2022-02-11-no-geoencoding-no-seed\n",
    "\n",
    "    --- Starting trial: run-0\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1714.8448 - mae: 1714.8448 - val_loss: 1698.0995 - val_mae: 1698.0995\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1676.5148 - mae: 1676.5148 - val_loss: 1662.7485 - val_mae: 1662.7485\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1640.6196 - mae: 1640.6196 - val_loss: 1628.1760 - val_mae: 1628.1760\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1605.7626 - mae: 1605.7626 - val_loss: 1593.2338 - val_mae: 1593.2338\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1572.2823 - mae: 1572.2823 - val_loss: 1558.5253 - val_mae: 1558.5253\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1537.3369 - mae: 1537.3369 - val_loss: 1526.2665 - val_mae: 1526.2665\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1503.9446 - mae: 1503.9446 - val_loss: 1490.5928 - val_mae: 1490.5928\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1470.5991 - mae: 1470.5991 - val_loss: 1459.7194 - val_mae: 1459.7194\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1438.0496 - mae: 1438.0496 - val_loss: 1427.7711 - val_mae: 1427.7711\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1406.5685 - mae: 1406.5685 - val_loss: 1397.7261 - val_mae: 1397.7261\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1375.7841 - mae: 1375.7841 - val_loss: 1368.0007 - val_mae: 1368.0007\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1345.6710 - mae: 1345.6710 - val_loss: 1335.7712 - val_mae: 1335.7712\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1315.6511 - mae: 1315.6511 - val_loss: 1307.7990 - val_mae: 1307.7990\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1286.4814 - mae: 1286.4814 - val_loss: 1277.9666 - val_mae: 1277.9666\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1258.2283 - mae: 1258.2283 - val_loss: 1252.6665 - val_mae: 1252.6665\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1229.5457 - mae: 1229.5457 - val_loss: 1225.6205 - val_mae: 1225.6205\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1210.3488 - mae: 1210.3488\n",
    "    --- Starting trial: run-1\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 35ms/step - loss: 1558.9589 - mae: 1558.9589 - val_loss: 1404.5382 - val_mae: 1404.5382\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1262.5734 - mae: 1262.5734 - val_loss: 1144.1647 - val_mae: 1144.1647\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1042.4114 - mae: 1042.4114 - val_loss: 988.6213 - val_mae: 988.6213\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 884.6074 - mae: 884.6074 - val_loss: 805.5174 - val_mae: 805.5174\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 712.0137 - mae: 712.0137 - val_loss: 639.9149 - val_mae: 639.9149\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 560.2214 - mae: 560.2214 - val_loss: 535.0701 - val_mae: 535.0701\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 470.0902 - mae: 470.0903 - val_loss: 440.2448 - val_mae: 440.2448\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 402.9714 - mae: 402.9714 - val_loss: 378.6845 - val_mae: 378.6845\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 376.1292 - mae: 376.1292 - val_loss: 351.0140 - val_mae: 351.0140\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 335.2832 - mae: 335.2832 - val_loss: 357.6178 - val_mae: 357.6178\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 309.3643 - mae: 309.3643 - val_loss: 274.2231 - val_mae: 274.2231\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 259.6491 - mae: 259.6491 - val_loss: 252.2940 - val_mae: 252.2940\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 240.8009 - mae: 240.8009 - val_loss: 249.1926 - val_mae: 249.1926\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 254.2686 - mae: 254.2686 - val_loss: 226.4079 - val_mae: 226.4079\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 216.1730 - mae: 216.1730 - val_loss: 212.1332 - val_mae: 212.1332\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 210.3830 - mae: 210.3830 - val_loss: 228.0083 - val_mae: 228.0083\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 222.4450 - mae: 222.4450\n",
    "    --- Starting trial: run-2\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.0021 - mae: 1715.0021 - val_loss: 1696.0353 - val_mae: 1696.0353\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1675.9495 - mae: 1675.9495 - val_loss: 1661.2244 - val_mae: 1661.2244\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1640.5969 - mae: 1640.5969 - val_loss: 1625.9960 - val_mae: 1625.9960\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1605.8707 - mae: 1605.8707 - val_loss: 1592.2338 - val_mae: 1592.2338\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1571.0924 - mae: 1571.0924 - val_loss: 1555.5453 - val_mae: 1555.5453\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1537.3981 - mae: 1537.3981 - val_loss: 1524.9481 - val_mae: 1524.9481\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1504.1108 - mae: 1504.1108 - val_loss: 1489.3528 - val_mae: 1489.3528\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1471.0813 - mae: 1471.0813 - val_loss: 1458.1285 - val_mae: 1458.1285\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1438.5751 - mae: 1438.5751 - val_loss: 1428.0801 - val_mae: 1428.0801\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1407.9596 - mae: 1407.9596 - val_loss: 1395.9967 - val_mae: 1395.9967\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1377.3335 - mae: 1377.3335 - val_loss: 1367.2148 - val_mae: 1367.2148\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1346.6747 - mae: 1346.6747 - val_loss: 1335.2466 - val_mae: 1335.2466\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1317.4336 - mae: 1317.4336 - val_loss: 1307.0999 - val_mae: 1307.0999\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1289.2543 - mae: 1289.2543 - val_loss: 1277.9841 - val_mae: 1277.9841\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1260.2249 - mae: 1260.2249 - val_loss: 1249.4365 - val_mae: 1249.4365\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1231.8981 - mae: 1231.8981 - val_loss: 1222.8695 - val_mae: 1222.8695\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1203.4991 - mae: 1203.4991\n",
    "    --- Starting trial: run-3\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1563.2950 - mae: 1563.2950 - val_loss: 1408.8458 - val_mae: 1408.8458\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1273.8225 - mae: 1273.8225 - val_loss: 1148.4290 - val_mae: 1148.4290\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1064.4722 - mae: 1064.4722 - val_loss: 963.9249 - val_mae: 963.9249\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 917.4946 - mae: 917.4946 - val_loss: 863.0434 - val_mae: 863.0434\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 822.7327 - mae: 822.7327 - val_loss: 775.3434 - val_mae: 775.3434\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 724.7830 - mae: 724.7830 - val_loss: 639.7692 - val_mae: 639.7692\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 614.3043 - mae: 614.3043 - val_loss: 516.8727 - val_mae: 516.8727\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 537.7888 - mae: 537.7888 - val_loss: 456.5970 - val_mae: 456.5970\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 467.8847 - mae: 467.8847 - val_loss: 357.0389 - val_mae: 357.0389\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 411.8556 - mae: 411.8556 - val_loss: 351.1302 - val_mae: 351.1302\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 410.8646 - mae: 410.8646 - val_loss: 283.1604 - val_mae: 283.1604\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 390.0403 - mae: 390.0403 - val_loss: 304.8136 - val_mae: 304.8136\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 386.8397 - mae: 386.8397 - val_loss: 258.3987 - val_mae: 258.3987\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 377.4386 - mae: 377.4386 - val_loss: 260.6350 - val_mae: 260.6350\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 373.3785 - mae: 373.3785 - val_loss: 276.4822 - val_mae: 276.4822\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 370.0834 - mae: 370.0834 - val_loss: 258.6117 - val_mae: 258.6117\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 251.2752 - mae: 251.2752\n",
    "    --- Starting trial: run-4\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1694.6875 - mae: 1694.6875 - val_loss: 1659.0698 - val_mae: 1659.0698\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1622.3441 - mae: 1622.3441 - val_loss: 1592.0505 - val_mae: 1592.0505\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1555.8229 - mae: 1555.8229 - val_loss: 1526.0814 - val_mae: 1526.0814\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1489.6663 - mae: 1489.6663 - val_loss: 1462.4453 - val_mae: 1462.4453\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1426.1626 - mae: 1426.1626 - val_loss: 1401.9027 - val_mae: 1401.9027\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1365.9249 - mae: 1365.9249 - val_loss: 1343.3584 - val_mae: 1343.3584\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1307.6086 - mae: 1307.6086 - val_loss: 1287.0707 - val_mae: 1287.0707\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1251.5591 - mae: 1251.5591 - val_loss: 1231.2085 - val_mae: 1231.2085\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1196.7355 - mae: 1196.7355 - val_loss: 1178.4388 - val_mae: 1178.4388\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1144.3530 - mae: 1144.3530 - val_loss: 1128.5942 - val_mae: 1128.5942\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1094.1368 - mae: 1094.1368 - val_loss: 1078.8323 - val_mae: 1078.8323\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1045.3096 - mae: 1045.3096 - val_loss: 1032.2638 - val_mae: 1032.2638\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 999.4599 - mae: 999.4599 - val_loss: 988.3935 - val_mae: 988.3935\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 953.3505 - mae: 953.3505 - val_loss: 942.0163 - val_mae: 942.0163\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 908.6214 - mae: 908.6214 - val_loss: 897.7542 - val_mae: 897.7542\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 865.5666 - mae: 865.5666 - val_loss: 855.2752 - val_mae: 855.2752\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 842.3766 - mae: 842.3766\n",
    "    --- Starting trial: run-5\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1419.5444 - mae: 1419.5444 - val_loss: 1181.4783 - val_mae: 1181.4783\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 999.2672 - mae: 999.2672 - val_loss: 847.1257 - val_mae: 847.1257\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 769.5046 - mae: 769.5046 - val_loss: 727.3767 - val_mae: 727.3767\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 642.5268 - mae: 642.5268 - val_loss: 578.0294 - val_mae: 578.0294\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 466.1321 - mae: 466.1321 - val_loss: 441.7265 - val_mae: 441.7265\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 408.8712 - mae: 408.8712 - val_loss: 422.5520 - val_mae: 422.5520\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 402.7164 - mae: 402.7164 - val_loss: 364.4363 - val_mae: 364.4363\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 372.0241 - mae: 372.0241 - val_loss: 397.2017 - val_mae: 397.2017\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 350.7146 - mae: 350.7146 - val_loss: 340.6723 - val_mae: 340.6723\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 282.5263 - mae: 282.5263 - val_loss: 294.4954 - val_mae: 294.4954\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 247.9475 - mae: 247.9475 - val_loss: 262.1410 - val_mae: 262.1410\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 261.2445 - mae: 261.2445 - val_loss: 264.5265 - val_mae: 264.5265\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 238.0075 - mae: 238.0075 - val_loss: 239.8622 - val_mae: 239.8622\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 232.3582 - mae: 232.3582 - val_loss: 237.3781 - val_mae: 237.3781\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 240.3781 - mae: 240.3781 - val_loss: 257.9360 - val_mae: 257.9360\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 234.7059 - mae: 234.7059 - val_loss: 244.7222 - val_mae: 244.7222\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 239.0346 - mae: 239.0346\n",
    "    --- Starting trial: run-6\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 35ms/step - loss: 1695.3423 - mae: 1695.3423 - val_loss: 1660.6970 - val_mae: 1660.6970\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1623.3080 - mae: 1623.3080 - val_loss: 1592.8999 - val_mae: 1592.8999\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1555.6199 - mae: 1555.6199 - val_loss: 1526.4407 - val_mae: 1526.4407\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1490.6748 - mae: 1490.6748 - val_loss: 1462.4221 - val_mae: 1462.4221\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1427.6664 - mae: 1427.6664 - val_loss: 1401.7178 - val_mae: 1401.7178\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1366.9703 - mae: 1366.9703 - val_loss: 1342.8413 - val_mae: 1342.8413\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1310.2562 - mae: 1310.2562 - val_loss: 1286.1354 - val_mae: 1286.1354\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1253.6902 - mae: 1253.6902 - val_loss: 1231.1028 - val_mae: 1231.1028\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1199.5375 - mae: 1199.5375 - val_loss: 1177.7694 - val_mae: 1177.7694\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1148.2744 - mae: 1148.2744 - val_loss: 1127.2009 - val_mae: 1127.2009\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1098.7969 - mae: 1098.7969 - val_loss: 1080.4926 - val_mae: 1080.4926\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1050.2742 - mae: 1050.2742 - val_loss: 1032.7545 - val_mae: 1032.7545\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1004.1852 - mae: 1004.1852 - val_loss: 985.5518 - val_mae: 985.5518\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 958.0642 - mae: 958.0642 - val_loss: 937.9445 - val_mae: 937.9445\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 913.8459 - mae: 913.8459 - val_loss: 895.4848 - val_mae: 895.4848\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 870.8777 - mae: 870.8777 - val_loss: 851.7709 - val_mae: 851.7709\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 839.1649 - mae: 839.1649\n",
    "    --- Starting trial: run-7\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1421.1002 - mae: 1421.1002 - val_loss: 1167.2504 - val_mae: 1167.2504\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1052.9386 - mae: 1052.9386 - val_loss: 935.0981 - val_mae: 935.0981\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 837.0973 - mae: 837.0973 - val_loss: 686.8972 - val_mae: 686.8972\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 632.3118 - mae: 632.3118 - val_loss: 578.2532 - val_mae: 578.2532\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 549.2214 - mae: 549.2214 - val_loss: 431.4684 - val_mae: 431.4684\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 413.2884 - mae: 413.2884 - val_loss: 404.3950 - val_mae: 404.3950\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 401.4612 - mae: 401.4612 - val_loss: 305.3578 - val_mae: 305.3578\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 377.7690 - mae: 377.7690 - val_loss: 289.4633 - val_mae: 289.4633\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 361.9821 - mae: 361.9821 - val_loss: 255.5692 - val_mae: 255.5692\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 350.9854 - mae: 350.9854 - val_loss: 259.7381 - val_mae: 259.7381\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 355.0371 - mae: 355.0371 - val_loss: 255.4603 - val_mae: 255.4603\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 356.7462 - mae: 356.7462 - val_loss: 280.8849 - val_mae: 280.8849\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 368.7358 - mae: 368.7358 - val_loss: 260.0245 - val_mae: 260.0245\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 419.5009 - mae: 419.5009 - val_loss: 295.2500 - val_mae: 295.2500\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 384.6466 - mae: 384.6466 - val_loss: 333.3893 - val_mae: 333.3893\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 412.6953 - mae: 412.6953 - val_loss: 292.2436 - val_mae: 292.2436\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 285.3722 - mae: 285.3722\n",
    "    --- Starting trial: run-8\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1655.6770 - mae: 1655.6769 - val_loss: 1590.8164 - val_mae: 1590.8164\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1520.9161 - mae: 1520.9161 - val_loss: 1460.7323 - val_mae: 1460.7323\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1396.8207 - mae: 1396.8207 - val_loss: 1346.4431 - val_mae: 1346.4431\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1284.3027 - mae: 1284.3027 - val_loss: 1236.5032 - val_mae: 1236.5032\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1177.3429 - mae: 1177.3429 - val_loss: 1135.1763 - val_mae: 1135.1763\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1079.6230 - mae: 1079.6230 - val_loss: 1042.6494 - val_mae: 1042.6494\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 989.1205 - mae: 989.1205 - val_loss: 956.0320 - val_mae: 956.0320\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 901.6415 - mae: 901.6415 - val_loss: 870.8302 - val_mae: 870.8302\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 820.0585 - mae: 820.0585 - val_loss: 792.4569 - val_mae: 792.4569\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 742.1097 - mae: 742.1097 - val_loss: 716.5116 - val_mae: 716.5116\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 665.9520 - mae: 665.9520 - val_loss: 640.8953 - val_mae: 640.8953\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 590.5352 - mae: 590.5352 - val_loss: 568.8193 - val_mae: 568.8193\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 518.8450 - mae: 518.8450 - val_loss: 494.9503 - val_mae: 494.9503\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 448.5501 - mae: 448.5501 - val_loss: 426.8623 - val_mae: 426.8623\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 380.4318 - mae: 380.4318 - val_loss: 360.3162 - val_mae: 360.3162\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 318.8451 - mae: 318.8451 - val_loss: 303.6874 - val_mae: 303.6874\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 296.1056 - mae: 296.1056\n",
    "    --- Starting trial: run-9\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1212.5996 - mae: 1212.5996 - val_loss: 875.3373 - val_mae: 875.3373\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 684.7706 - mae: 684.7706 - val_loss: 497.7931 - val_mae: 497.7931\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 406.5892 - mae: 406.5892 - val_loss: 359.6450 - val_mae: 359.6450\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 287.7188 - mae: 287.7188 - val_loss: 275.8997 - val_mae: 275.8997\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 250.5547 - mae: 250.5547 - val_loss: 262.4268 - val_mae: 262.4268\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 227.2360 - mae: 227.2360 - val_loss: 220.8222 - val_mae: 220.8222\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 213.4138 - mae: 213.4138 - val_loss: 217.0214 - val_mae: 217.0214\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 216.3246 - mae: 216.3246 - val_loss: 205.4248 - val_mae: 205.4248\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 202.3762 - mae: 202.3762 - val_loss: 207.0659 - val_mae: 207.0659\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 201.4633 - mae: 201.4633 - val_loss: 210.2004 - val_mae: 210.2004\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 200.4634 - mae: 200.4634 - val_loss: 204.3891 - val_mae: 204.3891\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 197.9046 - mae: 197.9046 - val_loss: 204.3471 - val_mae: 204.3471\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 198.0645 - mae: 198.0645 - val_loss: 203.8806 - val_mae: 203.8806\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 241.8784 - mae: 241.8784 - val_loss: 259.3530 - val_mae: 259.3530\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 234.5721 - mae: 234.5721 - val_loss: 239.7617 - val_mae: 239.7617\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 235.8562 - mae: 235.8562 - val_loss: 263.1089 - val_mae: 263.1089\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 255.6098 - mae: 255.6098\n",
    "    --- Starting trial: run-10\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1654.9674 - mae: 1654.9674 - val_loss: 1586.5536 - val_mae: 1586.5536\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1518.9548 - mae: 1518.9548 - val_loss: 1461.9506 - val_mae: 1461.9506\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1396.2662 - mae: 1396.2662 - val_loss: 1344.5743 - val_mae: 1344.5743\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1282.9016 - mae: 1282.9016 - val_loss: 1233.0769 - val_mae: 1233.0769\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1177.4990 - mae: 1177.4990 - val_loss: 1133.3776 - val_mae: 1133.3776\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1080.0483 - mae: 1080.0483 - val_loss: 1038.5483 - val_mae: 1038.5483\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 988.6093 - mae: 988.6093 - val_loss: 950.0257 - val_mae: 950.0257\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 901.9268 - mae: 901.9268 - val_loss: 864.5103 - val_mae: 864.5103\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 819.3678 - mae: 819.3678 - val_loss: 784.0034 - val_mae: 784.0034\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 740.5015 - mae: 740.5015 - val_loss: 706.0483 - val_mae: 706.0483\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 662.5690 - mae: 662.5690 - val_loss: 628.4536 - val_mae: 628.4536\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 586.1405 - mae: 586.1405 - val_loss: 552.5478 - val_mae: 552.5478\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 512.1078 - mae: 512.1078 - val_loss: 478.9315 - val_mae: 478.9315\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 441.2626 - mae: 441.2626 - val_loss: 407.5898 - val_mae: 407.5898\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 375.0634 - mae: 375.0634 - val_loss: 343.8361 - val_mae: 343.8361\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 317.2447 - mae: 317.2447 - val_loss: 285.1237 - val_mae: 285.1237\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 277.4681 - mae: 277.4681\n",
    "    --- Starting trial: run-11\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1234.6505 - mae: 1234.6505 - val_loss: 912.9206 - val_mae: 912.9206\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 710.6535 - mae: 710.6535 - val_loss: 520.6102 - val_mae: 520.6102\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 450.1460 - mae: 450.1460 - val_loss: 372.7587 - val_mae: 372.7587\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 359.3508 - mae: 359.3508 - val_loss: 288.9084 - val_mae: 288.9084\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.8996 - mae: 313.8996 - val_loss: 250.5107 - val_mae: 250.5107\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 318.7048 - mae: 318.7048 - val_loss: 254.1273 - val_mae: 254.1273\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 322.0482 - mae: 322.0482 - val_loss: 242.8332 - val_mae: 242.8332\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 325.5547 - mae: 325.5547 - val_loss: 252.1033 - val_mae: 252.1033\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 330.0924 - mae: 330.0924 - val_loss: 250.4689 - val_mae: 250.4689\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 338.8605 - mae: 338.8605 - val_loss: 260.5037 - val_mae: 260.5037\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 343.8581 - mae: 343.8581 - val_loss: 261.8107 - val_mae: 261.8107\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 347.2616 - mae: 347.2616 - val_loss: 264.5319 - val_mae: 264.5319\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 340.9579 - mae: 340.9579 - val_loss: 240.9453 - val_mae: 240.9453\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 333.7604 - mae: 333.7604 - val_loss: 226.1823 - val_mae: 226.1823\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 329.6325 - mae: 329.6325 - val_loss: 259.2430 - val_mae: 259.2430\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 343.8550 - mae: 343.8550 - val_loss: 261.8019 - val_mae: 261.8019\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 254.7327 - mae: 254.7327\n",
    "    --- Starting trial: run-12\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.8248 - mae: 1715.8248 - val_loss: 1697.2705 - val_mae: 1697.2705\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1677.9413 - mae: 1677.9413 - val_loss: 1664.1956 - val_mae: 1664.1956\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1642.4662 - mae: 1642.4662 - val_loss: 1625.6179 - val_mae: 1625.6179\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1607.7755 - mae: 1607.7755 - val_loss: 1595.0839 - val_mae: 1595.0839\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1573.5767 - mae: 1573.5767 - val_loss: 1561.6393 - val_mae: 1561.6393\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1539.0200 - mae: 1539.0200 - val_loss: 1525.4354 - val_mae: 1525.4354\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1505.3275 - mae: 1505.3275 - val_loss: 1492.3324 - val_mae: 1492.3324\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1472.4813 - mae: 1472.4813 - val_loss: 1462.2769 - val_mae: 1462.2769\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1439.2798 - mae: 1439.2798 - val_loss: 1429.9016 - val_mae: 1429.9016\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.4652 - mae: 1407.4652 - val_loss: 1398.5363 - val_mae: 1398.5363\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1377.0692 - mae: 1377.0692 - val_loss: 1368.0190 - val_mae: 1368.0190\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1346.9363 - mae: 1346.9363 - val_loss: 1337.4463 - val_mae: 1337.4463\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1317.8580 - mae: 1317.8580 - val_loss: 1309.9767 - val_mae: 1309.9767\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1288.1263 - mae: 1288.1263 - val_loss: 1281.7169 - val_mae: 1281.7169\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1259.7065 - mae: 1259.7065 - val_loss: 1252.4174 - val_mae: 1252.4174\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1231.6362 - mae: 1231.6362 - val_loss: 1225.9788 - val_mae: 1225.9788\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1207.9965 - mae: 1207.9965\n",
    "    --- Starting trial: run-13\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1566.5254 - mae: 1566.5254 - val_loss: 1404.3480 - val_mae: 1404.3480\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1265.8376 - mae: 1265.8376 - val_loss: 1180.4718 - val_mae: 1180.4718\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1098.5269 - mae: 1098.5269 - val_loss: 1044.3354 - val_mae: 1044.3354\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 978.0400 - mae: 978.0400 - val_loss: 939.7174 - val_mae: 939.7174\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 799.8273 - mae: 799.8273 - val_loss: 691.4581 - val_mae: 691.4581\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 776.6411 - mae: 776.6411 - val_loss: 809.2216 - val_mae: 809.2216\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 756.8688 - mae: 756.8688 - val_loss: 730.0041 - val_mae: 730.0041\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 683.1305 - mae: 683.1305 - val_loss: 490.2134 - val_mae: 490.2134\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 522.1832 - mae: 522.1832 - val_loss: 482.8342 - val_mae: 482.8342\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 530.3801 - mae: 530.3801 - val_loss: 555.1626 - val_mae: 555.1626\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 517.3920 - mae: 517.3920 - val_loss: 507.5374 - val_mae: 507.5374\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 461.6365 - mae: 461.6365 - val_loss: 705.4368 - val_mae: 705.4368\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 636.3152 - mae: 636.3152 - val_loss: 742.5924 - val_mae: 742.5924\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 449.6400 - mae: 449.6400 - val_loss: 799.6154 - val_mae: 799.6154\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 765.3070 - mae: 765.3070 - val_loss: 744.0755 - val_mae: 744.0755\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 839.9476 - mae: 839.9476 - val_loss: 857.7084 - val_mae: 857.7084\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 853.7109 - mae: 853.7109\n",
    "    --- Starting trial: run-14\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1714.8529 - mae: 1714.8529 - val_loss: 1698.8910 - val_mae: 1698.8910\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1677.5345 - mae: 1677.5345 - val_loss: 1661.2338 - val_mae: 1661.2338\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1642.2378 - mae: 1642.2378 - val_loss: 1627.2566 - val_mae: 1627.2566\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1607.4426 - mae: 1607.4426 - val_loss: 1592.7273 - val_mae: 1592.7273\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1572.9250 - mae: 1572.9250 - val_loss: 1559.6086 - val_mae: 1559.6086\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1538.4681 - mae: 1538.4681 - val_loss: 1524.4136 - val_mae: 1524.4136\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1505.1445 - mae: 1505.1445 - val_loss: 1493.8875 - val_mae: 1493.8875\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1472.4653 - mae: 1472.4653 - val_loss: 1460.0833 - val_mae: 1460.0833\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1439.9869 - mae: 1439.9869 - val_loss: 1427.9860 - val_mae: 1427.9860\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1408.8029 - mae: 1408.8029 - val_loss: 1399.9822 - val_mae: 1399.9822\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1377.9950 - mae: 1377.9950 - val_loss: 1366.0642 - val_mae: 1366.0642\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1348.1464 - mae: 1348.1464 - val_loss: 1337.9635 - val_mae: 1337.9635\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1318.6459 - mae: 1318.6459 - val_loss: 1308.7412 - val_mae: 1308.7412\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1289.9111 - mae: 1289.9111 - val_loss: 1278.3978 - val_mae: 1278.3978\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1261.1982 - mae: 1261.1982 - val_loss: 1253.7357 - val_mae: 1253.7357\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1232.6780 - mae: 1232.6780 - val_loss: 1224.5164 - val_mae: 1224.5164\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1209.4137 - mae: 1209.4137\n",
    "    --- Starting trial: run-15\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1562.0278 - mae: 1562.0278 - val_loss: 1406.2435 - val_mae: 1406.2435\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1268.8503 - mae: 1268.8503 - val_loss: 1149.9772 - val_mae: 1149.9772\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1038.7648 - mae: 1038.7648 - val_loss: 942.3271 - val_mae: 942.3271\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 840.6599 - mae: 840.6599 - val_loss: 775.3033 - val_mae: 775.3033\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 675.1769 - mae: 675.1769 - val_loss: 609.4270 - val_mae: 609.4270\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 541.7983 - mae: 541.7983 - val_loss: 507.8261 - val_mae: 507.8261\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 475.2537 - mae: 475.2537 - val_loss: 441.4498 - val_mae: 441.4498\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 447.5806 - mae: 447.5806 - val_loss: 422.3975 - val_mae: 422.3975\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 459.0667 - mae: 459.0667 - val_loss: 404.1974 - val_mae: 404.1974\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 364.6326 - mae: 364.6326 - val_loss: 350.4629 - val_mae: 350.4629\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 392.1742 - mae: 392.1742 - val_loss: 1082.8706 - val_mae: 1082.8706\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 363.4105 - mae: 363.4105 - val_loss: 332.4745 - val_mae: 332.4745\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 329.2193 - mae: 329.2193 - val_loss: 330.3246 - val_mae: 330.3246\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 320.8300 - mae: 320.8300 - val_loss: 319.8207 - val_mae: 319.8207\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 318.9174 - mae: 318.9174 - val_loss: 323.6231 - val_mae: 323.6231\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 316.3860 - mae: 316.3860 - val_loss: 311.9363 - val_mae: 311.9363\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 305.7241 - mae: 305.7241\n",
    "    --- Starting trial: run-16\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1694.1797 - mae: 1694.1797 - val_loss: 1662.0779 - val_mae: 1662.0779\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1623.5574 - mae: 1623.5574 - val_loss: 1592.0009 - val_mae: 1592.0009\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1556.7634 - mae: 1556.7634 - val_loss: 1528.4768 - val_mae: 1528.4768\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.7987 - mae: 1490.7987 - val_loss: 1463.1077 - val_mae: 1463.1077\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1427.0037 - mae: 1427.0037 - val_loss: 1402.7134 - val_mae: 1402.7134\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1367.0271 - mae: 1367.0271 - val_loss: 1344.2146 - val_mae: 1344.2146\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1309.4774 - mae: 1309.4774 - val_loss: 1288.8137 - val_mae: 1288.8137\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1253.0773 - mae: 1253.0773 - val_loss: 1232.4832 - val_mae: 1232.4832\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1198.2526 - mae: 1198.2526 - val_loss: 1178.3148 - val_mae: 1178.3148\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1146.0090 - mae: 1146.0090 - val_loss: 1128.2688 - val_mae: 1128.2688\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1095.0912 - mae: 1095.0912 - val_loss: 1078.5981 - val_mae: 1078.5981\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1047.3560 - mae: 1047.3560 - val_loss: 1032.5671 - val_mae: 1032.5671\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1001.1205 - mae: 1001.1205 - val_loss: 988.0533 - val_mae: 988.0533\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 955.5404 - mae: 955.5404 - val_loss: 944.2896 - val_mae: 944.2896\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 911.0602 - mae: 911.0602 - val_loss: 902.9487 - val_mae: 902.9487\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 867.1821 - mae: 867.1821 - val_loss: 858.6188 - val_mae: 858.6188\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 844.5689 - mae: 844.5689\n",
    "    --- Starting trial: run-17\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1419.4297 - mae: 1419.4297 - val_loss: 1196.6685 - val_mae: 1196.6685\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1006.7494 - mae: 1006.7494 - val_loss: 849.1498 - val_mae: 849.1498\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 796.4829 - mae: 796.4829 - val_loss: 821.0558 - val_mae: 821.0558\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 726.4808 - mae: 726.4808 - val_loss: 610.7658 - val_mae: 610.7658\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 501.3910 - mae: 501.3910 - val_loss: 518.0101 - val_mae: 518.0101\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 436.5090 - mae: 436.5090 - val_loss: 421.4387 - val_mae: 421.4387\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 429.1960 - mae: 429.1960 - val_loss: 496.3680 - val_mae: 496.3680\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 448.2284 - mae: 448.2284 - val_loss: 300.5168 - val_mae: 300.5168\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 425.0682 - mae: 425.0682 - val_loss: 308.5029 - val_mae: 308.5029\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 273.0026 - mae: 273.0026 - val_loss: 301.5456 - val_mae: 301.5456\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 263.3662 - mae: 263.3662 - val_loss: 266.9055 - val_mae: 266.9055\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 252.8967 - mae: 252.8967 - val_loss: 247.5253 - val_mae: 247.5253\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 430.7341 - mae: 430.7341 - val_loss: 333.0180 - val_mae: 333.0180\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 267.3344 - mae: 267.3344 - val_loss: 271.4555 - val_mae: 271.4555\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 266.0356 - mae: 266.0356 - val_loss: 268.0161 - val_mae: 268.0161\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 260.3858 - mae: 260.3858 - val_loss: 267.8378 - val_mae: 267.8378\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 260.0629 - mae: 260.0629\n",
    "    --- Starting trial: run-18\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1694.7997 - mae: 1694.7997 - val_loss: 1662.4231 - val_mae: 1662.4231\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1624.2465 - mae: 1624.2465 - val_loss: 1595.6680 - val_mae: 1595.6680\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1557.8837 - mae: 1557.8837 - val_loss: 1528.2695 - val_mae: 1528.2695\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1491.6995 - mae: 1491.6995 - val_loss: 1464.7749 - val_mae: 1464.7749\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1428.9586 - mae: 1428.9586 - val_loss: 1402.7351 - val_mae: 1402.7351\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1369.3558 - mae: 1369.3558 - val_loss: 1344.6344 - val_mae: 1344.6344\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1311.0494 - mae: 1311.0494 - val_loss: 1287.7323 - val_mae: 1287.7323\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1254.7527 - mae: 1254.7527 - val_loss: 1229.9874 - val_mae: 1229.9874\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1200.2535 - mae: 1200.2535 - val_loss: 1178.7148 - val_mae: 1178.7148\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1148.6194 - mae: 1148.6194 - val_loss: 1130.1833 - val_mae: 1130.1833\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1098.5969 - mae: 1098.5969 - val_loss: 1081.6409 - val_mae: 1081.6409\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1051.5981 - mae: 1051.5981 - val_loss: 1033.0592 - val_mae: 1033.0592\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1004.5767 - mae: 1004.5767 - val_loss: 987.8906 - val_mae: 987.8906\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 958.8726 - mae: 958.8726 - val_loss: 941.2385 - val_mae: 941.2385\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 914.5872 - mae: 914.5872 - val_loss: 896.4783 - val_mae: 896.4783\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 871.0402 - mae: 871.0402 - val_loss: 854.1886 - val_mae: 854.1886\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 841.2560 - mae: 841.2560\n",
    "    --- Starting trial: run-19\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1419.3757 - mae: 1419.3757 - val_loss: 1157.8914 - val_mae: 1157.8914\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 951.0938 - mae: 951.0938 - val_loss: 781.5115 - val_mae: 781.5115\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 612.8084 - mae: 612.8084 - val_loss: 458.9123 - val_mae: 458.9123\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 408.0029 - mae: 408.0029 - val_loss: 426.7612 - val_mae: 426.7612\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 375.3343 - mae: 375.3343 - val_loss: 342.4474 - val_mae: 342.4474\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 321.8578 - mae: 321.8578 - val_loss: 277.6904 - val_mae: 277.6904\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 305.2180 - mae: 305.2180 - val_loss: 283.7717 - val_mae: 283.7717\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 307.6875 - mae: 307.6875 - val_loss: 292.2432 - val_mae: 292.2432\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 391.0664 - mae: 391.0664 - val_loss: 422.2168 - val_mae: 422.2168\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 440.4038 - mae: 440.4038 - val_loss: 418.0160 - val_mae: 418.0160\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 439.6539 - mae: 439.6539 - val_loss: 422.3977 - val_mae: 422.3977\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 438.8077 - mae: 438.8077 - val_loss: 423.9795 - val_mae: 423.9795\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 437.6751 - mae: 437.6751 - val_loss: 417.5522 - val_mae: 417.5522\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 436.9357 - mae: 436.9357 - val_loss: 422.4628 - val_mae: 422.4628\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 437.2139 - mae: 437.2139 - val_loss: 422.5095 - val_mae: 422.5095\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 408.1948 - mae: 408.1948 - val_loss: 363.8466 - val_mae: 363.8466\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 356.8757 - mae: 356.8757\n",
    "    --- Starting trial: run-20\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1655.2758 - mae: 1655.2758 - val_loss: 1591.2709 - val_mae: 1591.2709\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1521.4144 - mae: 1521.4144 - val_loss: 1462.7307 - val_mae: 1462.7307\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1397.7123 - mae: 1397.7123 - val_loss: 1345.2852 - val_mae: 1345.2852\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1284.5764 - mae: 1284.5764 - val_loss: 1238.3728 - val_mae: 1238.3728\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1178.8931 - mae: 1178.8931 - val_loss: 1138.8553 - val_mae: 1138.8553\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1080.6917 - mae: 1080.6917 - val_loss: 1044.1903 - val_mae: 1044.1903\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 989.8049 - mae: 989.8049 - val_loss: 955.7908 - val_mae: 955.7908\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 903.2751 - mae: 903.2751 - val_loss: 873.7616 - val_mae: 873.7616\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 820.6246 - mae: 820.6246 - val_loss: 792.9376 - val_mae: 792.9376\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 743.0720 - mae: 743.0720 - val_loss: 724.5831 - val_mae: 724.5831\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 667.4967 - mae: 667.4967 - val_loss: 642.5319 - val_mae: 642.5319\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 592.8243 - mae: 592.8243 - val_loss: 568.1324 - val_mae: 568.1324\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 519.8990 - mae: 519.8990 - val_loss: 500.3583 - val_mae: 500.3583\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 449.8224 - mae: 449.8224 - val_loss: 437.3765 - val_mae: 437.3765\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 47s 37ms/step - loss: 382.6425 - mae: 382.6425 - val_loss: 362.7072 - val_mae: 362.7072\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 321.2919 - mae: 321.2919 - val_loss: 305.8570 - val_mae: 305.8570\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 296.7222 - mae: 296.7222\n",
    "    --- Starting trial: run-21\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1187.4447 - mae: 1187.4447 - val_loss: 799.1432 - val_mae: 799.1432\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 597.4485 - mae: 597.4485 - val_loss: 768.1321 - val_mae: 768.1321\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 479.0373 - mae: 479.0373 - val_loss: 363.7081 - val_mae: 363.7081\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 380.9041 - mae: 380.9041 - val_loss: 309.1804 - val_mae: 309.1804\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 267.3694 - mae: 267.3694 - val_loss: 313.9223 - val_mae: 313.9223\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 311.0589 - mae: 311.0589 - val_loss: 374.4903 - val_mae: 374.4903\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 274.8504 - mae: 274.8504 - val_loss: 271.0845 - val_mae: 271.0845\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 265.0187 - mae: 265.0187 - val_loss: 283.0406 - val_mae: 283.0406\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 256.0036 - mae: 256.0036 - val_loss: 266.4281 - val_mae: 266.4281\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 263.6881 - mae: 263.6881 - val_loss: 255.5882 - val_mae: 255.5882\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 269.9277 - mae: 269.9277 - val_loss: 255.0604 - val_mae: 255.0604\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 253.6220 - mae: 253.6220 - val_loss: 243.7174 - val_mae: 243.7174\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 248.7410 - mae: 248.7410 - val_loss: 297.6272 - val_mae: 297.6272\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 259.4736 - mae: 259.4736 - val_loss: 272.1282 - val_mae: 272.1282\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 260.2046 - mae: 260.2046 - val_loss: 260.6385 - val_mae: 260.6385\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 269.9621 - mae: 269.9621 - val_loss: 253.4625 - val_mae: 253.4625\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 246.4433 - mae: 246.4433\n",
    "    --- Starting trial: run-22\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1656.0427 - mae: 1656.0426 - val_loss: 1590.0323 - val_mae: 1590.0325\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1522.0487 - mae: 1522.0487 - val_loss: 1461.2520 - val_mae: 1461.2520\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1397.5829 - mae: 1397.5829 - val_loss: 1346.5229 - val_mae: 1346.5229\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1285.4709 - mae: 1285.4709 - val_loss: 1237.5023 - val_mae: 1237.5023\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1179.7264 - mae: 1179.7264 - val_loss: 1136.5604 - val_mae: 1136.5604\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1081.7936 - mae: 1081.7936 - val_loss: 1044.2214 - val_mae: 1044.2214\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 990.8118 - mae: 990.8118 - val_loss: 952.0539 - val_mae: 952.0539\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 903.2469 - mae: 903.2469 - val_loss: 870.6990 - val_mae: 870.6990\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 821.2315 - mae: 821.2314 - val_loss: 790.7815 - val_mae: 790.7815\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 742.0007 - mae: 742.0007 - val_loss: 713.6130 - val_mae: 713.6130\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 664.1683 - mae: 664.1683 - val_loss: 633.1598 - val_mae: 633.1598\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 587.0250 - mae: 587.0250 - val_loss: 560.1811 - val_mae: 560.1811\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 513.6300 - mae: 513.6300 - val_loss: 485.0017 - val_mae: 485.0017\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 441.8852 - mae: 441.8852 - val_loss: 420.1088 - val_mae: 420.1088\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 47s 37ms/step - loss: 375.7331 - mae: 375.7331 - val_loss: 348.5530 - val_mae: 348.5530\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 318.8253 - mae: 318.8253 - val_loss: 296.8869 - val_mae: 296.8869\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 288.9293 - mae: 288.9293\n",
    "    --- Starting trial: run-23\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1270.6874 - mae: 1270.6874 - val_loss: 817.3412 - val_mae: 817.3412\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 514.6238 - mae: 514.6238 - val_loss: 332.7230 - val_mae: 332.7230\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 330.9393 - mae: 330.9393 - val_loss: 289.5320 - val_mae: 289.5320\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 305.6551 - mae: 305.6551 - val_loss: 272.3338 - val_mae: 272.3338\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 318.8382 - mae: 318.8382 - val_loss: 311.9646 - val_mae: 311.9646\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 320.2411 - mae: 320.2411 - val_loss: 289.1891 - val_mae: 289.1891\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 319.9498 - mae: 319.9498 - val_loss: 305.3398 - val_mae: 305.3398\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 328.4742 - mae: 328.4742 - val_loss: 338.3555 - val_mae: 338.3555\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 357.2329 - mae: 357.2329 - val_loss: 463.6325 - val_mae: 463.6325\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 313.1739 - mae: 313.1739 - val_loss: 311.7986 - val_mae: 311.7986\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 318.0271 - mae: 318.0271 - val_loss: 332.7972 - val_mae: 332.7972\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 393.2340 - mae: 393.2340 - val_loss: 427.8026 - val_mae: 427.8026\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 380.4380 - mae: 380.4380 - val_loss: 297.6084 - val_mae: 297.6084\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 363.9304 - mae: 363.9304 - val_loss: 676.5594 - val_mae: 676.5594\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 331.8482 - mae: 331.8482 - val_loss: 332.9341 - val_mae: 332.9341\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 424.4282 - mae: 424.4282 - val_loss: 365.9059 - val_mae: 365.9059\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 359.0834 - mae: 359.0834\n",
    "    --- Starting trial: run-24\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.2584 - mae: 1715.2584 - val_loss: 1700.2073 - val_mae: 1700.2073\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1681.3557 - mae: 1681.3557 - val_loss: 1669.0344 - val_mae: 1669.0344\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1651.5637 - mae: 1651.5637 - val_loss: 1640.7041 - val_mae: 1640.7042\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1623.3107 - mae: 1623.3107 - val_loss: 1612.4390 - val_mae: 1612.4390\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1579.3289 - mae: 1579.3287 - val_loss: 1560.5695 - val_mae: 1560.5695\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1539.1229 - mae: 1539.1229 - val_loss: 1525.8325 - val_mae: 1525.8325\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1505.3556 - mae: 1505.3556 - val_loss: 1492.5293 - val_mae: 1492.5293\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1471.8590 - mae: 1471.8590 - val_loss: 1460.6884 - val_mae: 1460.6884\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1438.7396 - mae: 1438.7396 - val_loss: 1428.2408 - val_mae: 1428.2408\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.4236 - mae: 1407.4236 - val_loss: 1398.6946 - val_mae: 1398.6946\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1377.0978 - mae: 1377.0978 - val_loss: 1369.6388 - val_mae: 1369.6388\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1345.9254 - mae: 1345.9254 - val_loss: 1336.8884 - val_mae: 1336.8884\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1316.8832 - mae: 1316.8832 - val_loss: 1307.7303 - val_mae: 1307.7303\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1286.9031 - mae: 1286.9031 - val_loss: 1278.4175 - val_mae: 1278.4175\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1258.3693 - mae: 1258.3693 - val_loss: 1250.3518 - val_mae: 1250.3518\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1230.5686 - mae: 1230.5686 - val_loss: 1224.7064 - val_mae: 1224.7064\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1205.9163 - mae: 1205.9163\n",
    "    --- Starting trial: run-25\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1592.4969 - mae: 1592.4969 - val_loss: 1480.7988 - val_mae: 1480.7988\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1395.1003 - mae: 1395.1003 - val_loss: 1340.6558 - val_mae: 1340.6558\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1171.8480 - mae: 1171.8480 - val_loss: 1153.9695 - val_mae: 1153.9695\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1016.3562 - mae: 1016.3562 - val_loss: 963.1915 - val_mae: 963.1915\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 915.4506 - mae: 915.4506 - val_loss: 891.3950 - val_mae: 891.3950\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 780.9587 - mae: 780.9587 - val_loss: 733.4357 - val_mae: 733.4357\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 625.6290 - mae: 625.6290 - val_loss: 519.7130 - val_mae: 519.7130\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 581.6451 - mae: 581.6451 - val_loss: 631.0461 - val_mae: 631.0461\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 726.2726 - mae: 726.2726 - val_loss: 614.3286 - val_mae: 614.3286\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 437.8047 - mae: 437.8047 - val_loss: 364.6286 - val_mae: 364.6286\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 357.4409 - mae: 357.4409 - val_loss: 329.8905 - val_mae: 329.8905\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 284.4225 - mae: 284.4225 - val_loss: 339.6268 - val_mae: 339.6268\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.3744 - mae: 313.3744 - val_loss: 318.4585 - val_mae: 318.4585\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 516.9385 - mae: 516.9385 - val_loss: 583.9651 - val_mae: 583.9651\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 601.5540 - mae: 601.5540 - val_loss: 639.3000 - val_mae: 639.3000\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 555.8469 - mae: 555.8469 - val_loss: 570.8818 - val_mae: 570.8818\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 559.4802 - mae: 559.4802\n",
    "    --- Starting trial: run-26\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1716.0641 - mae: 1716.0641 - val_loss: 1701.7858 - val_mae: 1701.7858\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1680.3126 - mae: 1680.3126 - val_loss: 1664.3309 - val_mae: 1664.3309\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1643.3097 - mae: 1643.3097 - val_loss: 1630.5006 - val_mae: 1630.5006\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1607.8269 - mae: 1607.8269 - val_loss: 1595.0868 - val_mae: 1595.0868\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1573.8398 - mae: 1573.8398 - val_loss: 1560.7511 - val_mae: 1560.7511\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1538.8947 - mae: 1538.8947 - val_loss: 1526.4377 - val_mae: 1526.4377\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1505.6060 - mae: 1505.6060 - val_loss: 1495.1146 - val_mae: 1495.1146\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1472.7217 - mae: 1472.7217 - val_loss: 1460.9833 - val_mae: 1460.9833\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1440.0582 - mae: 1440.0582 - val_loss: 1429.7504 - val_mae: 1429.7504\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1409.5823 - mae: 1409.5823 - val_loss: 1398.1605 - val_mae: 1398.1605\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1388.9139 - mae: 1388.9139 - val_loss: 1367.7373 - val_mae: 1367.7373\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1351.1188 - mae: 1351.1188 - val_loss: 1339.1876 - val_mae: 1339.1876\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1318.9459 - mae: 1318.9459 - val_loss: 1310.9557 - val_mae: 1310.9557\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1289.9760 - mae: 1289.9760 - val_loss: 1278.6902 - val_mae: 1278.6902\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1261.5355 - mae: 1261.5355 - val_loss: 1251.9552 - val_mae: 1251.9552\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1233.4868 - mae: 1233.4868 - val_loss: 1223.1542 - val_mae: 1223.1542\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1208.6609 - mae: 1208.6609\n",
    "    --- Starting trial: run-27\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1567.7668 - mae: 1567.7668 - val_loss: 1406.9543 - val_mae: 1406.9543\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1264.6661 - mae: 1264.6661 - val_loss: 1143.7679 - val_mae: 1143.7679\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1029.7506 - mae: 1029.7506 - val_loss: 933.7031 - val_mae: 933.7031\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 847.8713 - mae: 847.8713 - val_loss: 776.5291 - val_mae: 776.5291\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 706.0790 - mae: 706.0790 - val_loss: 607.7781 - val_mae: 607.7781\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 510.8934 - mae: 510.8934 - val_loss: 418.7554 - val_mae: 418.7554\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 371.4506 - mae: 371.4506 - val_loss: 371.4606 - val_mae: 371.4606\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 532.1647 - mae: 532.1647 - val_loss: 349.2210 - val_mae: 349.2210\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 376.1600 - mae: 376.1600 - val_loss: 345.0703 - val_mae: 345.0703\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 411.3190 - mae: 411.3190 - val_loss: 401.2716 - val_mae: 401.2716\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 374.1356 - mae: 374.1356 - val_loss: 317.7419 - val_mae: 317.7419\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 403.0447 - mae: 403.0447 - val_loss: 327.2177 - val_mae: 327.2177\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 416.4857 - mae: 416.4857 - val_loss: 415.2970 - val_mae: 415.2970\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 448.0333 - mae: 448.0333 - val_loss: 414.7188 - val_mae: 414.7188\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 447.3336 - mae: 447.3336 - val_loss: 422.6076 - val_mae: 422.6076\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 517.6610 - mae: 517.6610 - val_loss: 509.3231 - val_mae: 509.3231\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 503.8761 - mae: 503.8761\n",
    "    --- Starting trial: run-28\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1696.7650 - mae: 1696.7650 - val_loss: 1668.9084 - val_mae: 1668.9084\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1628.7839 - mae: 1628.7839 - val_loss: 1593.6188 - val_mae: 1593.6188\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1556.1368 - mae: 1556.1368 - val_loss: 1526.3639 - val_mae: 1526.3639\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.2396 - mae: 1490.2396 - val_loss: 1461.8344 - val_mae: 1461.8344\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1426.0570 - mae: 1426.0570 - val_loss: 1402.6639 - val_mae: 1402.6639\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1366.0361 - mae: 1366.0361 - val_loss: 1343.7756 - val_mae: 1343.7756\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1308.5929 - mae: 1308.5929 - val_loss: 1286.7965 - val_mae: 1286.7965\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1251.5659 - mae: 1251.5659 - val_loss: 1233.4119 - val_mae: 1233.4119\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1197.6721 - mae: 1197.6721 - val_loss: 1180.1956 - val_mae: 1180.1956\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1145.1334 - mae: 1145.1334 - val_loss: 1128.7006 - val_mae: 1128.7006\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1095.2146 - mae: 1095.2146 - val_loss: 1081.0718 - val_mae: 1081.0718\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1046.3492 - mae: 1046.3492 - val_loss: 1033.7733 - val_mae: 1033.7733\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1001.0187 - mae: 1001.0187 - val_loss: 987.9163 - val_mae: 987.9163\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 955.1597 - mae: 955.1597 - val_loss: 945.1874 - val_mae: 945.1874\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 911.0806 - mae: 911.0806 - val_loss: 899.4464 - val_mae: 899.4464\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 867.8735 - mae: 867.8735 - val_loss: 858.7758 - val_mae: 858.7758\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 842.9398 - mae: 842.9398\n",
    "    --- Starting trial: run-29\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1471.6891 - mae: 1471.6891 - val_loss: 1200.9448 - val_mae: 1200.9448\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 978.2620 - mae: 978.2620 - val_loss: 786.5012 - val_mae: 786.5012\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 776.1443 - mae: 776.1443 - val_loss: 789.0593 - val_mae: 789.0593\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 656.6492 - mae: 656.6492 - val_loss: 644.4000 - val_mae: 644.4000\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 502.4906 - mae: 502.4906 - val_loss: 356.0527 - val_mae: 356.0527\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 389.9354 - mae: 389.9354 - val_loss: 413.3930 - val_mae: 413.3930\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 487.8478 - mae: 487.8478 - val_loss: 503.7323 - val_mae: 503.7323\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 500.8051 - mae: 500.8051 - val_loss: 503.9304 - val_mae: 503.9304\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 896.9134 - mae: 896.9134 - val_loss: 848.4318 - val_mae: 848.4318\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 694.9317 - mae: 694.9317 - val_loss: 598.9346 - val_mae: 598.9346\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 507.5535 - mae: 507.5535 - val_loss: 523.3628 - val_mae: 523.3628\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 501.6664 - mae: 501.6664 - val_loss: 497.1284 - val_mae: 497.1284\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 488.8983 - mae: 488.8983 - val_loss: 490.1184 - val_mae: 490.1184\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 487.4877 - mae: 487.4877 - val_loss: 490.1988 - val_mae: 490.1988\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 459.9962 - mae: 459.9962 - val_loss: 417.2057 - val_mae: 417.2057\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 414.3864 - mae: 414.3864 - val_loss: 416.7916 - val_mae: 416.7916\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 410.6290 - mae: 410.6290\n",
    "    --- Starting trial: run-30\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1697.3124 - mae: 1697.3124 - val_loss: 1669.5159 - val_mae: 1669.5159\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1628.5945 - mae: 1628.5945 - val_loss: 1595.8706 - val_mae: 1595.8706\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1557.4340 - mae: 1557.4340 - val_loss: 1527.5540 - val_mae: 1527.5540\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.7546 - mae: 1490.7546 - val_loss: 1463.7521 - val_mae: 1463.7521\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1427.8077 - mae: 1427.8077 - val_loss: 1401.4994 - val_mae: 1401.4994\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1367.8618 - mae: 1367.8618 - val_loss: 1340.8381 - val_mae: 1340.8381\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1309.7174 - mae: 1309.7174 - val_loss: 1287.9558 - val_mae: 1287.9558\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1254.0361 - mae: 1254.0361 - val_loss: 1232.8689 - val_mae: 1232.8689\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1199.3678 - mae: 1199.3678 - val_loss: 1179.3184 - val_mae: 1179.3184\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1147.4644 - mae: 1147.4644 - val_loss: 1125.9730 - val_mae: 1125.9730\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1097.3810 - mae: 1097.3810 - val_loss: 1077.9038 - val_mae: 1077.9038\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1049.7014 - mae: 1049.7014 - val_loss: 1032.2191 - val_mae: 1032.2191\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1002.9841 - mae: 1002.9841 - val_loss: 985.7662 - val_mae: 985.7662\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 1178.7196 - mae: 1178.7196 - val_loss: 1134.6793 - val_mae: 1134.6793\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 1080.5533 - mae: 1080.5533 - val_loss: 1040.9648 - val_mae: 1040.9648\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 990.7078 - mae: 990.7078 - val_loss: 955.0836 - val_mae: 955.0836\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 903.3972 - mae: 903.3972 - val_loss: 869.0078 - val_mae: 869.0078\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 820.5344 - mae: 820.5344 - val_loss: 786.7252 - val_mae: 786.7252\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 55s 43ms/step - loss: 741.6986 - mae: 741.6987 - val_loss: 710.1413 - val_mae: 710.1413\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 663.7181 - mae: 663.7181 - val_loss: 632.6093 - val_mae: 632.6093\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 586.7770 - mae: 586.7770 - val_loss: 556.9543 - val_mae: 556.9543\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 513.3420 - mae: 513.3420 - val_loss: 484.5912 - val_mae: 484.5912\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 441.1182 - mae: 441.1182 - val_loss: 411.9660 - val_mae: 411.9660\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 55s 44ms/step - loss: 375.5436 - mae: 375.5436 - val_loss: 346.7868 - val_mae: 346.7868\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 316.7939 - mae: 316.7939 - val_loss: 291.4937 - val_mae: 291.4937\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 283.1966 - mae: 283.1966\n",
    "    --- Starting trial: run-35\n",
    "    {'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 57s 43ms/step - loss: 1232.3571 - mae: 1232.3571 - val_loss: 783.3661 - val_mae: 783.3661\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 497.3005 - mae: 497.3005 - val_loss: 315.7192 - val_mae: 315.7192\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 286.1610 - mae: 286.1610 - val_loss: 312.7931 - val_mae: 312.7931\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 286.3627 - mae: 286.3627 - val_loss: 269.8783 - val_mae: 269.8783\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 281.8381 - mae: 281.8381 - val_loss: 272.8588 - val_mae: 272.8588\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 318.1495 - mae: 318.1495 - val_loss: 313.0517 - val_mae: 313.0517\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 333.5344 - mae: 333.5344 - val_loss: 304.4086 - val_mae: 304.4086\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 297.0162 - mae: 297.0162 - val_loss: 390.3582 - val_mae: 390.3582\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 307.0529 - mae: 307.0529 - val_loss: 476.4601 - val_mae: 476.4601\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 296.0189 - mae: 296.0189 - val_loss: 366.3567 - val_mae: 366.3567\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 322.2481 - mae: 322.2481 - val_loss: 526.5839 - val_mae: 526.5839\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 459.0078 - mae: 459.0078 - val_loss: 565.0197 - val_mae: 565.0197\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 405.0327 - mae: 405.0327 - val_loss: 375.6803 - val_mae: 375.6803\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 454.1462 - mae: 454.1462 - val_loss: 978.4724 - val_mae: 978.4724\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 600.6526 - mae: 600.6526 - val_loss: 426.8247 - val_mae: 426.8247\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 54s 43ms/step - loss: 435.1583 - mae: 435.1583 - val_loss: 437.0663 - val_mae: 437.0663\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 432.2417 - mae: 432.2417\n",
    "    --- Starting trial: run-36\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1717.2101 - mae: 1717.2101 - val_loss: 1702.2744 - val_mae: 1702.2744\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1681.4932 - mae: 1681.4932 - val_loss: 1665.9336 - val_mae: 1665.9336\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1646.9165 - mae: 1646.9165 - val_loss: 1632.1249 - val_mae: 1632.1249\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1613.0847 - mae: 1613.0847 - val_loss: 1600.9336 - val_mae: 1600.9336\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1581.1233 - mae: 1581.1233 - val_loss: 1569.0088 - val_mae: 1569.0088\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1549.0660 - mae: 1549.0660 - val_loss: 1536.2501 - val_mae: 1536.2501\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1516.9631 - mae: 1516.9631 - val_loss: 1503.3112 - val_mae: 1503.3112\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1485.6986 - mae: 1485.6986 - val_loss: 1472.5837 - val_mae: 1472.5837\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1454.6257 - mae: 1454.6257 - val_loss: 1445.9840 - val_mae: 1445.9840\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1424.5015 - mae: 1424.5015 - val_loss: 1415.4459 - val_mae: 1415.4459\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1395.4667 - mae: 1395.4667 - val_loss: 1386.7228 - val_mae: 1386.7228\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1374.3759 - mae: 1374.3759 - val_loss: 1367.5306 - val_mae: 1367.5306\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1349.2841 - mae: 1349.2841 - val_loss: 1342.1028 - val_mae: 1342.1028\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1322.0649 - mae: 1322.0649 - val_loss: 1315.7899 - val_mae: 1315.7899\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1294.8787 - mae: 1294.8787 - val_loss: 1289.1993 - val_mae: 1289.1993\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1269.2714 - mae: 1269.2714 - val_loss: 1263.8138 - val_mae: 1263.8138\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1247.0769 - mae: 1247.0769\n",
    "    --- Starting trial: run-37\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1611.3224 - mae: 1611.3224 - val_loss: 1499.4774 - val_mae: 1499.4773\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1387.1478 - mae: 1387.1478 - val_loss: 1292.5542 - val_mae: 1292.5542\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1196.3809 - mae: 1196.3809 - val_loss: 1116.8970 - val_mae: 1116.8970\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1028.5817 - mae: 1028.5817 - val_loss: 959.9434 - val_mae: 959.9434\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 878.6913 - mae: 878.6913 - val_loss: 818.9588 - val_mae: 818.9588\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 771.8566 - mae: 771.8566 - val_loss: 743.7043 - val_mae: 743.7043\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 660.9505 - mae: 660.9505 - val_loss: 608.5946 - val_mae: 608.5946\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 532.9277 - mae: 532.9277 - val_loss: 529.6520 - val_mae: 529.6520\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 444.4644 - mae: 444.4644 - val_loss: 396.4323 - val_mae: 396.4323\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 363.3088 - mae: 363.3088 - val_loss: 335.2719 - val_mae: 335.2719\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 325.1737 - mae: 325.1737 - val_loss: 305.7590 - val_mae: 305.7590\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 246.6276 - mae: 246.6276 - val_loss: 233.7120 - val_mae: 233.7120\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 239.5803 - mae: 239.5803 - val_loss: 248.2155 - val_mae: 248.2155\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 224.4225 - mae: 224.4225 - val_loss: 228.9791 - val_mae: 228.9791\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 254.3052 - mae: 254.3052 - val_loss: 236.1215 - val_mae: 236.1215\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 241.6220 - mae: 241.6220 - val_loss: 239.1614 - val_mae: 239.1614\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 231.1915 - mae: 231.1915\n",
    "    --- Starting trial: run-38\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1715.2869 - mae: 1715.2869 - val_loss: 1698.2845 - val_mae: 1698.2845\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1676.3069 - mae: 1676.3069 - val_loss: 1660.4004 - val_mae: 1660.4004\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1639.7496 - mae: 1639.7496 - val_loss: 1624.7610 - val_mae: 1624.7610\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1605.3876 - mae: 1605.3876 - val_loss: 1590.8740 - val_mae: 1590.8740\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1571.2634 - mae: 1571.2634 - val_loss: 1557.4092 - val_mae: 1557.4092\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1536.8989 - mae: 1536.8989 - val_loss: 1524.5914 - val_mae: 1524.5914\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1502.9780 - mae: 1502.9780 - val_loss: 1491.7908 - val_mae: 1491.7908\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1470.3958 - mae: 1470.3958 - val_loss: 1456.8350 - val_mae: 1456.8350\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1437.9915 - mae: 1437.9915 - val_loss: 1425.9243 - val_mae: 1425.9243\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.0947 - mae: 1407.0947 - val_loss: 1394.1692 - val_mae: 1394.1692\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1376.3447 - mae: 1376.3447 - val_loss: 1365.2189 - val_mae: 1365.2189\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1346.2587 - mae: 1346.2587 - val_loss: 1335.6074 - val_mae: 1335.6074\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1317.1383 - mae: 1317.1383 - val_loss: 1306.1637 - val_mae: 1306.1637\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1288.7775 - mae: 1288.7775 - val_loss: 1290.1830 - val_mae: 1290.1830\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1272.1890 - mae: 1272.1890 - val_loss: 1260.7997 - val_mae: 1260.7997\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1244.9976 - mae: 1244.9976 - val_loss: 1236.9117 - val_mae: 1236.9117\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1218.4209 - mae: 1218.4209\n",
    "    --- Starting trial: run-39\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1585.4639 - mae: 1585.4639 - val_loss: 1449.8101 - val_mae: 1449.8101\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1326.8323 - mae: 1326.8323 - val_loss: 1219.2919 - val_mae: 1219.2919\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1116.0609 - mae: 1116.0609 - val_loss: 1021.7063 - val_mae: 1021.7063\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 935.3168 - mae: 935.3168 - val_loss: 847.9759 - val_mae: 847.9758\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 775.4478 - mae: 775.4478 - val_loss: 690.9235 - val_mae: 690.9235\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 626.6030 - mae: 626.6030 - val_loss: 558.0425 - val_mae: 558.0425\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 498.8426 - mae: 498.8426 - val_loss: 419.3124 - val_mae: 419.3124\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 384.6506 - mae: 384.6506 - val_loss: 347.4950 - val_mae: 347.4950\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 341.1142 - mae: 341.1142 - val_loss: 284.7704 - val_mae: 284.7704\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 307.8639 - mae: 307.8639 - val_loss: 241.8632 - val_mae: 241.8632\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 296.6828 - mae: 296.6828 - val_loss: 228.1328 - val_mae: 228.1328\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 311.5308 - mae: 311.5308 - val_loss: 251.6422 - val_mae: 251.6422\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.3402 - mae: 313.3402 - val_loss: 228.2218 - val_mae: 228.2218\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 295.0386 - mae: 295.0386 - val_loss: 223.6129 - val_mae: 223.6129\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 317.4360 - mae: 317.4360 - val_loss: 236.7501 - val_mae: 236.7501\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 325.2464 - mae: 325.2464 - val_loss: 279.0769 - val_mae: 279.0769\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 271.2676 - mae: 271.2676\n",
    "    --- Starting trial: run-40\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1695.5148 - mae: 1695.5148 - val_loss: 1662.8513 - val_mae: 1662.8513\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1626.9287 - mae: 1626.9287 - val_loss: 1600.3616 - val_mae: 1600.3616\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1564.5111 - mae: 1564.5111 - val_loss: 1540.4408 - val_mae: 1540.4408\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1503.3486 - mae: 1503.3486 - val_loss: 1478.1812 - val_mae: 1478.1812\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1443.7743 - mae: 1443.7743 - val_loss: 1419.5554 - val_mae: 1419.5554\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1387.6229 - mae: 1387.6229 - val_loss: 1365.7874 - val_mae: 1365.7874\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1332.5813 - mae: 1332.5813 - val_loss: 1311.7603 - val_mae: 1311.7603\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1279.4694 - mae: 1279.4694 - val_loss: 1261.5223 - val_mae: 1261.5223\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1227.0084 - mae: 1227.0084 - val_loss: 1210.1066 - val_mae: 1210.1066\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1178.0132 - mae: 1178.0132 - val_loss: 1161.7855 - val_mae: 1161.7855\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1129.8492 - mae: 1129.8492 - val_loss: 1113.4271 - val_mae: 1113.4271\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1083.3160 - mae: 1083.3160 - val_loss: 1071.1749 - val_mae: 1071.1749\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1038.5382 - mae: 1038.5382 - val_loss: 1026.0815 - val_mae: 1026.0815\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 995.6134 - mae: 995.6134 - val_loss: 985.9888 - val_mae: 985.9888\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 953.0128 - mae: 953.0128 - val_loss: 943.3952 - val_mae: 943.3952\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 911.9023 - mae: 911.9023 - val_loss: 904.7714 - val_mae: 904.7714\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 889.4129 - mae: 889.4129\n",
    "    --- Starting trial: run-41\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1493.8533 - mae: 1493.8533 - val_loss: 1287.2686 - val_mae: 1287.2686\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1116.1770 - mae: 1116.1770 - val_loss: 975.8002 - val_mae: 975.8002\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 831.2684 - mae: 831.2684 - val_loss: 713.1877 - val_mae: 713.1877\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 574.3075 - mae: 574.3075 - val_loss: 460.3633 - val_mae: 460.3633\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 356.9521 - mae: 356.9521 - val_loss: 315.4146 - val_mae: 315.4146\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 266.4378 - mae: 266.4378 - val_loss: 260.4943 - val_mae: 260.4943\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 223.5232 - mae: 223.5232 - val_loss: 229.5172 - val_mae: 229.5172\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 218.9276 - mae: 218.9276 - val_loss: 223.6136 - val_mae: 223.6136\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 214.6121 - mae: 214.6121 - val_loss: 220.9611 - val_mae: 220.9611\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 216.9231 - mae: 216.9231 - val_loss: 221.3165 - val_mae: 221.3165\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 216.4179 - mae: 216.4179 - val_loss: 250.1006 - val_mae: 250.1006\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 219.0348 - mae: 219.0348 - val_loss: 224.7102 - val_mae: 224.7102\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 220.1164 - mae: 220.1164 - val_loss: 243.0972 - val_mae: 243.0972\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 218.7108 - mae: 218.7108 - val_loss: 225.7091 - val_mae: 225.7091\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 230.5063 - mae: 230.5063 - val_loss: 224.8069 - val_mae: 224.8069\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 221.1390 - mae: 221.1390 - val_loss: 226.4967 - val_mae: 226.4967\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 219.0990 - mae: 219.0990\n",
    "    --- Starting trial: run-42\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1696.5499 - mae: 1696.5499 - val_loss: 1662.5385 - val_mae: 1662.5385\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1627.2061 - mae: 1627.2061 - val_loss: 1595.7772 - val_mae: 1595.7772\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1561.1919 - mae: 1561.1919 - val_loss: 1533.8206 - val_mae: 1533.8206\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1498.3416 - mae: 1498.3416 - val_loss: 1472.3705 - val_mae: 1472.3705\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1437.7375 - mae: 1437.7375 - val_loss: 1412.9568 - val_mae: 1412.9568\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1378.6720 - mae: 1378.6720 - val_loss: 1354.6799 - val_mae: 1354.6799\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1322.0428 - mae: 1322.0428 - val_loss: 1298.6385 - val_mae: 1298.6385\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1266.8304 - mae: 1266.8304 - val_loss: 1244.8041 - val_mae: 1244.8041\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1214.1224 - mae: 1214.1224 - val_loss: 1193.2323 - val_mae: 1193.2323\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1162.5565 - mae: 1162.5565 - val_loss: 1141.9067 - val_mae: 1141.9067\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1114.1790 - mae: 1114.1790 - val_loss: 1093.0789 - val_mae: 1093.0789\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1067.4868 - mae: 1067.4868 - val_loss: 1048.0905 - val_mae: 1048.0905\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1022.0480 - mae: 1022.0480 - val_loss: 1003.9880 - val_mae: 1003.9880\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 977.1545 - mae: 977.1545 - val_loss: 958.0012 - val_mae: 958.0012\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 933.5703 - mae: 933.5703 - val_loss: 916.3241 - val_mae: 916.3241\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 891.2424 - mae: 891.2424 - val_loss: 872.8399 - val_mae: 872.8399\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 861.5611 - mae: 861.5611\n",
    "    --- Starting trial: run-43\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1459.8247 - mae: 1459.8247 - val_loss: 1223.8055 - val_mae: 1223.8055\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1035.9802 - mae: 1035.9802 - val_loss: 872.6981 - val_mae: 872.6981\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 723.4987 - mae: 723.4987 - val_loss: 579.1658 - val_mae: 579.1658\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 452.6002 - mae: 452.6002 - val_loss: 339.4690 - val_mae: 339.4690\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 302.7859 - mae: 302.7859 - val_loss: 240.8830 - val_mae: 240.8830\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 275.8090 - mae: 275.8090 - val_loss: 227.5034 - val_mae: 227.5034\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 280.4904 - mae: 280.4904 - val_loss: 230.2850 - val_mae: 230.2850\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 271.3615 - mae: 271.3615 - val_loss: 217.9926 - val_mae: 217.9926\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 281.5079 - mae: 281.5079 - val_loss: 226.8006 - val_mae: 226.8006\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 293.2867 - mae: 293.2867 - val_loss: 234.4205 - val_mae: 234.4205\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 287.6664 - mae: 287.6664 - val_loss: 211.8952 - val_mae: 211.8952\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 301.7506 - mae: 301.7506 - val_loss: 247.1399 - val_mae: 247.1399\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 304.3917 - mae: 304.3917 - val_loss: 251.6075 - val_mae: 251.6075\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 316.4518 - mae: 316.4518 - val_loss: 258.4073 - val_mae: 258.4073\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 320.3954 - mae: 320.3954 - val_loss: 249.4064 - val_mae: 249.4064\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 321.5111 - mae: 321.5111 - val_loss: 241.1985 - val_mae: 241.1985\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 233.6783 - mae: 233.6783\n",
    "    --- Starting trial: run-44\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1658.6611 - mae: 1658.6611 - val_loss: 1596.2274 - val_mae: 1596.2274\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1532.4823 - mae: 1532.4823 - val_loss: 1478.1375 - val_mae: 1478.1375\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1416.1304 - mae: 1416.1304 - val_loss: 1366.5580 - val_mae: 1366.5580\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1310.2621 - mae: 1310.2621 - val_loss: 1266.9862 - val_mae: 1266.9862\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1209.6355 - mae: 1209.6355 - val_loss: 1171.2717 - val_mae: 1171.2717\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1116.1415 - mae: 1116.1415 - val_loss: 1081.3884 - val_mae: 1081.3884\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1029.4832 - mae: 1029.4832 - val_loss: 997.9949 - val_mae: 997.9949\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 946.7191 - mae: 946.7191 - val_loss: 917.0673 - val_mae: 917.0673\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 867.1848 - mae: 867.1848 - val_loss: 840.4197 - val_mae: 840.4197\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 791.6609 - mae: 791.6609 - val_loss: 766.9932 - val_mae: 766.9932\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 720.2026 - mae: 720.2026 - val_loss: 696.8099 - val_mae: 696.8099\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 649.1319 - mae: 649.1319 - val_loss: 626.9329 - val_mae: 626.9329\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 578.9605 - mae: 578.9605 - val_loss: 559.7176 - val_mae: 559.7176\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 514.1655 - mae: 514.1655 - val_loss: 506.8719 - val_mae: 506.8719\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 462.1168 - mae: 462.1168 - val_loss: 458.2577 - val_mae: 458.2577\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 411.0039 - mae: 411.0039 - val_loss: 385.6982 - val_mae: 385.6982\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 375.5765 - mae: 375.5765\n",
    "    --- Starting trial: run-45\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1301.4998 - mae: 1301.4998 - val_loss: 974.3832 - val_mae: 974.3832\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 743.1517 - mae: 743.1517 - val_loss: 538.9504 - val_mae: 538.9504\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 354.5157 - mae: 354.5157 - val_loss: 281.5479 - val_mae: 281.5479\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 227.7712 - mae: 227.7712 - val_loss: 230.4115 - val_mae: 230.4115\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 215.0101 - mae: 215.0101 - val_loss: 236.0885 - val_mae: 236.0885\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 212.4322 - mae: 212.4322 - val_loss: 219.3351 - val_mae: 219.3351\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 206.2629 - mae: 206.2629 - val_loss: 212.2724 - val_mae: 212.2724\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 208.5021 - mae: 208.5021 - val_loss: 230.5046 - val_mae: 230.5046\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 208.5500 - mae: 208.5500 - val_loss: 209.0724 - val_mae: 209.0724\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 195.6858 - mae: 195.6858 - val_loss: 195.0979 - val_mae: 195.0979\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 191.0587 - mae: 191.0587 - val_loss: 199.2634 - val_mae: 199.2634\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 187.6154 - mae: 187.6154 - val_loss: 233.6762 - val_mae: 233.6762\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 209.0860 - mae: 209.0860 - val_loss: 217.7801 - val_mae: 217.7801\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 220.1286 - mae: 220.1286 - val_loss: 230.8294 - val_mae: 230.8294\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 221.4493 - mae: 221.4493 - val_loss: 228.1542 - val_mae: 228.1542\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 228.5196 - mae: 228.5196 - val_loss: 243.7557 - val_mae: 243.7557\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 236.1976 - mae: 236.1976\n",
    "    --- Starting trial: run-46\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1658.5260 - mae: 1658.5260 - val_loss: 1597.6737 - val_mae: 1597.6737\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1531.5337 - mae: 1531.5337 - val_loss: 1471.7532 - val_mae: 1471.7532\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1410.5439 - mae: 1410.5439 - val_loss: 1357.5292 - val_mae: 1357.5292\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1299.5779 - mae: 1299.5779 - val_loss: 1253.6926 - val_mae: 1253.6926\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1196.8199 - mae: 1196.8199 - val_loss: 1152.6139 - val_mae: 1152.6139\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1102.1105 - mae: 1102.1105 - val_loss: 1060.9233 - val_mae: 1060.9233\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1013.0812 - mae: 1013.0812 - val_loss: 976.4382 - val_mae: 976.4382\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 928.9305 - mae: 928.9305 - val_loss: 892.9048 - val_mae: 892.9048\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 847.9688 - mae: 847.9688 - val_loss: 813.4162 - val_mae: 813.4162\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 774.0177 - mae: 774.0177 - val_loss: 743.8135 - val_mae: 743.8135\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 702.6194 - mae: 702.6195 - val_loss: 670.4092 - val_mae: 670.4092\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 628.5816 - mae: 628.5816 - val_loss: 597.4449 - val_mae: 597.4449\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 559.0278 - mae: 559.0278 - val_loss: 534.7897 - val_mae: 534.7897\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 495.0808 - mae: 495.0808 - val_loss: 465.4654 - val_mae: 465.4654\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 428.1409 - mae: 428.1409 - val_loss: 398.1586 - val_mae: 398.1586\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 366.6304 - mae: 366.6304 - val_loss: 336.1463 - val_mae: 336.1463\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 328.1187 - mae: 328.1187\n",
    "    --- Starting trial: run-47\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 46s 35ms/step - loss: 1277.4482 - mae: 1277.4482 - val_loss: 912.3274 - val_mae: 912.3274\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 651.0007 - mae: 651.0007 - val_loss: 405.2114 - val_mae: 405.2114\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 295.3008 - mae: 295.3008 - val_loss: 210.4671 - val_mae: 210.4671\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 246.5305 - mae: 246.5305 - val_loss: 220.3249 - val_mae: 220.3249\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 251.0187 - mae: 251.0187 - val_loss: 234.6508 - val_mae: 234.6508\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 260.0192 - mae: 260.0192 - val_loss: 229.4008 - val_mae: 229.4008\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 266.8639 - mae: 266.8639 - val_loss: 215.5391 - val_mae: 215.5391\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 263.3572 - mae: 263.3572 - val_loss: 218.4511 - val_mae: 218.4511\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 272.1925 - mae: 272.1925 - val_loss: 209.1567 - val_mae: 209.1567\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 287.9868 - mae: 287.9868 - val_loss: 248.9987 - val_mae: 248.9987\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 295.4473 - mae: 295.4473 - val_loss: 236.4415 - val_mae: 236.4415\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 304.6299 - mae: 304.6299 - val_loss: 237.2237 - val_mae: 237.2237\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 319.2805 - mae: 319.2805 - val_loss: 248.2975 - val_mae: 248.2975\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 317.8445 - mae: 317.8445 - val_loss: 239.5460 - val_mae: 239.5460\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 320.9797 - mae: 320.9797 - val_loss: 247.0933 - val_mae: 247.0933\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 316.0599 - mae: 316.0599 - val_loss: 222.7614 - val_mae: 222.7614\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 217.4175 - mae: 217.4175\n",
    "    --- Starting trial: run-48\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1715.9551 - mae: 1715.9551 - val_loss: 1699.8214 - val_mae: 1699.8214\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1679.0156 - mae: 1679.0156 - val_loss: 1664.7561 - val_mae: 1664.7561\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1643.9565 - mae: 1643.9565 - val_loss: 1631.3694 - val_mae: 1631.3694\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1609.8430 - mae: 1609.8430 - val_loss: 1596.0638 - val_mae: 1596.0638\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1576.0675 - mae: 1576.0675 - val_loss: 1564.2960 - val_mae: 1564.2960\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1543.7927 - mae: 1543.7927 - val_loss: 1531.6519 - val_mae: 1531.6519\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1510.3280 - mae: 1510.3280 - val_loss: 1499.4088 - val_mae: 1499.4088\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1477.7925 - mae: 1477.7925 - val_loss: 1466.2664 - val_mae: 1466.2664\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1446.4912 - mae: 1446.4912 - val_loss: 1437.6030 - val_mae: 1437.6030\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1414.9462 - mae: 1414.9462 - val_loss: 1405.4879 - val_mae: 1405.4879\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1385.2902 - mae: 1385.2902 - val_loss: 1375.7491 - val_mae: 1375.7491\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1356.1321 - mae: 1356.1321 - val_loss: 1346.4485 - val_mae: 1346.4485\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1327.4646 - mae: 1327.4646 - val_loss: 1318.2596 - val_mae: 1318.2596\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1298.1749 - mae: 1298.1749 - val_loss: 1293.1355 - val_mae: 1293.1355\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1270.0986 - mae: 1270.0986 - val_loss: 1262.7316 - val_mae: 1262.7316\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1242.5336 - mae: 1242.5336 - val_loss: 1237.7620 - val_mae: 1237.7620\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1221.3234 - mae: 1221.3234\n",
    "    --- Starting trial: run-49\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1590.7982 - mae: 1590.7982 - val_loss: 1480.0334 - val_mae: 1480.0334\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1347.5516 - mae: 1347.5516 - val_loss: 1220.7795 - val_mae: 1220.7795\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1095.9360 - mae: 1095.9360 - val_loss: 973.3386 - val_mae: 973.3386\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 862.7107 - mae: 862.7107 - val_loss: 826.3100 - val_mae: 826.3100\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 728.9100 - mae: 728.9100 - val_loss: 646.0491 - val_mae: 646.0491\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 614.8599 - mae: 614.8599 - val_loss: 814.2495 - val_mae: 814.2495\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 640.9283 - mae: 640.9283 - val_loss: 885.9072 - val_mae: 885.9072\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 761.2232 - mae: 761.2232 - val_loss: 642.6960 - val_mae: 642.6960\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 540.6483 - mae: 540.6483 - val_loss: 521.8348 - val_mae: 521.8348\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 543.3502 - mae: 543.3502 - val_loss: 358.5876 - val_mae: 358.5876\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 281.2585 - mae: 281.2585 - val_loss: 264.4980 - val_mae: 264.4980\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 514.2418 - mae: 514.2418 - val_loss: 738.4224 - val_mae: 738.4224\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 686.8334 - mae: 686.8334 - val_loss: 480.1422 - val_mae: 480.1422\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 376.9387 - mae: 376.9387 - val_loss: 277.1323 - val_mae: 277.1323\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 313.5328 - mae: 313.5328 - val_loss: 286.8384 - val_mae: 286.8384\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 276.2646 - mae: 276.2646 - val_loss: 263.9634 - val_mae: 263.9634\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 257.0282 - mae: 257.0282\n",
    "    --- Starting trial: run-50\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1714.3967 - mae: 1714.3967 - val_loss: 1698.2332 - val_mae: 1698.2332\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1676.7014 - mae: 1676.7014 - val_loss: 1661.6172 - val_mae: 1661.6172\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1641.3059 - mae: 1641.3059 - val_loss: 1625.0699 - val_mae: 1625.0699\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1605.9208 - mae: 1605.9208 - val_loss: 1591.0507 - val_mae: 1591.0507\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1571.8026 - mae: 1571.8026 - val_loss: 1558.4971 - val_mae: 1558.4971\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1537.4042 - mae: 1537.4042 - val_loss: 1525.4352 - val_mae: 1525.4352\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1503.0554 - mae: 1503.0554 - val_loss: 1492.2814 - val_mae: 1492.2814\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1471.5179 - mae: 1471.5179 - val_loss: 1457.4338 - val_mae: 1457.4338\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1438.9426 - mae: 1438.9426 - val_loss: 1428.5422 - val_mae: 1428.5422\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1407.8379 - mae: 1407.8379 - val_loss: 1396.7726 - val_mae: 1396.7726\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1376.8347 - mae: 1376.8347 - val_loss: 1366.3796 - val_mae: 1366.3796\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1347.2869 - mae: 1347.2869 - val_loss: 1334.8883 - val_mae: 1334.8883\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1318.0419 - mae: 1318.0419 - val_loss: 1306.7230 - val_mae: 1306.7230\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1288.4674 - mae: 1288.4674 - val_loss: 1277.4142 - val_mae: 1277.4142\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1260.1821 - mae: 1260.1820 - val_loss: 1250.1243 - val_mae: 1250.1243\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1231.9685 - mae: 1231.9685 - val_loss: 1221.5571 - val_mae: 1221.5571\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1205.3240 - mae: 1205.3240\n",
    "    --- Starting trial: run-51\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1570.5490 - mae: 1570.5490 - val_loss: 1412.1570 - val_mae: 1412.1570\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1273.6757 - mae: 1273.6757 - val_loss: 1153.4180 - val_mae: 1153.4180\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1044.8888 - mae: 1044.8888 - val_loss: 946.6771 - val_mae: 946.6771\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 855.4116 - mae: 855.4116 - val_loss: 782.0162 - val_mae: 782.0162\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 683.1075 - mae: 683.1075 - val_loss: 586.1111 - val_mae: 586.1111\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 514.6006 - mae: 514.6006 - val_loss: 419.7685 - val_mae: 419.7685\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 368.9599 - mae: 368.9599 - val_loss: 303.8125 - val_mae: 303.8125\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 313.2482 - mae: 313.2482 - val_loss: 245.3420 - val_mae: 245.3420\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 297.4578 - mae: 297.4578 - val_loss: 263.6404 - val_mae: 263.6404\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 283.6970 - mae: 283.6970 - val_loss: 235.9379 - val_mae: 235.9379\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 277.9241 - mae: 277.9241 - val_loss: 258.3922 - val_mae: 258.3922\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 274.0790 - mae: 274.0790 - val_loss: 186.7096 - val_mae: 186.7096\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 319.7712 - mae: 319.7712 - val_loss: 302.3244 - val_mae: 302.3244\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 310.0781 - mae: 310.0781 - val_loss: 268.6696 - val_mae: 268.6696\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 281.9485 - mae: 281.9485 - val_loss: 180.2285 - val_mae: 180.2285\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 293.6541 - mae: 293.6541 - val_loss: 267.5230 - val_mae: 267.5230\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 261.1690 - mae: 261.1690\n",
    "    --- Starting trial: run-52\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1695.8501 - mae: 1695.8501 - val_loss: 1663.2485 - val_mae: 1663.2485\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1626.2513 - mae: 1626.2513 - val_loss: 1596.1555 - val_mae: 1596.1555\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1561.0544 - mae: 1561.0544 - val_loss: 1534.0005 - val_mae: 1534.0005\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1495.6290 - mae: 1495.6290 - val_loss: 1467.7705 - val_mae: 1467.7705\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1432.6783 - mae: 1432.6783 - val_loss: 1407.7566 - val_mae: 1407.7566\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1373.1915 - mae: 1373.1915 - val_loss: 1348.9343 - val_mae: 1348.9343\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1315.6998 - mae: 1315.6998 - val_loss: 1294.7925 - val_mae: 1294.7925\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1259.6613 - mae: 1259.6613 - val_loss: 1240.6241 - val_mae: 1240.6241\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1205.4904 - mae: 1205.4904 - val_loss: 1187.2410 - val_mae: 1187.2410\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1154.2507 - mae: 1154.2507 - val_loss: 1137.1331 - val_mae: 1137.1331\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1103.9575 - mae: 1103.9575 - val_loss: 1088.1261 - val_mae: 1088.1261\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1056.1140 - mae: 1056.1140 - val_loss: 1042.2157 - val_mae: 1042.2157\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1009.7570 - mae: 1009.7570 - val_loss: 997.8799 - val_mae: 997.8799\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 964.6146 - mae: 964.6146 - val_loss: 952.8818 - val_mae: 952.8818\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 920.4008 - mae: 920.4008 - val_loss: 909.0135 - val_mae: 909.0135\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 877.5481 - mae: 877.5481 - val_loss: 868.9973 - val_mae: 868.9973\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 854.8322 - mae: 854.8322\n",
    "    --- Starting trial: run-53\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1502.4844 - mae: 1502.4844 - val_loss: 1354.7896 - val_mae: 1354.7896\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1271.2701 - mae: 1271.2701 - val_loss: 1244.1079 - val_mae: 1244.1079\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1202.0615 - mae: 1202.0615 - val_loss: 1208.1639 - val_mae: 1208.1639\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1249.0465 - mae: 1249.0465 - val_loss: 1388.5922 - val_mae: 1388.5922\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1322.0587 - mae: 1322.0587 - val_loss: 1294.1733 - val_mae: 1294.1733\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1257.0309 - mae: 1257.0309 - val_loss: 1255.7302 - val_mae: 1255.7302\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1223.9701 - mae: 1223.9701 - val_loss: 1230.4979 - val_mae: 1230.4979\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1202.8103 - mae: 1202.8103 - val_loss: 1215.9810 - val_mae: 1215.9810\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1189.9863 - mae: 1189.9863 - val_loss: 1207.2212 - val_mae: 1207.2212\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1183.5328 - mae: 1183.5328 - val_loss: 1202.7959 - val_mae: 1202.7959\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1178.9757 - mae: 1178.9757 - val_loss: 1198.9844 - val_mae: 1198.9844\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1180.2412 - mae: 1180.2412 - val_loss: 1201.9044 - val_mae: 1201.9044\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1170.0922 - mae: 1170.0922 - val_loss: 1058.1254 - val_mae: 1058.1254\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 852.0829 - mae: 852.0829 - val_loss: 759.8188 - val_mae: 759.8188\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 821.8002 - mae: 821.8002 - val_loss: 580.1573 - val_mae: 580.1573\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 701.4338 - mae: 701.4338 - val_loss: 858.2007 - val_mae: 858.2007\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 852.1119 - mae: 852.1119\n",
    "    --- Starting trial: run-54\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1694.4193 - mae: 1694.4193 - val_loss: 1661.6246 - val_mae: 1661.6246\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1623.7301 - mae: 1623.7301 - val_loss: 1593.1334 - val_mae: 1593.1334\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1556.1296 - mae: 1556.1296 - val_loss: 1526.8265 - val_mae: 1526.8265\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1490.9556 - mae: 1490.9556 - val_loss: 1460.1119 - val_mae: 1460.1119\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1427.8076 - mae: 1427.8076 - val_loss: 1400.5409 - val_mae: 1400.5409\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1367.5930 - mae: 1367.5930 - val_loss: 1345.0804 - val_mae: 1345.0804\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1310.0184 - mae: 1310.0184 - val_loss: 1285.4628 - val_mae: 1285.4628\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1253.3188 - mae: 1253.3188 - val_loss: 1230.3567 - val_mae: 1230.3567\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1199.4473 - mae: 1199.4473 - val_loss: 1178.5710 - val_mae: 1178.5710\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1148.0098 - mae: 1148.0098 - val_loss: 1125.5715 - val_mae: 1125.5715\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1098.2299 - mae: 1098.2299 - val_loss: 1078.4414 - val_mae: 1078.4414\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1049.9579 - mae: 1049.9579 - val_loss: 1031.6129 - val_mae: 1031.6129\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1003.3875 - mae: 1003.3875 - val_loss: 985.7200 - val_mae: 985.7200\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 957.4155 - mae: 957.4155 - val_loss: 937.9058 - val_mae: 937.9058\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 913.8922 - mae: 913.8922 - val_loss: 894.8328 - val_mae: 894.8328\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 870.7062 - mae: 870.7062 - val_loss: 852.1217 - val_mae: 852.1217\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 838.9028 - mae: 838.9028\n",
    "    --- Starting trial: run-55\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1430.4091 - mae: 1430.4091 - val_loss: 1173.6382 - val_mae: 1173.6382\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 963.4750 - mae: 963.4750 - val_loss: 850.0145 - val_mae: 850.0145\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 619.6481 - mae: 619.6481 - val_loss: 448.4393 - val_mae: 448.4393\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 350.6143 - mae: 350.6143 - val_loss: 263.8476 - val_mae: 263.8476\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 324.3431 - mae: 324.3431 - val_loss: 259.9662 - val_mae: 259.9662\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 303.6106 - mae: 303.6106 - val_loss: 291.3770 - val_mae: 291.3770\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 289.1208 - mae: 289.1208 - val_loss: 246.4542 - val_mae: 246.4542\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 298.6770 - mae: 298.6770 - val_loss: 261.8775 - val_mae: 261.8775\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 284.3426 - mae: 284.3426 - val_loss: 248.1265 - val_mae: 248.1265\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 287.6434 - mae: 287.6434 - val_loss: 270.9297 - val_mae: 270.9297\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 282.9439 - mae: 282.9439 - val_loss: 219.0473 - val_mae: 219.0473\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 334.6888 - mae: 334.6888 - val_loss: 278.2180 - val_mae: 278.2180\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 335.8440 - mae: 335.8440 - val_loss: 332.3330 - val_mae: 332.3330\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 307.0844 - mae: 307.0844 - val_loss: 268.7868 - val_mae: 268.7868\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 285.3783 - mae: 285.3783 - val_loss: 276.0867 - val_mae: 276.0867\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 349.4684 - mae: 349.4684 - val_loss: 333.5100 - val_mae: 333.5100\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 327.2126 - mae: 327.2126\n",
    "    --- Starting trial: run-56\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1658.2850 - mae: 1658.2850 - val_loss: 1591.9558 - val_mae: 1591.9558\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1521.5365 - mae: 1521.5365 - val_loss: 1461.7190 - val_mae: 1461.7190\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1397.4489 - mae: 1397.4489 - val_loss: 1345.9897 - val_mae: 1345.9897\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1285.4169 - mae: 1285.4169 - val_loss: 1243.4918 - val_mae: 1243.4918\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1180.4349 - mae: 1180.4349 - val_loss: 1136.4615 - val_mae: 1136.4615\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1082.0282 - mae: 1082.0282 - val_loss: 1045.4163 - val_mae: 1045.4163\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 991.0557 - mae: 991.0557 - val_loss: 957.9186 - val_mae: 957.9186\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 904.5130 - mae: 904.5130 - val_loss: 872.7933 - val_mae: 872.7933\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 820.6494 - mae: 820.6494 - val_loss: 794.7302 - val_mae: 794.7302\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 742.6188 - mae: 742.6188 - val_loss: 716.9299 - val_mae: 716.9299\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 665.4910 - mae: 665.4910 - val_loss: 640.5123 - val_mae: 640.5123\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 589.0759 - mae: 589.0759 - val_loss: 566.1230 - val_mae: 566.1230\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 516.3492 - mae: 516.3492 - val_loss: 494.2713 - val_mae: 494.2713\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 443.9258 - mae: 443.9258 - val_loss: 428.8368 - val_mae: 428.8368\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 375.5524 - mae: 375.5524 - val_loss: 360.6385 - val_mae: 360.6385\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 313.4577 - mae: 313.4577 - val_loss: 297.9229 - val_mae: 297.9229\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 288.8310 - mae: 288.8310\n",
    "    --- Starting trial: run-57\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1393.8712 - mae: 1393.8712 - val_loss: 1165.2119 - val_mae: 1165.2119\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 913.7104 - mae: 913.7104 - val_loss: 701.9658 - val_mae: 701.9658\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 479.5779 - mae: 479.5779 - val_loss: 383.4006 - val_mae: 383.4006\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 290.8652 - mae: 290.8652 - val_loss: 266.3282 - val_mae: 266.3282\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 253.8491 - mae: 253.8491 - val_loss: 307.6183 - val_mae: 307.6183\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 293.0007 - mae: 293.0007 - val_loss: 260.0633 - val_mae: 260.0633\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 241.4846 - mae: 241.4846 - val_loss: 252.0758 - val_mae: 252.0758\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 236.6730 - mae: 236.6730 - val_loss: 253.3675 - val_mae: 253.3675\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 229.5158 - mae: 229.5158 - val_loss: 227.3736 - val_mae: 227.3736\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 219.7806 - mae: 219.7806 - val_loss: 227.4309 - val_mae: 227.4309\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 236.8613 - mae: 236.8613 - val_loss: 259.1495 - val_mae: 259.1495\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 247.9173 - mae: 247.9173 - val_loss: 247.2944 - val_mae: 247.2944\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 230.1233 - mae: 230.1233 - val_loss: 237.6126 - val_mae: 237.6126\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 234.4413 - mae: 234.4413 - val_loss: 245.7092 - val_mae: 245.7092\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 247.9096 - mae: 247.9096 - val_loss: 275.5294 - val_mae: 275.5294\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 279.2746 - mae: 279.2746 - val_loss: 264.5772 - val_mae: 264.5772\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 258.7753 - mae: 258.7753\n",
    "    --- Starting trial: run-58\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1654.1593 - mae: 1654.1593 - val_loss: 1588.5820 - val_mae: 1588.5820\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1520.9879 - mae: 1520.9879 - val_loss: 1462.1177 - val_mae: 1462.1177\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1397.7701 - mae: 1397.7701 - val_loss: 1342.3485 - val_mae: 1342.3485\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1284.4426 - mae: 1284.4426 - val_loss: 1234.8505 - val_mae: 1234.8505\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1178.6564 - mae: 1178.6564 - val_loss: 1134.4463 - val_mae: 1134.4463\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1081.3783 - mae: 1081.3783 - val_loss: 1043.1702 - val_mae: 1043.1702\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 990.3057 - mae: 990.3057 - val_loss: 951.9045 - val_mae: 951.9045\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 902.6321 - mae: 902.6321 - val_loss: 866.2329 - val_mae: 866.2329\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 820.3498 - mae: 820.3498 - val_loss: 786.3863 - val_mae: 786.3863\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 741.0311 - mae: 741.0311 - val_loss: 707.8280 - val_mae: 707.8280\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 663.2288 - mae: 663.2288 - val_loss: 630.3914 - val_mae: 630.3914\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 586.4191 - mae: 586.4191 - val_loss: 554.6180 - val_mae: 554.6180\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 512.5125 - mae: 512.5125 - val_loss: 481.3751 - val_mae: 481.3751\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 440.0503 - mae: 440.0503 - val_loss: 409.3201 - val_mae: 409.3201\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 374.1540 - mae: 374.1540 - val_loss: 344.3039 - val_mae: 344.3039\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 316.0261 - mae: 316.0261 - val_loss: 286.0162 - val_mae: 286.0162\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 277.9122 - mae: 277.9122\n",
    "    --- Starting trial: run-59\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 2, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 37ms/step - loss: 1392.4385 - mae: 1392.4385 - val_loss: 1250.3171 - val_mae: 1250.3171\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1197.3577 - mae: 1197.3577 - val_loss: 1201.2832 - val_mae: 1201.2832\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1174.1700 - mae: 1174.1700 - val_loss: 1192.3412 - val_mae: 1192.3412\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1168.2170 - mae: 1168.2170 - val_loss: 1189.7781 - val_mae: 1189.7781\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1166.9709 - mae: 1166.9709 - val_loss: 1190.3672 - val_mae: 1190.3672\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1166.1785 - mae: 1166.1785 - val_loss: 1190.2751 - val_mae: 1190.2751\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.4803 - mae: 1166.4803 - val_loss: 1190.5962 - val_mae: 1190.5962\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1166.8518 - mae: 1166.8518 - val_loss: 1190.4844 - val_mae: 1190.4844\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.3948 - mae: 1166.3948 - val_loss: 1189.2499 - val_mae: 1189.2499\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.2703 - mae: 1166.2703 - val_loss: 1191.2209 - val_mae: 1191.2209\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 1046.9111 - mae: 1046.9111 - val_loss: 830.7166 - val_mae: 830.7166\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 658.5361 - mae: 658.5361 - val_loss: 480.1817 - val_mae: 480.1817\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 381.9342 - mae: 381.9342 - val_loss: 744.7347 - val_mae: 744.7347\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 320.2578 - mae: 320.2578 - val_loss: 249.4001 - val_mae: 249.4001\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 287.5827 - mae: 287.5827 - val_loss: 250.1878 - val_mae: 250.1878\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 37ms/step - loss: 280.8719 - mae: 280.8719 - val_loss: 294.1183 - val_mae: 294.1183\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 287.3031 - mae: 287.3031\n",
    "    --- Starting trial: run-60\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1715.2748 - mae: 1715.2748 - val_loss: 1700.8225 - val_mae: 1700.8225\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1681.0942 - mae: 1681.0942 - val_loss: 1669.6406 - val_mae: 1669.6406\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1651.2910 - mae: 1651.2910 - val_loss: 1642.4690 - val_mae: 1642.4690\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1623.7871 - mae: 1623.7871 - val_loss: 1614.1747 - val_mae: 1614.1747\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1596.3390 - mae: 1596.3390 - val_loss: 1589.2986 - val_mae: 1589.2986\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1568.7782 - mae: 1568.7782 - val_loss: 1561.4919 - val_mae: 1561.4919\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1541.7036 - mae: 1541.7036 - val_loss: 1500.9530 - val_mae: 1500.9530\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1477.8937 - mae: 1477.8937 - val_loss: 1462.9747 - val_mae: 1462.9747\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1443.2484 - mae: 1443.2484 - val_loss: 1431.3762 - val_mae: 1431.3762\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1410.7377 - mae: 1410.7377 - val_loss: 1401.5405 - val_mae: 1401.5405\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1379.5552 - mae: 1379.5552 - val_loss: 1370.0154 - val_mae: 1370.0154\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1349.6385 - mae: 1349.6385 - val_loss: 1341.7344 - val_mae: 1341.7344\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1357.4172 - mae: 1357.4172 - val_loss: 1354.0046 - val_mae: 1354.0046\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1332.7262 - mae: 1332.7262 - val_loss: 1323.8184 - val_mae: 1323.8184\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1303.0415 - mae: 1303.0415 - val_loss: 1296.4604 - val_mae: 1296.4604\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1271.9946 - mae: 1271.9946 - val_loss: 1259.4896 - val_mae: 1259.4896\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1244.1625 - mae: 1244.1625\n",
    "    --- Starting trial: run-61\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1591.4929 - mae: 1591.4929 - val_loss: 1480.5995 - val_mae: 1480.5995\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1394.2026 - mae: 1394.2026 - val_loss: 1341.5234 - val_mae: 1341.5234\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1285.2896 - mae: 1285.2896 - val_loss: 1270.2513 - val_mae: 1270.2513\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1231.3619 - mae: 1231.3619 - val_loss: 1233.0593 - val_mae: 1233.0593\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1201.1576 - mae: 1201.1576 - val_loss: 1212.6584 - val_mae: 1212.6584\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1186.1936 - mae: 1186.1936 - val_loss: 1203.3052 - val_mae: 1203.3052\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1179.8710 - mae: 1179.8710 - val_loss: 1198.6248 - val_mae: 1198.6248\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1175.8376 - mae: 1175.8376 - val_loss: 1196.5792 - val_mae: 1196.5792\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1173.0054 - mae: 1173.0054 - val_loss: 1194.4427 - val_mae: 1194.4427\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1171.1752 - mae: 1171.1752 - val_loss: 1193.2313 - val_mae: 1193.2313\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1168.9563 - mae: 1168.9563 - val_loss: 1192.1876 - val_mae: 1192.1876\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1167.9323 - mae: 1167.9323 - val_loss: 1190.2078 - val_mae: 1190.2078\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1167.0031 - mae: 1167.0031 - val_loss: 1190.0613 - val_mae: 1190.0613\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1166.2233 - mae: 1166.2233 - val_loss: 1190.4324 - val_mae: 1190.4324\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1165.9617 - mae: 1165.9617 - val_loss: 1189.8519 - val_mae: 1189.8519\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1165.9121 - mae: 1165.9121 - val_loss: 1189.7178 - val_mae: 1189.7178\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1181.1914 - mae: 1181.1914\n",
    "    --- Starting trial: run-62\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1715.9768 - mae: 1715.9768 - val_loss: 1700.1365 - val_mae: 1700.1365\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1682.1923 - mae: 1682.1923 - val_loss: 1668.7361 - val_mae: 1668.7361\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1645.0662 - mae: 1645.0662 - val_loss: 1629.2726 - val_mae: 1629.2726\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1608.1929 - mae: 1608.1929 - val_loss: 1594.8641 - val_mae: 1594.8641\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1573.3793 - mae: 1573.3793 - val_loss: 1558.7391 - val_mae: 1558.7391\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1538.6969 - mae: 1538.6969 - val_loss: 1527.7262 - val_mae: 1527.7262\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1505.5537 - mae: 1505.5537 - val_loss: 1491.9905 - val_mae: 1491.9905\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1472.1675 - mae: 1472.1675 - val_loss: 1460.4608 - val_mae: 1460.4608\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1440.3983 - mae: 1440.3983 - val_loss: 1429.3096 - val_mae: 1429.3094\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1408.9263 - mae: 1408.9263 - val_loss: 1397.1091 - val_mae: 1397.1091\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1378.8611 - mae: 1378.8611 - val_loss: 1366.4390 - val_mae: 1366.4390\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1347.7401 - mae: 1347.7401 - val_loss: 1338.5714 - val_mae: 1338.5714\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1318.4534 - mae: 1318.4534 - val_loss: 1307.1971 - val_mae: 1307.1971\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1289.7780 - mae: 1289.7780 - val_loss: 1281.5234 - val_mae: 1281.5234\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1260.6885 - mae: 1260.6885 - val_loss: 1252.1787 - val_mae: 1252.1787\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 1232.7468 - mae: 1232.7468 - val_loss: 1222.5785 - val_mae: 1222.5785\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 1206.2313 - mae: 1206.2313\n",
    "    --- Starting trial: run-63\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1595.6200 - mae: 1595.6200 - val_loss: 1487.7579 - val_mae: 1487.7579\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1400.6389 - mae: 1400.6389 - val_loss: 1343.9597 - val_mae: 1343.9596\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1288.3634 - mae: 1288.3634 - val_loss: 1269.7526 - val_mae: 1269.7526\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1233.0900 - mae: 1233.0900 - val_loss: 1232.9026 - val_mae: 1232.9026\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1203.7236 - mae: 1203.7236 - val_loss: 1213.1016 - val_mae: 1213.1016\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1289.3771 - mae: 1289.3771 - val_loss: 1215.4357 - val_mae: 1215.4357\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1191.6283 - mae: 1191.6283 - val_loss: 1206.5406 - val_mae: 1206.5406\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1174.5946 - mae: 1174.5946 - val_loss: 1176.1923 - val_mae: 1176.1923\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 878.6800 - mae: 878.6800 - val_loss: 616.5504 - val_mae: 616.5504\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 505.5520 - mae: 505.5520 - val_loss: 408.8690 - val_mae: 408.8690\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 478.2428 - mae: 478.2428 - val_loss: 446.8481 - val_mae: 446.8481\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 472.9128 - mae: 472.9128 - val_loss: 337.6688 - val_mae: 337.6688\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 344.9160 - mae: 344.9160 - val_loss: 285.9080 - val_mae: 285.9080\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 332.0251 - mae: 332.0251 - val_loss: 276.5626 - val_mae: 276.5626\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 368.0942 - mae: 368.0942 - val_loss: 338.9963 - val_mae: 338.9963\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 382.4677 - mae: 382.4677 - val_loss: 347.5350 - val_mae: 347.5350\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 340.6219 - mae: 340.6219\n",
    "    --- Starting trial: run-64\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1697.7865 - mae: 1697.7865 - val_loss: 1669.7599 - val_mae: 1669.7599\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1635.1888 - mae: 1635.1888 - val_loss: 1597.5497 - val_mae: 1597.5497\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1559.0917 - mae: 1559.0917 - val_loss: 1526.9058 - val_mae: 1526.9058\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1490.9906 - mae: 1490.9906 - val_loss: 1462.7157 - val_mae: 1462.7157\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1427.6451 - mae: 1427.6451 - val_loss: 1404.5392 - val_mae: 1404.5392\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1367.3839 - mae: 1367.3839 - val_loss: 1345.6179 - val_mae: 1345.6179\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1309.5031 - mae: 1309.5031 - val_loss: 1286.0879 - val_mae: 1286.0879\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 44s 35ms/step - loss: 1251.9056 - mae: 1251.9056 - val_loss: 1230.3121 - val_mae: 1230.3121\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1197.0972 - mae: 1197.0972 - val_loss: 1179.2968 - val_mae: 1179.2968\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1144.1387 - mae: 1144.1387 - val_loss: 1126.5173 - val_mae: 1126.5173\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1094.0553 - mae: 1094.0553 - val_loss: 1079.6848 - val_mae: 1079.6848\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1045.9073 - mae: 1045.9073 - val_loss: 1031.7556 - val_mae: 1031.7556\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 999.1041 - mae: 999.1041 - val_loss: 987.5872 - val_mae: 987.5872\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 953.2366 - mae: 953.2366 - val_loss: 942.5939 - val_mae: 942.5939\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 908.8012 - mae: 908.8012 - val_loss: 896.9649 - val_mae: 896.9649\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 865.8039 - mae: 865.8039 - val_loss: 854.4534 - val_mae: 854.4534\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 841.1807 - mae: 841.1807\n",
    "    --- Starting trial: run-65\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 47s 36ms/step - loss: 1500.3896 - mae: 1500.3896 - val_loss: 1353.6429 - val_mae: 1353.6429\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1269.3514 - mae: 1269.3514 - val_loss: 1242.7375 - val_mae: 1242.7375\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1201.1636 - mae: 1201.1636 - val_loss: 1208.4069 - val_mae: 1208.4069\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1296.0450 - mae: 1296.0450 - val_loss: 1272.9860 - val_mae: 1272.9860\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1065.6682 - mae: 1065.6682 - val_loss: 937.1535 - val_mae: 937.1535\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 863.2352 - mae: 863.2352 - val_loss: 825.2983 - val_mae: 825.2983\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 781.0197 - mae: 781.0197 - val_loss: 761.8275 - val_mae: 761.8275\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 548.8966 - mae: 548.8966 - val_loss: 922.2382 - val_mae: 922.2382\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 721.9518 - mae: 721.9518 - val_loss: 674.2233 - val_mae: 674.2233\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 428.9033 - mae: 428.9033 - val_loss: 310.6823 - val_mae: 310.6823\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 228.6779 - mae: 228.6779 - val_loss: 232.3949 - val_mae: 232.3949\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 319.3440 - mae: 319.3440 - val_loss: 268.2849 - val_mae: 268.2849\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 245.7328 - mae: 245.7328 - val_loss: 249.9334 - val_mae: 249.9334\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 309.2064 - mae: 309.2064 - val_loss: 295.4243 - val_mae: 295.4243\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 46s 36ms/step - loss: 409.6797 - mae: 409.6797 - val_loss: 697.2623 - val_mae: 697.2623\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 595.1288 - mae: 595.1288 - val_loss: 505.8657 - val_mae: 505.8657\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 496.2920 - mae: 496.2920\n",
    "    --- Starting trial: run-66\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1696.4565 - mae: 1696.4565 - val_loss: 1669.0969 - val_mae: 1669.0969\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1637.5795 - mae: 1637.5795 - val_loss: 1617.6250 - val_mae: 1617.6250\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1586.1687 - mae: 1586.1687 - val_loss: 1565.0692 - val_mae: 1565.0692\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1536.2133 - mae: 1536.2133 - val_loss: 1520.1093 - val_mae: 1520.1093\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1462.5615 - mae: 1462.5615 - val_loss: 1415.0160 - val_mae: 1415.0160\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1378.6464 - mae: 1378.6464 - val_loss: 1353.4991 - val_mae: 1353.4991\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1320.3718 - mae: 1320.3718 - val_loss: 1295.7448 - val_mae: 1295.7448\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1262.8053 - mae: 1262.8053 - val_loss: 1240.6733 - val_mae: 1240.6733\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 35ms/step - loss: 1208.9750 - mae: 1208.9750 - val_loss: 1188.1603 - val_mae: 1188.1603\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1157.9777 - mae: 1157.9777 - val_loss: 1138.0925 - val_mae: 1138.0925\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1108.5146 - mae: 1108.5146 - val_loss: 1089.1493 - val_mae: 1089.1493\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1061.1853 - mae: 1061.1853 - val_loss: 1044.2592 - val_mae: 1044.2592\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1014.6068 - mae: 1014.6068 - val_loss: 996.2401 - val_mae: 996.2401\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 969.8047 - mae: 969.8047 - val_loss: 951.8912 - val_mae: 951.8912\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 925.5235 - mae: 925.5235 - val_loss: 908.1586 - val_mae: 908.1586\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 882.4880 - mae: 882.4880 - val_loss: 864.4131 - val_mae: 864.4131\n",
    "    268/268 [==============================] - 8s 29ms/step - loss: 851.2615 - mae: 851.2615\n",
    "    --- Starting trial: run-67\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 48s 36ms/step - loss: 1500.5712 - mae: 1500.5712 - val_loss: 1352.8546 - val_mae: 1352.8546\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1291.4684 - mae: 1291.4684 - val_loss: 1489.7858 - val_mae: 1489.7858\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1407.3368 - mae: 1407.3368 - val_loss: 1360.9995 - val_mae: 1360.9995\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1302.7561 - mae: 1302.7561 - val_loss: 1276.9496 - val_mae: 1276.9496\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 1122.5923 - mae: 1122.5923 - val_loss: 975.3730 - val_mae: 975.3730\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 880.1927 - mae: 880.1927 - val_loss: 797.9919 - val_mae: 797.9919\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 733.1785 - mae: 733.1785 - val_loss: 678.7579 - val_mae: 678.7579\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 612.8644 - mae: 612.8644 - val_loss: 610.0171 - val_mae: 610.0171\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 460.0597 - mae: 460.0597 - val_loss: 343.3179 - val_mae: 343.3179\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 379.8155 - mae: 379.8155 - val_loss: 320.0001 - val_mae: 320.0001\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 373.2209 - mae: 373.2209 - val_loss: 310.9376 - val_mae: 310.9376\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 366.4509 - mae: 366.4509 - val_loss: 327.1859 - val_mae: 327.1859\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 365.6005 - mae: 365.6005 - val_loss: 460.3434 - val_mae: 460.3434\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 390.1715 - mae: 390.1715 - val_loss: 334.4388 - val_mae: 334.4388\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 376.8595 - mae: 376.8595 - val_loss: 335.8213 - val_mae: 335.8213\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 45s 36ms/step - loss: 381.7434 - mae: 381.7434 - val_loss: 339.1835 - val_mae: 339.1835\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 332.5205 - mae: 332.5205\n",
    "    --- Starting trial: run-68\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 60s 46ms/step - loss: 1663.6064 - mae: 1663.6064 - val_loss: 1614.4357 - val_mae: 1614.4357\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1559.5764 - mae: 1559.5764 - val_loss: 1519.3787 - val_mae: 1519.3787\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1467.7078 - mae: 1467.7078 - val_loss: 1398.7457 - val_mae: 1398.7457\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1328.1588 - mae: 1328.1588 - val_loss: 1276.2462 - val_mae: 1276.2462\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1214.9052 - mae: 1214.9052 - val_loss: 1171.7322 - val_mae: 1171.7322\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1118.0210 - mae: 1118.0210 - val_loss: 1081.5664 - val_mae: 1081.5664\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1029.3809 - mae: 1029.3809 - val_loss: 998.2254 - val_mae: 998.2254\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 946.2430 - mae: 946.2430 - val_loss: 918.2692 - val_mae: 918.2692\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 866.0884 - mae: 866.0884 - val_loss: 838.9724 - val_mae: 838.9724\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 791.7746 - mae: 791.7746 - val_loss: 768.1389 - val_mae: 768.1389\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 718.3478 - mae: 718.3478 - val_loss: 695.4339 - val_mae: 695.4339\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 647.2088 - mae: 647.2088 - val_loss: 628.5600 - val_mae: 628.5600\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 576.3525 - mae: 576.3525 - val_loss: 553.3524 - val_mae: 553.3524\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 507.9065 - mae: 507.9065 - val_loss: 491.3889 - val_mae: 491.3889\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 441.0811 - mae: 441.0811 - val_loss: 421.3066 - val_mae: 421.3066\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 378.0339 - mae: 378.0339 - val_loss: 362.7409 - val_mae: 362.7409\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 353.1181 - mae: 353.1181\n",
    "    --- Starting trial: run-69\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 60s 46ms/step - loss: 1386.4313 - mae: 1386.4313 - val_loss: 1243.6243 - val_mae: 1243.6243\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1193.9862 - mae: 1193.9862 - val_loss: 1201.1508 - val_mae: 1201.1508\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1173.7668 - mae: 1173.7668 - val_loss: 1193.4762 - val_mae: 1193.4762\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1167.6324 - mae: 1167.6324 - val_loss: 1189.1149 - val_mae: 1189.1149\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.1710 - mae: 1166.1710 - val_loss: 1188.8170 - val_mae: 1188.8170\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1166.0034 - mae: 1166.0034 - val_loss: 1189.7418 - val_mae: 1189.7418\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1165.9618 - mae: 1165.9618 - val_loss: 1190.1847 - val_mae: 1190.1847\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.0416 - mae: 1166.0416 - val_loss: 1190.1637 - val_mae: 1190.1637\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.8226 - mae: 1165.8226 - val_loss: 1190.2715 - val_mae: 1190.2715\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1166.2115 - mae: 1166.2115 - val_loss: 1191.0272 - val_mae: 1191.0272\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 57s 46ms/step - loss: 1165.6149 - mae: 1165.6149 - val_loss: 1189.4729 - val_mae: 1189.4729\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.9915 - mae: 1165.9915 - val_loss: 1190.9021 - val_mae: 1190.9021\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.8752 - mae: 1165.8752 - val_loss: 1188.5698 - val_mae: 1188.5698\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.0256 - mae: 1166.0256 - val_loss: 1189.5872 - val_mae: 1189.5872\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1165.8643 - mae: 1165.8643 - val_loss: 1189.8784 - val_mae: 1189.8784\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 58s 46ms/step - loss: 1166.1276 - mae: 1166.1276 - val_loss: 1190.4354 - val_mae: 1190.4354\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1183.3245 - mae: 1183.3245\n",
    "    --- Starting trial: run-70\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 63s 48ms/step - loss: 1664.3086 - mae: 1664.3086 - val_loss: 1599.2139 - val_mae: 1599.2139\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1526.7667 - mae: 1526.7668 - val_loss: 1463.8284 - val_mae: 1463.8284\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1398.5803 - mae: 1398.5803 - val_loss: 1343.9360 - val_mae: 1343.9360\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1283.9706 - mae: 1283.9706 - val_loss: 1237.0625 - val_mae: 1237.0625\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1178.6718 - mae: 1178.6718 - val_loss: 1135.2367 - val_mae: 1135.2367\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1081.6058 - mae: 1081.6058 - val_loss: 1041.8905 - val_mae: 1041.8905\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 990.2513 - mae: 990.2513 - val_loss: 952.1376 - val_mae: 952.1376\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 903.1245 - mae: 903.1245 - val_loss: 865.3026 - val_mae: 865.3026\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 820.2087 - mae: 820.2087 - val_loss: 784.1815 - val_mae: 784.1815\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 741.3391 - mae: 741.3391 - val_loss: 707.8594 - val_mae: 707.8594\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 663.3409 - mae: 663.3409 - val_loss: 630.2474 - val_mae: 630.2474\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 585.9130 - mae: 585.9130 - val_loss: 553.8346 - val_mae: 553.8346\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 512.3198 - mae: 512.3198 - val_loss: 479.7880 - val_mae: 479.7880\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 440.6092 - mae: 440.6092 - val_loss: 410.9050 - val_mae: 410.9050\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 374.1082 - mae: 374.1082 - val_loss: 348.0106 - val_mae: 348.0106\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 315.7215 - mae: 315.7215 - val_loss: 286.8600 - val_mae: 286.8600\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 277.8535 - mae: 277.8535\n",
    "    --- Starting trial: run-71\n",
    "    {'layer_type': 'keras.layers.LSTM', 'n_recurrent': 3, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.01}\n",
    "    Epoch 1/16\n",
    "    1256/1256 [==============================] - 63s 48ms/step - loss: 1392.1804 - mae: 1392.1804 - val_loss: 1252.0527 - val_mae: 1252.0527\n",
    "    Epoch 2/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1197.3865 - mae: 1197.3865 - val_loss: 1199.4036 - val_mae: 1199.4036\n",
    "    Epoch 3/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1173.7020 - mae: 1173.7020 - val_loss: 1193.5513 - val_mae: 1193.5513\n",
    "    Epoch 4/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1168.0837 - mae: 1168.0837 - val_loss: 1190.2594 - val_mae: 1190.2594\n",
    "    Epoch 5/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.6333 - mae: 1166.6333 - val_loss: 1190.3444 - val_mae: 1190.3444\n",
    "    Epoch 6/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.7308 - mae: 1166.7308 - val_loss: 1189.4094 - val_mae: 1189.4094\n",
    "    Epoch 7/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5526 - mae: 1166.5526 - val_loss: 1190.6384 - val_mae: 1190.6384\n",
    "    Epoch 8/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.2701 - mae: 1166.2701 - val_loss: 1189.3081 - val_mae: 1189.3081\n",
    "    Epoch 9/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.2324 - mae: 1166.2324 - val_loss: 1190.3262 - val_mae: 1190.3262\n",
    "    Epoch 10/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.1544 - mae: 1166.1544 - val_loss: 1190.9987 - val_mae: 1190.9987\n",
    "    Epoch 11/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.4553 - mae: 1166.4553 - val_loss: 1190.5316 - val_mae: 1190.5316\n",
    "    Epoch 12/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.8123 - mae: 1166.8123 - val_loss: 1189.6770 - val_mae: 1189.6770\n",
    "    Epoch 13/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5917 - mae: 1166.5917 - val_loss: 1188.7397 - val_mae: 1188.7397\n",
    "    Epoch 14/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.4796 - mae: 1166.4796 - val_loss: 1189.9938 - val_mae: 1189.9938\n",
    "    Epoch 15/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5144 - mae: 1166.5144 - val_loss: 1190.9899 - val_mae: 1190.9899\n",
    "    Epoch 16/16\n",
    "    1256/1256 [==============================] - 60s 48ms/step - loss: 1166.5955 - mae: 1166.5955 - val_loss: 1190.0695 - val_mae: 1190.0695\n",
    "    268/268 [==============================] - 8s 30ms/step - loss: 1183.2845 - mae: 1183.2845\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f591b3b-a116-4223-9a66-b549b75a1030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-3')(dropout2)\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(1, name='Output'))(lstm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a747ed-ff54-40f4-85bc-96b35165648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model multi dense\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-3')(dropout2)\n",
    "dense = keras.layers.Dense(128, name='Dense')(lstm3)\n",
    "output = keras.layers.Dense(1, name='Output')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996daba-5af0-40cc-a2b2-faa36f00f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for quick testing\n",
    "\n",
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(32, return_sequences=True)(input)\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm1)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd42f8d-55a5-4ef6-962f-bf2cdfe25af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157095f-0712-4ff5-9559-b7e99ab8e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model: \"Covid-Prediction-30-1\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " Input (InputLayer)          [(None, 30, 101)]         0         \n",
    "                                                                 \n",
    " LSTM-1 (LSTM)               (None, 30, 256)           366592    \n",
    "                                                                 \n",
    " Dropout-1 (Dropout)         (None, 30, 256)           0         \n",
    "                                                                 \n",
    " LSTM-2 (LSTM)               (None, 30, 256)           525312    \n",
    "                                                                 \n",
    " Dropout-2 (Dropout)         (None, 30, 256)           0         \n",
    "                                                                 \n",
    " LSTM-3 (LSTM)               (None, 30, 256)           525312    \n",
    "                                                                 \n",
    " Output (Dense)              (None, 30, 1)             257       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 1,417,473\n",
    "Trainable params: 1,417,473\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3280bb06-dae2-4115-9eaa-2c24336ea76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, to_file='multilayer_perceptron_graph.png')\n",
    "keras.utils.plot_model(model, to_file='model_3_256.png', show_shapes=False, show_dtype=False, \n",
    "                       show_layer_activations=False, rankdir='LR', dpi=70, show_layer_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a23bb-19f0-472c-8f52-963f931b9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, to_file='model_3_256_detailed.png', show_shapes=True, show_dtype=True, \n",
    "                       show_layer_activations=False, rankdir='TB', dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692656d-ff4e-43ce-b6b5-93301c71c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME='densex2'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_densex2.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=50, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a52820-5929-47a6-b787-9ab60dd994e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 16:42:36.473302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.473553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.529052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.529340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.529558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.529891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.530551: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-23 16:42:36.678295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.678518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.678709: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.678886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.679052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:36.679218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:37.394060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:37.394318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:37.394503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:37.394682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:37.394854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:37.397754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2937 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2022-02-23 16:42:37.398293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 16:42:37.398455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 3743 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 16:42:41.467881: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 150s 116ms/step - loss: 6352.8433 - mse: 849018752.0000 - mae: 6352.8965 - val_loss: 5876.3960 - val_mse: 666665152.0000 - val_mae: 5876.3965\n",
      "Epoch 2/1024\n",
      "1258/1258 [==============================] - 151s 119ms/step - loss: 5651.1768 - mse: 826904896.0000 - mae: 5651.2222 - val_loss: 5440.3423 - val_mse: 649336448.0000 - val_mae: 5440.3442\n",
      "Epoch 3/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 5351.3687 - mse: 812456384.0000 - mae: 5351.4062 - val_loss: 5243.6865 - val_mse: 635334208.0000 - val_mae: 5243.6870\n",
      "Epoch 4/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 5009.8110 - mse: 797150528.0000 - mae: 5009.8457 - val_loss: 4863.4507 - val_mse: 620863552.0000 - val_mae: 4863.4507\n",
      "Epoch 5/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4885.0444 - mse: 787299072.0000 - mae: 4885.0786 - val_loss: 5223.2368 - val_mse: 613858688.0000 - val_mae: 5223.2378\n",
      "Epoch 6/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4861.7725 - mse: 784336384.0000 - mae: 4861.8032 - val_loss: 4677.7720 - val_mse: 608458944.0000 - val_mae: 4677.7720\n",
      "Epoch 7/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4727.3101 - mse: 776084864.0000 - mae: 4727.3389 - val_loss: 4688.6157 - val_mse: 601681152.0000 - val_mae: 4688.6172\n",
      "Epoch 8/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4618.6401 - mse: 767180672.0000 - mae: 4618.6709 - val_loss: 4427.1831 - val_mse: 588756096.0000 - val_mae: 4427.1826\n",
      "Epoch 9/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4657.0405 - mse: 768384256.0000 - mae: 4657.0752 - val_loss: 4578.8828 - val_mse: 597445568.0000 - val_mae: 4578.8833\n",
      "Epoch 10/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4574.7769 - mse: 765243712.0000 - mae: 4574.8086 - val_loss: 4468.5166 - val_mse: 588998336.0000 - val_mae: 4468.5186\n",
      "Epoch 11/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4932.7642 - mse: 776446912.0000 - mae: 4932.7969 - val_loss: 4742.2207 - val_mse: 606809152.0000 - val_mae: 4742.2192\n",
      "Epoch 12/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4640.7725 - mse: 760709056.0000 - mae: 4640.8086 - val_loss: 4422.1528 - val_mse: 576943872.0000 - val_mae: 4422.1558\n",
      "Epoch 13/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4547.7007 - mse: 755328192.0000 - mae: 4547.7324 - val_loss: 4516.9453 - val_mse: 575133440.0000 - val_mae: 4516.9458\n",
      "Epoch 14/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4484.8677 - mse: 748344640.0000 - mae: 4484.9004 - val_loss: 4469.3125 - val_mse: 568283968.0000 - val_mae: 4469.3154\n",
      "Epoch 15/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4465.0156 - mse: 745520896.0000 - mae: 4465.0439 - val_loss: 4116.2993 - val_mse: 561891200.0000 - val_mae: 4116.2979\n",
      "Epoch 16/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4613.0933 - mse: 753836544.0000 - mae: 4613.1357 - val_loss: 5043.1304 - val_mse: 592894400.0000 - val_mae: 5043.1333\n",
      "Epoch 17/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 5006.3799 - mse: 767667456.0000 - mae: 5006.4087 - val_loss: 5287.3335 - val_mse: 620204992.0000 - val_mae: 5287.3335\n",
      "Epoch 18/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4552.2710 - mse: 747816768.0000 - mae: 4552.2983 - val_loss: 4208.9873 - val_mse: 555599360.0000 - val_mae: 4208.9868\n",
      "Epoch 19/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4904.4639 - mse: 764322752.0000 - mae: 4904.5024 - val_loss: 5036.2544 - val_mse: 594205056.0000 - val_mae: 5036.2549\n",
      "Epoch 20/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4645.1304 - mse: 747831360.0000 - mae: 4645.1631 - val_loss: 4260.1782 - val_mse: 555239296.0000 - val_mae: 4260.1768\n",
      "Epoch 21/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4276.4595 - mse: 730041536.0000 - mae: 4276.4878 - val_loss: 4080.2722 - val_mse: 545573632.0000 - val_mae: 4080.2737\n",
      "Epoch 22/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4377.0029 - mse: 736623424.0000 - mae: 4377.0356 - val_loss: 4226.0596 - val_mse: 547253312.0000 - val_mae: 4226.0591\n",
      "Epoch 23/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4200.1270 - mse: 724582656.0000 - mae: 4200.1587 - val_loss: 4045.8074 - val_mse: 537587456.0000 - val_mae: 4045.8064\n",
      "Epoch 24/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4176.8579 - mse: 721154432.0000 - mae: 4176.8901 - val_loss: 4508.9814 - val_mse: 548047616.0000 - val_mae: 4508.9839\n",
      "Epoch 25/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4301.5820 - mse: 722872256.0000 - mae: 4301.6108 - val_loss: 3929.9785 - val_mse: 536630208.0000 - val_mae: 3929.9800\n",
      "Epoch 26/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4315.3696 - mse: 723836416.0000 - mae: 4315.4077 - val_loss: 4049.3977 - val_mse: 532374432.0000 - val_mae: 4049.3997\n",
      "Epoch 27/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4368.2026 - mse: 730059968.0000 - mae: 4368.2329 - val_loss: 4326.4351 - val_mse: 536245472.0000 - val_mae: 4326.4341\n",
      "Epoch 28/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4344.4014 - mse: 721914368.0000 - mae: 4344.4404 - val_loss: 4771.7065 - val_mse: 610244928.0000 - val_mae: 4771.7070\n",
      "Epoch 29/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4204.2710 - mse: 722937792.0000 - mae: 4204.3008 - val_loss: 3809.3564 - val_mse: 522787648.0000 - val_mae: 3809.3572\n",
      "Epoch 30/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4182.7705 - mse: 712403264.0000 - mae: 4182.8052 - val_loss: 4663.1489 - val_mse: 589593600.0000 - val_mae: 4663.1475\n",
      "Epoch 31/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4153.8838 - mse: 709555072.0000 - mae: 4153.9126 - val_loss: 4188.0132 - val_mse: 522670176.0000 - val_mae: 4188.0122\n",
      "Epoch 32/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4306.7295 - mse: 718262464.0000 - mae: 4306.7617 - val_loss: 4715.2871 - val_mse: 607952960.0000 - val_mae: 4715.2871\n",
      "Epoch 33/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4303.4551 - mse: 718344640.0000 - mae: 4303.4834 - val_loss: 4043.1584 - val_mse: 523107296.0000 - val_mae: 4043.1584\n",
      "Epoch 34/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 3921.1646 - mse: 697504000.0000 - mae: 3921.1897 - val_loss: 3904.9719 - val_mse: 516429408.0000 - val_mae: 3904.9712\n",
      "Epoch 35/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 3954.5327 - mse: 700653056.0000 - mae: 3954.5557 - val_loss: 3728.1360 - val_mse: 519320352.0000 - val_mae: 3728.1360\n",
      "Epoch 36/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 3776.5066 - mse: 690354304.0000 - mae: 3776.5308 - val_loss: 4011.8562 - val_mse: 512972224.0000 - val_mae: 4011.8557\n",
      "Epoch 37/1024\n",
      "1258/1258 [==============================] - 153s 121ms/step - loss: 4116.6367 - mse: 709706688.0000 - mae: 4116.6650 - val_loss: 4401.0303 - val_mse: 568590016.0000 - val_mae: 4401.0283\n",
      "Epoch 38/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4583.9556 - mse: 735898368.0000 - mae: 4583.9922 - val_loss: 3921.3062 - val_mse: 511656416.0000 - val_mae: 3921.3059\n",
      "Epoch 39/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4150.7012 - mse: 706039872.0000 - mae: 4150.7300 - val_loss: 3865.2227 - val_mse: 512471712.0000 - val_mae: 3865.2224\n",
      "Epoch 40/1024\n",
      "1258/1258 [==============================] - 152s 121ms/step - loss: 4005.2405 - mse: 700664192.0000 - mae: 4005.2644 - val_loss: 5274.6250 - val_mse: 529441504.0000 - val_mae: 5274.6235\n",
      "Epoch 41/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 3871.7168 - mse: 691197312.0000 - mae: 3871.7454 - val_loss: 4983.0024 - val_mse: 614114880.0000 - val_mae: 4983.0024\n",
      "Epoch 42/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4385.2612 - mse: 722338432.0000 - mae: 4385.2939 - val_loss: 3746.2649 - val_mse: 502784512.0000 - val_mae: 3746.2637\n",
      "Epoch 43/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4241.7090 - mse: 715202048.0000 - mae: 4241.7407 - val_loss: 4270.5171 - val_mse: 559763008.0000 - val_mae: 4270.5176\n",
      "Epoch 44/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4414.3516 - mse: 731024512.0000 - mae: 4414.3853 - val_loss: 3997.4668 - val_mse: 514905024.0000 - val_mae: 3997.4670\n",
      "Epoch 45/1024\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 4034.3713 - mse: 690629824.0000 - mae: 4034.3984 - val_loss: 4077.3789 - val_mse: 506697536.0000 - val_mae: 4077.3772\n",
      "Epoch 46/1024\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 4571.2676 - mse: 734611712.0000 - mae: 4571.2988 - val_loss: 4907.6152 - val_mse: 585853696.0000 - val_mae: 4907.6147\n",
      "Epoch 47/1024\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 4890.8003 - mse: 762591552.0000 - mae: 4890.8364 - val_loss: 4038.1545 - val_mse: 533741472.0000 - val_mae: 4038.1531\n",
      "Epoch 48/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4321.2861 - mse: 712972224.0000 - mae: 4321.3179 - val_loss: 3753.3909 - val_mse: 506093184.0000 - val_mae: 3753.3892\n",
      "Epoch 49/1024\n",
      "1258/1258 [==============================] - 151s 120ms/step - loss: 4291.8955 - mse: 716273408.0000 - mae: 4291.9263 - val_loss: 4973.6826 - val_mse: 621774400.0000 - val_mae: 4973.6802\n",
      "Epoch 50/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4207.4604 - mse: 703652864.0000 - mae: 4207.4844 - val_loss: 4644.3623 - val_mse: 598909824.0000 - val_mae: 4644.3618\n",
      "Epoch 51/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4513.2188 - mse: 733728640.0000 - mae: 4513.2490 - val_loss: 4657.0186 - val_mse: 545217792.0000 - val_mae: 4657.0181\n",
      "Epoch 52/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4023.6450 - mse: 694369536.0000 - mae: 4023.6731 - val_loss: 5356.8857 - val_mse: 621681024.0000 - val_mae: 5356.8862\n",
      "Epoch 53/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4364.7783 - mse: 717840512.0000 - mae: 4364.8101 - val_loss: 4078.9766 - val_mse: 538810304.0000 - val_mae: 4078.9783\n",
      "Epoch 54/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4407.9556 - mse: 721565632.0000 - mae: 4407.9883 - val_loss: 3932.5310 - val_mse: 515143072.0000 - val_mae: 3932.5300\n",
      "Epoch 55/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 4499.7588 - mse: 734391680.0000 - mae: 4499.7925 - val_loss: 4161.6177 - val_mse: 548223104.0000 - val_mae: 4161.6182\n",
      "Epoch 56/1024\n",
      "1258/1258 [==============================] - 152s 120ms/step - loss: 3865.1128 - mse: 684097856.0000 - mae: 3865.1399 - val_loss: 3780.2732 - val_mse: 492414464.0000 - val_mae: 3780.2739\n"
     ]
    }
   ],
   "source": [
    "# Selected model trained to convergence\n",
    "\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "\n",
    "input = keras.layers.Input(shape=(30,100), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-3')(dropout2)\n",
    "output = keras.layers.Dense(1, name='Output')(lstm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')\n",
    "print(model.summary())\n",
    "\n",
    "NAME='selected_model'\n",
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate=0.01),  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_selected.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03cbb93a-133a-4770-8573-bd4d6d30d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "    973/Unknown - 113s 113ms/step - loss: nan - mse: nan - mae: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(NAME))\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py:1221\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1221\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1223\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:436\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 436\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    319\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:354\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    353\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 354\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1032\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1032\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1104\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1103\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:554\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:550\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    549\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 550\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1149\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1149\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1115\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1114\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complex model multi dense\n",
    "\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-3')(dropout2)\n",
    "dense = keras.layers.Dense(128, name='Dense', activation='relu')(lstm3)\n",
    "output = keras.layers.Dense(1, name='Output')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')\n",
    "\n",
    "\n",
    "NAME='densex2c'\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),  loss='mse',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_densex2c.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mse', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, batch_size=128,\n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebcbd3-8213-4e55-8ecd-90f7ffdad381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model one dense, batch normalization\n",
    "\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "batchnorm1 = keras.layers.BatchNormalization(name='Batch-Normalization-1')(dropout1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-2')(batchnorm1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "batchnorm2 = keras.layers.BatchNormalization(name='Batch-Normalization-2')(dropout2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-3')(batchnorm2)\n",
    "dropout3 = keras.layers.Dropout(0.20, name='Dropout-3')(lstm3)\n",
    "batchnorm3 = keras.layers.BatchNormalization(name='Batch-Normalization-3')(dropout3)\n",
    "output = keras.layers.Dense(1, name='Output')(batchnorm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-BatchNorm')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "NAME='batchnorm'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_batchnorm.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d3f08-9917-4700-aa20-875fcedfa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added this to the data preprocessing, so not required\n",
    "\n",
    "class TimeEncoding(keras.layers.Layer):\n",
    "    \"\"\" Layer to encode cyclical and continuous time.  \n",
    "    Input should an n x 1 array or vector of integers.  \n",
    "    Integers represent number of time units (i.e., days) from the starting point\"\"\"\n",
    "\n",
    "    def __init__(self, cyclical_interval=365, continuous_interval=3650 , **kwargs):\n",
    "        self.cyclical_interval = cyclical_interval\n",
    "        self.continuous_interval = continuous_interval\n",
    "        super(TimeEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        cyclical_sin = np.sin((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        cyclical_cos = np.cos((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        continuous_sin = np.sin((x * 2 * np.pi)/self.continuous_interval)\n",
    "        continuous_cos = np.cos((x * 2 * np.pi)/self.continuous_interval)\n",
    "        \n",
    "        return keras.layers.concatenate([cyclical_sin, cyclical_cos, continuous_sin, continuous_cos], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42a910-0e7c-4bad-b21a-fc9b8a939924",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_encoding = TimeEncoding()\n",
    "x = np.arange(30)/1.0\n",
    "x = x[:, tf.newaxis]\n",
    "time_encoding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fb679-5415-4550-ad4c-e80deaa94f1f",
   "metadata": {},
   "source": [
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(512, return_sequences=True)(input)\n",
    "dropout1 = keras.layers.Dropout(0.20)(lstm1)\n",
    "lstm2 = keras.layers.LSTM(512, return_sequences=True)(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20)(lstm2)\n",
    "lstm3 = keras.layers.LSTM(512, return_sequences=True)(dropout2)\n",
    "dropout3 = keras.layers.Dropout(0.20)(lstm3)\n",
    "lstm4 = keras.layers.LSTM(512, return_sequences=True)(dropout3)\n",
    "dropout4 = keras.layers.Dropout(0.20)(lstm4)\n",
    "lstm5 = keras.layers.LSTM(512)(dropout4)\n",
    "output = keras.layers.Dense(1)(lstm5)\n",
    "model = keras.models.Model(inputs=input, outputs=output)\n",
    "\n",
    "\n",
    "Output with 5 LSTM @512, one dense layer\n",
    "\n",
    "Epoch 1/32\n",
    "1256/1256 [==============================] - 854s 677ms/step - loss: 1525.3854 - mse: 3823870.7500 - mae: 1525.3854 - val_loss: 1395.9611 - val_mse: 3222036.5000 - val_mae: 1395.9611\n",
    "Epoch 2/32\n",
    "1256/1256 [==============================] - 860s 685ms/step - loss: 1306.3831 - mse: 2720509.2500 - mae: 1306.3831 - val_loss: 1268.0597 - val_mse: 2458543.2500 - val_mae: 1268.0597\n",
    "Epoch 3/32\n",
    "1256/1256 [==============================] - 861s 686ms/step - loss: 1224.3491 - mse: 2193218.7500 - mae: 1224.3491 - val_loss: 1221.3195 - val_mse: 2095028.1250 - val_mae: 1221.3195\n",
    "Epoch 4/32\n",
    "1256/1256 [==============================] - 861s 685ms/step - loss: 1190.3538 - mse: 1940674.8750 - mae: 1190.3538 - val_loss: 1205.6909 - val_mse: 1935832.7500 - val_mae: 1205.6909\n",
    "Epoch 5/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1179.1588 - mse: 1829032.5000 - mae: 1179.1588 - val_loss: 1197.5588 - val_mse: 1859381.2500 - val_mae: 1197.5588\n",
    "Epoch 6/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1173.8285 - mse: 1778276.3750 - mae: 1173.8285 - val_loss: 1195.9708 - val_mse: 1827763.8750 - val_mae: 1195.9708\n",
    "Epoch 7/32\n",
    "1256/1256 [==============================] - 858s 683ms/step - loss: 1170.9568 - mse: 1751288.7500 - mae: 1170.9568 - val_loss: 1193.4899 - val_mse: 1804784.8750 - val_mae: 1193.4899\n",
    "Epoch 8/32\n",
    "1256/1256 [==============================] - 859s 684ms/step - loss: 1168.4807 - mse: 1736100.6250 - mae: 1168.4807 - val_loss: 1190.9672 - val_mse: 1791784.7500 - val_mae: 1190.9672\n",
    "Epoch 9/32\n",
    "1256/1256 [==============================] - 856s 682ms/step - loss: 1166.9685 - mse: 1730238.2500 - mae: 1166.9685 - val_loss: 1191.2921 - val_mse: 1792275.2500 - val_mae: 1191.2921\n",
    "Epoch 10/32\n",
    "1256/1256 [==============================] - 855s 681ms/step - loss: 1166.2148 - mse: 1729396.2500 - mae: 1166.2148 - val_loss: 1189.8693 - val_mse: 1789450.0000 - val_mae: 1189.8693\n",
    "Epoch 11/32\n",
    "1256/1256 [==============================] - 856s 681ms/step - loss: 1165.9210 - mse: 1729189.6250 - mae: 1165.9210 - val_loss: 1190.8143 - val_mse: 1792432.6250 - val_mae: 1190.8143\n",
    "Epoch 12/32\n",
    "1256/1256 [==============================] - 856s 682ms/step - loss: 1165.8933 - mse: 1730290.7500 - mae: 1165.8933 - val_loss: 1190.2284 - val_mse: 1791079.6250 - val_mae: 1190.2284\n",
    "Epoch 13/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1165.8029 - mse: 1731005.8750 - mae: 1165.8029 - val_loss: 1189.7284 - val_mse: 1790104.0000 - val_mae: 1189.7284\n",
    "Epoch 14/32\n",
    "1256/1256 [==============================] - 854s 680ms/step - loss: 1165.6666 - mse: 1729771.0000 - mae: 1165.6666 - val_loss: 1190.9232 - val_mse: 1793442.3750 - val_mae: 1190.9232\n",
    "Epoch 15/32\n",
    "1256/1256 [==============================] - 853s 680ms/step - loss: 1166.0372 - mse: 1731329.2500 - mae: 1166.0372 - val_loss: 1190.1742 - val_mse: 1791922.6250 - val_mae: 1190.1742\n",
    "Epoch 16/32\n",
    " 532/1256 [===========>..................] - ETA: 7:34 - loss: 1170.4701 - mse: 1743321.3750 - mae: 1170.4701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d016b9-bb92-4212-b14f-2ff53eb074a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
