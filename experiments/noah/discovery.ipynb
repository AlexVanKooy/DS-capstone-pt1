{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2d0698-a89c-4f60-8092-ad842f06b879",
   "metadata": {},
   "source": [
    "# Experiments Notebook 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8519c3-fa4d-4404-a8c7-9486a7e14861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d96a9b-5d5d-4b5d-b461-746bcc80384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-hypetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f293244-628b-4bee-a316-cf96b7b89bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import bz2\n",
    "from glob import glob\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import importlib\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kerashypetune import KerasGridSearch\n",
    "\n",
    "\n",
    "import preprocessing as pp\n",
    "import dsci592.model as dsci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b46a13d-bfa6-4e51-a90e-8d7fb47d8a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dsci592.model' from '/home/noah/projects/drexel/dsci592/DS-capstone-pt1/code/src/dsci592/model.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dsci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce25521f-bad6-4705-b439-eda844e32615",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183fcd9-48c1-4e6d-a197-4c39853316c7",
   "metadata": {},
   "source": [
    "### Load the golden data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c511402-262c-45a0-a9ae-148b6802fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows version\n",
    "golden_dataset_file_name = os.path.join('..', '..', 'data', 'golden', 'feeFiFoFum.pbz2')\n",
    "\n",
    "# data = bz2.BZ2File(golden_dataset_file_name,'rb')\n",
    "with bz2.BZ2File(golden_dataset_file_name,'rb') as data:\n",
    "    df = pd.read_pickle(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a67ff56-fe24-41c7-8942-56c66eb1451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dates</th>\n",
       "      <th>JHU_ConfirmedCases.data</th>\n",
       "      <th>NYT_ConfirmedCases.data</th>\n",
       "      <th>JHU_ConfirmedDeaths.data</th>\n",
       "      <th>NYT_ConfirmedDeaths.data</th>\n",
       "      <th>NYT_ConfirmedDeaths.missing</th>\n",
       "      <th>TotalPopulation.data</th>\n",
       "      <th>MaleAndFemale_AtLeast65_Population.data</th>\n",
       "      <th>Male_Total_Population.data</th>\n",
       "      <th>Female_Total_Population.data</th>\n",
       "      <th>MaleAndFemale_Under18_Population.data</th>\n",
       "      <th>BLS_EmployedPopulation.data</th>\n",
       "      <th>BLS_EmployedPopulation.missing</th>\n",
       "      <th>BLS_UnemployedPopulation.data</th>\n",
       "      <th>BLS_UnemployedPopulation.missing</th>\n",
       "      <th>BLS_UnemploymentRate.data</th>\n",
       "      <th>BLS_UnemploymentRate.missing</th>\n",
       "      <th>BLS_LaborForcePopulation.data</th>\n",
       "      <th>BLS_LaborForcePopulation.missing</th>\n",
       "      <th>AverageDailyTemperature.data</th>\n",
       "      <th>AverageDailyTemperature.missing</th>\n",
       "      <th>AverageDewPoint.data</th>\n",
       "      <th>AverageDewPoint.missing</th>\n",
       "      <th>AverageRelativeHumidity.data</th>\n",
       "      <th>AverageRelativeHumidity.missing</th>\n",
       "      <th>AverageSurfaceAirPressure.data</th>\n",
       "      <th>AverageSurfaceAirPressure.missing</th>\n",
       "      <th>AveragePrecipitationTotal.data</th>\n",
       "      <th>AveragePrecipitationTotal.missing</th>\n",
       "      <th>AveragePrecipitation.data</th>\n",
       "      <th>AveragePrecipitation.missing</th>\n",
       "      <th>AverageWindDirection.data</th>\n",
       "      <th>AverageWindDirection.missing</th>\n",
       "      <th>AverageWindSpeed.data</th>\n",
       "      <th>AverageWindSpeed.missing</th>\n",
       "      <th>hospitalIcuBeds</th>\n",
       "      <th>hospitalStaffedBeds</th>\n",
       "      <th>hospitalLicensedBeds</th>\n",
       "      <th>latestTotalPopulation</th>\n",
       "      <th>fips</th>\n",
       "      <th>county</th>\n",
       "      <th>jhu_daily_death</th>\n",
       "      <th>jhu_daily_cases</th>\n",
       "      <th>jhu_daily_new_cases</th>\n",
       "      <th>jhu_daily_death_rolling_7</th>\n",
       "      <th>jhu_daily_cases_rolling_7</th>\n",
       "      <th>jhu_daily_new_cases_rolling_7</th>\n",
       "      <th>jhu_daily_death_rolling_30</th>\n",
       "      <th>jhu_daily_cases_rolling_30</th>\n",
       "      <th>jhu_daily_new_cases_rolling_30</th>\n",
       "      <th>LND110210</th>\n",
       "      <th>jhu_death_rate</th>\n",
       "      <th>jhu_case_rate</th>\n",
       "      <th>jhu_new_case_rate</th>\n",
       "      <th>density</th>\n",
       "      <th>icu_beds_per_person</th>\n",
       "      <th>staffed_beds_per_person</th>\n",
       "      <th>licensed_beds_per_person</th>\n",
       "      <th>cold_days</th>\n",
       "      <th>hot_days</th>\n",
       "      <th>moderate_days</th>\n",
       "      <th>gte_65_percent</th>\n",
       "      <th>lt_18_percent</th>\n",
       "      <th>employed_percent</th>\n",
       "      <th>unemployed_percent</th>\n",
       "      <th>totalMoved</th>\n",
       "      <th>movedWithinState</th>\n",
       "      <th>movedWithoutState</th>\n",
       "      <th>movedFromAbroad</th>\n",
       "      <th>publicTrans</th>\n",
       "      <th>totalTrans</th>\n",
       "      <th>householdsTotal</th>\n",
       "      <th>houseWith65</th>\n",
       "      <th>house2+with65</th>\n",
       "      <th>houseFamily65</th>\n",
       "      <th>houseNonfam65</th>\n",
       "      <th>houseNo65</th>\n",
       "      <th>house2+No65</th>\n",
       "      <th>houseFamilyNo65</th>\n",
       "      <th>houseNonfamNo65</th>\n",
       "      <th>householdStructuresTotal</th>\n",
       "      <th>householdIncomeMedian</th>\n",
       "      <th>gini</th>\n",
       "      <th>hoursWorkedMean</th>\n",
       "      <th>unitsInStructure</th>\n",
       "      <th>healthInsTotal</th>\n",
       "      <th>healthInsNativeWith</th>\n",
       "      <th>healthInsForeignNatWith</th>\n",
       "      <th>healthInsForeignNoncitWith</th>\n",
       "      <th>healthInsForeignNatNo</th>\n",
       "      <th>healthInsForeignNoncitNo</th>\n",
       "      <th>healthInsNativeNo</th>\n",
       "      <th>countyStateName</th>\n",
       "      <th>stateFip</th>\n",
       "      <th>countyFip</th>\n",
       "      <th>pm25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.659722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.590139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.326389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.732639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.541667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.537708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.520417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.770833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.395833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.642708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.458333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.197917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.548944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.705556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>Abbeville_SouthCarolina_UnitedStates</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>490.48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.479</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>Abbeville County, South Carolina</td>\n",
       "      <td>45</td>\n",
       "      <td>001</td>\n",
       "      <td>9.618551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dates  JHU_ConfirmedCases.data  NYT_ConfirmedCases.data  \\\n",
       "0  2020-03-19                      1.0                      1.0   \n",
       "1  2020-03-20                      1.0                      1.0   \n",
       "2  2020-03-21                      1.0                      1.0   \n",
       "3  2020-03-22                      1.0                      1.0   \n",
       "4  2020-03-23                      1.0                      1.0   \n",
       "\n",
       "   JHU_ConfirmedDeaths.data  NYT_ConfirmedDeaths.data  \\\n",
       "0                       0.0                       0.0   \n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   NYT_ConfirmedDeaths.missing  TotalPopulation.data  \\\n",
       "0                            0               24527.0   \n",
       "1                            0               24527.0   \n",
       "2                            0               24527.0   \n",
       "3                            0               24527.0   \n",
       "4                            0               24527.0   \n",
       "\n",
       "   MaleAndFemale_AtLeast65_Population.data  Male_Total_Population.data  \\\n",
       "0                                   5343.0                     11868.0   \n",
       "1                                   5343.0                     11868.0   \n",
       "2                                   5343.0                     11868.0   \n",
       "3                                   5343.0                     11868.0   \n",
       "4                                   5343.0                     11868.0   \n",
       "\n",
       "   Female_Total_Population.data  MaleAndFemale_Under18_Population.data  \\\n",
       "0                       12673.0                                 4924.0   \n",
       "1                       12673.0                                 4924.0   \n",
       "2                       12673.0                                 4924.0   \n",
       "3                       12673.0                                 4924.0   \n",
       "4                       12673.0                                 4924.0   \n",
       "\n",
       "   BLS_EmployedPopulation.data  BLS_EmployedPopulation.missing  \\\n",
       "0                       9716.5                             0.0   \n",
       "1                       9716.5                             0.0   \n",
       "2                       9716.5                             0.0   \n",
       "3                       9716.5                             0.0   \n",
       "4                       9716.5                             0.0   \n",
       "\n",
       "   BLS_UnemployedPopulation.data  BLS_UnemployedPopulation.missing  \\\n",
       "0                          386.5                               0.0   \n",
       "1                          386.5                               0.0   \n",
       "2                          386.5                               0.0   \n",
       "3                          386.5                               0.0   \n",
       "4                          386.5                               0.0   \n",
       "\n",
       "   BLS_UnemploymentRate.data  BLS_UnemploymentRate.missing  \\\n",
       "0                   3.825596                           0.0   \n",
       "1                   3.825596                           0.0   \n",
       "2                   3.825596                           0.0   \n",
       "3                   3.825596                           0.0   \n",
       "4                   3.825596                           0.0   \n",
       "\n",
       "   BLS_LaborForcePopulation.data  BLS_LaborForcePopulation.missing  \\\n",
       "0                        10103.0                               0.0   \n",
       "1                        10103.0                               0.0   \n",
       "2                        10103.0                               0.0   \n",
       "3                        10103.0                               0.0   \n",
       "4                        10103.0                               0.0   \n",
       "\n",
       "   AverageDailyTemperature.data  AverageDailyTemperature.missing  \\\n",
       "0                     68.659722                              0.0   \n",
       "1                     72.541667                              0.0   \n",
       "2                     69.208333                              0.0   \n",
       "3                     53.916667                              0.0   \n",
       "4                     49.863889                              0.0   \n",
       "\n",
       "   AverageDewPoint.data  AverageDewPoint.missing  \\\n",
       "0             63.152778                      0.0   \n",
       "1             60.583333                      0.0   \n",
       "2             56.708333                      0.0   \n",
       "3             49.770833                      0.0   \n",
       "4             49.863889                      0.0   \n",
       "\n",
       "   AverageRelativeHumidity.data  AverageRelativeHumidity.missing  \\\n",
       "0                     84.152778                              0.0   \n",
       "1                     68.708333                              0.0   \n",
       "2                     66.583333                              0.0   \n",
       "3                     86.395833                              0.0   \n",
       "4                    100.000000                              0.0   \n",
       "\n",
       "   AverageSurfaceAirPressure.data  AverageSurfaceAirPressure.missing  \\\n",
       "0                       29.590139                                0.0   \n",
       "1                       29.537708                                0.0   \n",
       "2                       29.520417                                0.0   \n",
       "3                       29.642708                                0.0   \n",
       "4                       29.548944                                0.0   \n",
       "\n",
       "   AveragePrecipitationTotal.data  AveragePrecipitationTotal.missing  \\\n",
       "0                        0.000833                                0.0   \n",
       "1                        0.000000                                0.0   \n",
       "2                        0.000000                                0.0   \n",
       "3                        0.000000                                0.0   \n",
       "4                        0.013750                                0.0   \n",
       "\n",
       "   AveragePrecipitation.data  AveragePrecipitation.missing  \\\n",
       "0                   0.000000                           0.0   \n",
       "1                   0.000000                           0.0   \n",
       "2                   0.000000                           0.0   \n",
       "3                   0.011354                           0.0   \n",
       "4                   0.008042                           0.0   \n",
       "\n",
       "   AverageWindDirection.data  AverageWindDirection.missing  \\\n",
       "0                 192.326389                           0.0   \n",
       "1                 208.125000                           0.0   \n",
       "2                 252.916667                           0.0   \n",
       "3                  91.458333                           0.0   \n",
       "4                  83.333333                           0.0   \n",
       "\n",
       "   AverageWindSpeed.data  AverageWindSpeed.missing  hospitalIcuBeds  \\\n",
       "0               6.732639                       0.0              6.0   \n",
       "1              10.833333                       0.0              6.0   \n",
       "2               8.125000                       0.0              6.0   \n",
       "3               7.197917                       0.0              6.0   \n",
       "4               3.705556                       0.0              6.0   \n",
       "\n",
       "   hospitalStaffedBeds  hospitalLicensedBeds  latestTotalPopulation   fips  \\\n",
       "0                 25.0                  25.0                24527.0  45001   \n",
       "1                 25.0                  25.0                24527.0  45001   \n",
       "2                 25.0                  25.0                24527.0  45001   \n",
       "3                 25.0                  25.0                24527.0  45001   \n",
       "4                 25.0                  25.0                24527.0  45001   \n",
       "\n",
       "                                 county  jhu_daily_death  jhu_daily_cases  \\\n",
       "0  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "1  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "2  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "3  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "4  Abbeville_SouthCarolina_UnitedStates              0.0              0.0   \n",
       "\n",
       "   jhu_daily_new_cases  jhu_daily_death_rolling_7  jhu_daily_cases_rolling_7  \\\n",
       "0                  0.0                        0.0                        0.0   \n",
       "1                  0.0                        0.0                        0.0   \n",
       "2                  0.0                        0.0                        0.0   \n",
       "3                  0.0                        0.0                        0.0   \n",
       "4                  0.0                        0.0                        0.0   \n",
       "\n",
       "   jhu_daily_new_cases_rolling_7  jhu_daily_death_rolling_30  \\\n",
       "0                            0.0                         0.0   \n",
       "1                            0.0                         0.0   \n",
       "2                            0.0                         0.0   \n",
       "3                            0.0                         0.0   \n",
       "4                            0.0                         0.0   \n",
       "\n",
       "   jhu_daily_cases_rolling_30  jhu_daily_new_cases_rolling_30  LND110210  \\\n",
       "0                         0.0                             0.0     490.48   \n",
       "1                         0.0                             0.0     490.48   \n",
       "2                         0.0                             0.0     490.48   \n",
       "3                         0.0                             0.0     490.48   \n",
       "4                         0.0                             0.0     490.48   \n",
       "\n",
       "   jhu_death_rate  jhu_case_rate  jhu_new_case_rate    density  \\\n",
       "0             0.0            0.0                0.0  50.006116   \n",
       "1             0.0            0.0                0.0  50.006116   \n",
       "2             0.0            0.0                0.0  50.006116   \n",
       "3             0.0            0.0                0.0  50.006116   \n",
       "4             0.0            0.0                0.0  50.006116   \n",
       "\n",
       "   icu_beds_per_person  staffed_beds_per_person  licensed_beds_per_person  \\\n",
       "0             0.000245                 0.001019                  0.001019   \n",
       "1             0.000245                 0.001019                  0.001019   \n",
       "2             0.000245                 0.001019                  0.001019   \n",
       "3             0.000245                 0.001019                  0.001019   \n",
       "4             0.000245                 0.001019                  0.001019   \n",
       "\n",
       "   cold_days  hot_days  moderate_days  gte_65_percent  lt_18_percent  \\\n",
       "0          0         0              1        0.217842       0.200758   \n",
       "1          0         0              1        0.217842       0.200758   \n",
       "2          0         0              1        0.217842       0.200758   \n",
       "3          0         0              1        0.217842       0.200758   \n",
       "4          1         0              0        0.217842       0.200758   \n",
       "\n",
       "   employed_percent  unemployed_percent  totalMoved  movedWithinState  \\\n",
       "0          0.961744            0.038256     24317.0            1118.0   \n",
       "1          0.961744            0.038256     24317.0            1118.0   \n",
       "2          0.961744            0.038256     24317.0            1118.0   \n",
       "3          0.961744            0.038256     24317.0            1118.0   \n",
       "4          0.961744            0.038256     24317.0            1118.0   \n",
       "\n",
       "   movedWithoutState  movedFromAbroad  publicTrans  totalTrans  \\\n",
       "0              329.0             12.0           44        9791   \n",
       "1              329.0             12.0           44        9791   \n",
       "2              329.0             12.0           44        9791   \n",
       "3              329.0             12.0           44        9791   \n",
       "4              329.0             12.0           44        9791   \n",
       "\n",
       "   householdsTotal  houseWith65  house2+with65  houseFamily65  houseNonfam65  \\\n",
       "0             9660         3645           2081           2042             39   \n",
       "1             9660         3645           2081           2042             39   \n",
       "2             9660         3645           2081           2042             39   \n",
       "3             9660         3645           2081           2042             39   \n",
       "4             9660         3645           2081           2042             39   \n",
       "\n",
       "   houseNo65  house2+No65  houseFamilyNo65  houseNonfamNo65  \\\n",
       "0       6015         4531             4206              325   \n",
       "1       6015         4531             4206              325   \n",
       "2       6015         4531             4206              325   \n",
       "3       6015         4531             4206              325   \n",
       "4       6015         4531             4206              325   \n",
       "\n",
       "   householdStructuresTotal  householdIncomeMedian   gini  hoursWorkedMean  \\\n",
       "0                      9660                  38741  0.479             38.1   \n",
       "1                      9660                  38741  0.479             38.1   \n",
       "2                      9660                  38741  0.479             38.1   \n",
       "3                      9660                  38741  0.479             38.1   \n",
       "4                      9660                  38741  0.479             38.1   \n",
       "\n",
       "   unitsInStructure  healthInsTotal  healthInsNativeWith  \\\n",
       "0             12191           24397                21415   \n",
       "1             12191           24397                21415   \n",
       "2             12191           24397                21415   \n",
       "3             12191           24397                21415   \n",
       "4             12191           24397                21415   \n",
       "\n",
       "   healthInsForeignNatWith  healthInsForeignNoncitWith  healthInsForeignNatNo  \\\n",
       "0                      179                          87                     39   \n",
       "1                      179                          87                     39   \n",
       "2                      179                          87                     39   \n",
       "3                      179                          87                     39   \n",
       "4                      179                          87                     39   \n",
       "\n",
       "   healthInsForeignNoncitNo  healthInsNativeNo  \\\n",
       "0                       122               2555   \n",
       "1                       122               2555   \n",
       "2                       122               2555   \n",
       "3                       122               2555   \n",
       "4                       122               2555   \n",
       "\n",
       "                    countyStateName stateFip countyFip      pm25  \n",
       "0  Abbeville County, South Carolina       45       001  9.618551  \n",
       "1  Abbeville County, South Carolina       45       001  9.618551  \n",
       "2  Abbeville County, South Carolina       45       001  9.618551  \n",
       "3  Abbeville County, South Carolina       45       001  9.618551  \n",
       "4  Abbeville County, South Carolina       45       001  9.618551  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linux version (problems with bz2 file)\n",
    "\n",
    "golden_dataset_file_name = os.path.join('/', 'data', 'projects', 'drexel', 'dsci592', 'feeFiFoFum.pkl')\n",
    "\n",
    "df = pd.read_pickle(golden_dataset_file_name)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4fde0d-13e5-4198-9f8e-5689f1130301",
   "metadata": {},
   "source": [
    "### Clean up the data\n",
    "\n",
    "#### Drop non-numeric and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e761798-4dba-4c6c-96db-04ba85fb6d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['NYT_ConfirmedCases.data','NYT_ConfirmedDeaths.data','NYT_ConfirmedDeaths.missing','county','LND110210','countyStateName','stateFip','countyFip']\n",
    "\n",
    "df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf9d9-f105-4376-80ce-65a15ad0bca0",
   "metadata": {},
   "source": [
    "#### Temporarily, replace FIPS codes with latitude and longitude of the centroid of each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d613f0d-2b87-4c84-80d1-a34cb680909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = pd.read_csv('2021_Gaz_counties_national.txt', delimiter='\\t')\n",
    "counties.rename(columns={'INTPTLONG                                                                                                               ': 'longitude',\n",
    "                        'INTPTLAT': 'latitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8cce69e-9e11-4ebf-ad6a-7257ea693173",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = counties[['GEOID', 'latitude', 'longitude' ]]\n",
    "df.fips = df.fips.astype('int64')\n",
    "\n",
    "df = df.merge(counties, how='left', left_on='fips', right_on='GEOID')\n",
    "df.drop(['GEOID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324af6e-050d-4195-b5d9-4c9d51dac540",
   "metadata": {},
   "source": [
    "#### Replace dates with monotonically increasing integers starting with the minimum date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54d464b3-5904-4671-9ef8-97b2450bfc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-03-11 00:00:00'),\n",
       " Timestamp('2022-01-16 00:00:00'),\n",
       " dtype('<M8[ns]'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dates = pd.to_datetime(df.dates, format='%Y-%m-%d')\n",
    "min_date = min(df.dates)\n",
    "max_date = max(df.dates)\n",
    "min_date, max_date, df.dates.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82a52c51-f7f0-4835-a57e-8ec4298e220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] =(df.dates - min_date).dt.days\n",
    "df.drop(['dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0581ee-2583-4748-8a3c-e4ccadc6a3ba",
   "metadata": {},
   "source": [
    "#### Replace the integer representation of date with sin and cosine encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4517112d-13a1-4685-9f96-22e5ece969b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclical_interval = 365\n",
    "continuous_interval = 3650\n",
    "df['cyclical_sin'] = np.sin((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['cyclical_cos'] = np.cos((df.day * 2 * np.pi)/cyclical_interval)\n",
    "df['continuous_sin'] = np.sin((df.day * 2 * np.pi)/continuous_interval)\n",
    "df['continuous_cos'] = np.cos((df.day * 2 * np.pi)/continuous_interval)\n",
    "df.drop('day', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc592ca-b282-47f6-a26c-da7033ac9194",
   "metadata": {},
   "source": [
    "#### Get the feature column for latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5743a233-aa53-4a8d-a047-4f05805fac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossed_latlong = pp.get_latlong_fc(df)\n",
    "\n",
    "lat_buckets = list(np.linspace(df.latitude.min(), df.latitude.max(),100))\n",
    "long_buckets = list(np.linspace(df.longitude.min(), df.longitude.max(),100))\n",
    "\n",
    "#make feature columns\n",
    "lat_fc = tf.feature_column.bucketized_column(tf.feature_column.numeric_column('latitude'),lat_buckets)\n",
    "long_fc= tf.feature_column.bucketized_column(tf.feature_column.numeric_column('longitude'),long_buckets)\n",
    "    \n",
    "# crossed columns tell the model how the features relate\n",
    "crossed_latlong = tf.feature_column.crossed_column(keys=[lat_fc, long_fc], hash_bucket_size=1000) # No precise rule, maybe 1000 buckets will be good?\n",
    "    \n",
    "embedded_latlong = tf.feature_column.embedding_column(crossed_latlong,9)\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(embedded_latlong)\n",
    "\n",
    "df[['geo0', 'geo1', 'geo2','geo3', 'geo4','geo5','geo6','geo7','geo8']] = feature_layer({'latitude': df.latitude, 'longitude': df.longitude})\n",
    "\n",
    "# df.drop(['longitude', 'latitude'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "961ad6ca-75b4-4be9-84d7-5deddf14bd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JHU_ConfirmedCases.data</th>\n",
       "      <th>JHU_ConfirmedDeaths.data</th>\n",
       "      <th>TotalPopulation.data</th>\n",
       "      <th>MaleAndFemale_AtLeast65_Population.data</th>\n",
       "      <th>Male_Total_Population.data</th>\n",
       "      <th>Female_Total_Population.data</th>\n",
       "      <th>MaleAndFemale_Under18_Population.data</th>\n",
       "      <th>BLS_EmployedPopulation.data</th>\n",
       "      <th>BLS_EmployedPopulation.missing</th>\n",
       "      <th>BLS_UnemployedPopulation.data</th>\n",
       "      <th>BLS_UnemployedPopulation.missing</th>\n",
       "      <th>BLS_UnemploymentRate.data</th>\n",
       "      <th>BLS_UnemploymentRate.missing</th>\n",
       "      <th>BLS_LaborForcePopulation.data</th>\n",
       "      <th>BLS_LaborForcePopulation.missing</th>\n",
       "      <th>AverageDailyTemperature.data</th>\n",
       "      <th>AverageDailyTemperature.missing</th>\n",
       "      <th>AverageDewPoint.data</th>\n",
       "      <th>AverageDewPoint.missing</th>\n",
       "      <th>AverageRelativeHumidity.data</th>\n",
       "      <th>AverageRelativeHumidity.missing</th>\n",
       "      <th>AverageSurfaceAirPressure.data</th>\n",
       "      <th>AverageSurfaceAirPressure.missing</th>\n",
       "      <th>AveragePrecipitationTotal.data</th>\n",
       "      <th>AveragePrecipitationTotal.missing</th>\n",
       "      <th>AveragePrecipitation.data</th>\n",
       "      <th>AveragePrecipitation.missing</th>\n",
       "      <th>AverageWindDirection.data</th>\n",
       "      <th>AverageWindDirection.missing</th>\n",
       "      <th>AverageWindSpeed.data</th>\n",
       "      <th>AverageWindSpeed.missing</th>\n",
       "      <th>hospitalIcuBeds</th>\n",
       "      <th>hospitalStaffedBeds</th>\n",
       "      <th>hospitalLicensedBeds</th>\n",
       "      <th>latestTotalPopulation</th>\n",
       "      <th>fips</th>\n",
       "      <th>jhu_daily_death</th>\n",
       "      <th>jhu_daily_cases</th>\n",
       "      <th>jhu_daily_new_cases</th>\n",
       "      <th>jhu_daily_death_rolling_7</th>\n",
       "      <th>jhu_daily_cases_rolling_7</th>\n",
       "      <th>jhu_daily_new_cases_rolling_7</th>\n",
       "      <th>jhu_daily_death_rolling_30</th>\n",
       "      <th>jhu_daily_cases_rolling_30</th>\n",
       "      <th>jhu_daily_new_cases_rolling_30</th>\n",
       "      <th>jhu_death_rate</th>\n",
       "      <th>jhu_case_rate</th>\n",
       "      <th>jhu_new_case_rate</th>\n",
       "      <th>density</th>\n",
       "      <th>icu_beds_per_person</th>\n",
       "      <th>staffed_beds_per_person</th>\n",
       "      <th>licensed_beds_per_person</th>\n",
       "      <th>cold_days</th>\n",
       "      <th>hot_days</th>\n",
       "      <th>moderate_days</th>\n",
       "      <th>gte_65_percent</th>\n",
       "      <th>lt_18_percent</th>\n",
       "      <th>employed_percent</th>\n",
       "      <th>unemployed_percent</th>\n",
       "      <th>totalMoved</th>\n",
       "      <th>movedWithinState</th>\n",
       "      <th>movedWithoutState</th>\n",
       "      <th>movedFromAbroad</th>\n",
       "      <th>publicTrans</th>\n",
       "      <th>totalTrans</th>\n",
       "      <th>householdsTotal</th>\n",
       "      <th>houseWith65</th>\n",
       "      <th>house2+with65</th>\n",
       "      <th>houseFamily65</th>\n",
       "      <th>houseNonfam65</th>\n",
       "      <th>houseNo65</th>\n",
       "      <th>house2+No65</th>\n",
       "      <th>houseFamilyNo65</th>\n",
       "      <th>houseNonfamNo65</th>\n",
       "      <th>householdStructuresTotal</th>\n",
       "      <th>householdIncomeMedian</th>\n",
       "      <th>gini</th>\n",
       "      <th>hoursWorkedMean</th>\n",
       "      <th>unitsInStructure</th>\n",
       "      <th>healthInsTotal</th>\n",
       "      <th>healthInsNativeWith</th>\n",
       "      <th>healthInsForeignNatWith</th>\n",
       "      <th>healthInsForeignNoncitWith</th>\n",
       "      <th>healthInsForeignNatNo</th>\n",
       "      <th>healthInsForeignNoncitNo</th>\n",
       "      <th>healthInsNativeNo</th>\n",
       "      <th>pm25</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>cyclical_sin</th>\n",
       "      <th>cyclical_cos</th>\n",
       "      <th>continuous_sin</th>\n",
       "      <th>continuous_cos</th>\n",
       "      <th>geo0</th>\n",
       "      <th>geo1</th>\n",
       "      <th>geo2</th>\n",
       "      <th>geo3</th>\n",
       "      <th>geo4</th>\n",
       "      <th>geo5</th>\n",
       "      <th>geo6</th>\n",
       "      <th>geo7</th>\n",
       "      <th>geo8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.659722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.152778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.590139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.326389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.732639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.137279</td>\n",
       "      <td>0.990532</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.999905</td>\n",
       "      <td>-0.206256</td>\n",
       "      <td>0.121853</td>\n",
       "      <td>-0.059924</td>\n",
       "      <td>-0.341429</td>\n",
       "      <td>-0.240499</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>-0.162839</td>\n",
       "      <td>-0.213781</td>\n",
       "      <td>0.165044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.541667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.537708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.154309</td>\n",
       "      <td>0.988023</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>0.999880</td>\n",
       "      <td>-0.206256</td>\n",
       "      <td>0.121853</td>\n",
       "      <td>-0.059924</td>\n",
       "      <td>-0.341429</td>\n",
       "      <td>-0.240499</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>-0.162839</td>\n",
       "      <td>-0.213781</td>\n",
       "      <td>0.165044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.208333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.520417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.171293</td>\n",
       "      <td>0.985220</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>-0.206256</td>\n",
       "      <td>0.121853</td>\n",
       "      <td>-0.059924</td>\n",
       "      <td>-0.341429</td>\n",
       "      <td>-0.240499</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>-0.162839</td>\n",
       "      <td>-0.213781</td>\n",
       "      <td>0.165044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.770833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.395833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.642708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91.458333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.197917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.188227</td>\n",
       "      <td>0.982126</td>\n",
       "      <td>0.018934</td>\n",
       "      <td>0.999821</td>\n",
       "      <td>-0.206256</td>\n",
       "      <td>0.121853</td>\n",
       "      <td>-0.059924</td>\n",
       "      <td>-0.341429</td>\n",
       "      <td>-0.240499</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>-0.162839</td>\n",
       "      <td>-0.213781</td>\n",
       "      <td>0.165044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>5343.0</td>\n",
       "      <td>11868.0</td>\n",
       "      <td>12673.0</td>\n",
       "      <td>4924.0</td>\n",
       "      <td>9716.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>386.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.825596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10103.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.863889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.548944</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.705556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>24527.0</td>\n",
       "      <td>45001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.006116</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217842</td>\n",
       "      <td>0.200758</td>\n",
       "      <td>0.961744</td>\n",
       "      <td>0.038256</td>\n",
       "      <td>24317.0</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>44</td>\n",
       "      <td>9791</td>\n",
       "      <td>9660</td>\n",
       "      <td>3645</td>\n",
       "      <td>2081</td>\n",
       "      <td>2042</td>\n",
       "      <td>39</td>\n",
       "      <td>6015</td>\n",
       "      <td>4531</td>\n",
       "      <td>4206</td>\n",
       "      <td>325</td>\n",
       "      <td>9660</td>\n",
       "      <td>38741</td>\n",
       "      <td>0.4790</td>\n",
       "      <td>38.1</td>\n",
       "      <td>12191</td>\n",
       "      <td>24397</td>\n",
       "      <td>21415</td>\n",
       "      <td>179</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>122</td>\n",
       "      <td>2555</td>\n",
       "      <td>9.618551</td>\n",
       "      <td>34.229041</td>\n",
       "      <td>-82.454058</td>\n",
       "      <td>0.205104</td>\n",
       "      <td>0.978740</td>\n",
       "      <td>0.020656</td>\n",
       "      <td>0.999787</td>\n",
       "      <td>-0.206256</td>\n",
       "      <td>0.121853</td>\n",
       "      <td>-0.059924</td>\n",
       "      <td>-0.341429</td>\n",
       "      <td>-0.240499</td>\n",
       "      <td>0.052870</td>\n",
       "      <td>-0.162839</td>\n",
       "      <td>-0.213781</td>\n",
       "      <td>0.165044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879584</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.302083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>283.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.791667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>0.541628</td>\n",
       "      <td>0.915518</td>\n",
       "      <td>0.402276</td>\n",
       "      <td>0.071524</td>\n",
       "      <td>0.178312</td>\n",
       "      <td>0.439406</td>\n",
       "      <td>0.202796</td>\n",
       "      <td>-0.297441</td>\n",
       "      <td>0.400439</td>\n",
       "      <td>0.367141</td>\n",
       "      <td>-0.038317</td>\n",
       "      <td>0.179132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879585</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.854167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.958333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.625000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.303125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>209.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.291667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.831171</td>\n",
       "      <td>0.556017</td>\n",
       "      <td>0.916210</td>\n",
       "      <td>0.400700</td>\n",
       "      <td>0.071524</td>\n",
       "      <td>0.178312</td>\n",
       "      <td>0.439406</td>\n",
       "      <td>0.202796</td>\n",
       "      <td>-0.297441</td>\n",
       "      <td>0.400439</td>\n",
       "      <td>0.367141</td>\n",
       "      <td>-0.038317</td>\n",
       "      <td>0.179132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879586</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.104167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.729167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.562500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.498125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.821477</td>\n",
       "      <td>0.570242</td>\n",
       "      <td>0.916898</td>\n",
       "      <td>0.399122</td>\n",
       "      <td>0.071524</td>\n",
       "      <td>0.178312</td>\n",
       "      <td>0.439406</td>\n",
       "      <td>0.202796</td>\n",
       "      <td>-0.297441</td>\n",
       "      <td>0.400439</td>\n",
       "      <td>0.367141</td>\n",
       "      <td>-0.038317</td>\n",
       "      <td>0.179132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879587</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.412500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>208.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.416667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.811539</td>\n",
       "      <td>0.584298</td>\n",
       "      <td>0.917584</td>\n",
       "      <td>0.397543</td>\n",
       "      <td>0.071524</td>\n",
       "      <td>0.178312</td>\n",
       "      <td>0.439406</td>\n",
       "      <td>0.202796</td>\n",
       "      <td>-0.297441</td>\n",
       "      <td>0.400439</td>\n",
       "      <td>0.367141</td>\n",
       "      <td>-0.038317</td>\n",
       "      <td>0.179132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879588</th>\n",
       "      <td>501.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>1369.0</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>774.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.740157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.708333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.750000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.265000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>46137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>1.405212</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095428</td>\n",
       "      <td>0.280842</td>\n",
       "      <td>0.962598</td>\n",
       "      <td>0.037402</td>\n",
       "      <td>2753.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>951</td>\n",
       "      <td>754</td>\n",
       "      <td>185</td>\n",
       "      <td>137</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>569</td>\n",
       "      <td>482</td>\n",
       "      <td>474</td>\n",
       "      <td>8</td>\n",
       "      <td>754</td>\n",
       "      <td>37400</td>\n",
       "      <td>0.4834</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1009</td>\n",
       "      <td>2791</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>751</td>\n",
       "      <td>4.322196</td>\n",
       "      <td>44.989764</td>\n",
       "      <td>-101.660827</td>\n",
       "      <td>-0.801361</td>\n",
       "      <td>0.598181</td>\n",
       "      <td>0.918267</td>\n",
       "      <td>0.395963</td>\n",
       "      <td>0.071524</td>\n",
       "      <td>0.178312</td>\n",
       "      <td>0.439406</td>\n",
       "      <td>0.202796</td>\n",
       "      <td>-0.297441</td>\n",
       "      <td>0.400439</td>\n",
       "      <td>0.367141</td>\n",
       "      <td>-0.038317</td>\n",
       "      <td>0.179132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1879589 rows  102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         JHU_ConfirmedCases.data  JHU_ConfirmedDeaths.data  \\\n",
       "0                            1.0                       0.0   \n",
       "1                            1.0                       0.0   \n",
       "2                            1.0                       0.0   \n",
       "3                            1.0                       0.0   \n",
       "4                            1.0                       0.0   \n",
       "...                          ...                       ...   \n",
       "1879584                    501.0                      11.0   \n",
       "1879585                    501.0                      11.0   \n",
       "1879586                    501.0                      11.0   \n",
       "1879587                    501.0                      11.0   \n",
       "1879588                    501.0                      11.0   \n",
       "\n",
       "         TotalPopulation.data  MaleAndFemale_AtLeast65_Population.data  \\\n",
       "0                     24527.0                                   5343.0   \n",
       "1                     24527.0                                   5343.0   \n",
       "2                     24527.0                                   5343.0   \n",
       "3                     24527.0                                   5343.0   \n",
       "4                     24527.0                                   5343.0   \n",
       "...                       ...                                      ...   \n",
       "1879584                2756.0                                    263.0   \n",
       "1879585                2756.0                                    263.0   \n",
       "1879586                2756.0                                    263.0   \n",
       "1879587                2756.0                                    263.0   \n",
       "1879588                2756.0                                    263.0   \n",
       "\n",
       "         Male_Total_Population.data  Female_Total_Population.data  \\\n",
       "0                           11868.0                       12673.0   \n",
       "1                           11868.0                       12673.0   \n",
       "2                           11868.0                       12673.0   \n",
       "3                           11868.0                       12673.0   \n",
       "4                           11868.0                       12673.0   \n",
       "...                             ...                           ...   \n",
       "1879584                      1369.0                        1373.0   \n",
       "1879585                      1369.0                        1373.0   \n",
       "1879586                      1369.0                        1373.0   \n",
       "1879587                      1369.0                        1373.0   \n",
       "1879588                      1369.0                        1373.0   \n",
       "\n",
       "         MaleAndFemale_Under18_Population.data  BLS_EmployedPopulation.data  \\\n",
       "0                                       4924.0                       9716.5   \n",
       "1                                       4924.0                       9716.5   \n",
       "2                                       4924.0                       9716.5   \n",
       "3                                       4924.0                       9716.5   \n",
       "4                                       4924.0                       9716.5   \n",
       "...                                        ...                          ...   \n",
       "1879584                                  774.0                        978.0   \n",
       "1879585                                  774.0                        978.0   \n",
       "1879586                                  774.0                        978.0   \n",
       "1879587                                  774.0                        978.0   \n",
       "1879588                                  774.0                        978.0   \n",
       "\n",
       "         BLS_EmployedPopulation.missing  BLS_UnemployedPopulation.data  \\\n",
       "0                                   0.0                          386.5   \n",
       "1                                   0.0                          386.5   \n",
       "2                                   0.0                          386.5   \n",
       "3                                   0.0                          386.5   \n",
       "4                                   0.0                          386.5   \n",
       "...                                 ...                            ...   \n",
       "1879584                             0.0                           38.0   \n",
       "1879585                             0.0                           38.0   \n",
       "1879586                             0.0                           38.0   \n",
       "1879587                             0.0                           38.0   \n",
       "1879588                             0.0                           38.0   \n",
       "\n",
       "         BLS_UnemployedPopulation.missing  BLS_UnemploymentRate.data  \\\n",
       "0                                     0.0                   3.825596   \n",
       "1                                     0.0                   3.825596   \n",
       "2                                     0.0                   3.825596   \n",
       "3                                     0.0                   3.825596   \n",
       "4                                     0.0                   3.825596   \n",
       "...                                   ...                        ...   \n",
       "1879584                               0.0                   3.740157   \n",
       "1879585                               0.0                   3.740157   \n",
       "1879586                               0.0                   3.740157   \n",
       "1879587                               0.0                   3.740157   \n",
       "1879588                               0.0                   3.740157   \n",
       "\n",
       "         BLS_UnemploymentRate.missing  BLS_LaborForcePopulation.data  \\\n",
       "0                                 0.0                        10103.0   \n",
       "1                                 0.0                        10103.0   \n",
       "2                                 0.0                        10103.0   \n",
       "3                                 0.0                        10103.0   \n",
       "4                                 0.0                        10103.0   \n",
       "...                               ...                            ...   \n",
       "1879584                           0.0                         1016.0   \n",
       "1879585                           0.0                         1016.0   \n",
       "1879586                           0.0                         1016.0   \n",
       "1879587                           0.0                         1016.0   \n",
       "1879588                           0.0                         1016.0   \n",
       "\n",
       "         BLS_LaborForcePopulation.missing  AverageDailyTemperature.data  \\\n",
       "0                                     0.0                     68.659722   \n",
       "1                                     0.0                     72.541667   \n",
       "2                                     0.0                     69.208333   \n",
       "3                                     0.0                     53.916667   \n",
       "4                                     0.0                     49.863889   \n",
       "...                                   ...                           ...   \n",
       "1879584                               0.0                     40.666667   \n",
       "1879585                               0.0                     36.854167   \n",
       "1879586                               0.0                     24.104167   \n",
       "1879587                               0.0                     28.500000   \n",
       "1879588                               0.0                     33.708333   \n",
       "\n",
       "         AverageDailyTemperature.missing  AverageDewPoint.data  \\\n",
       "0                                    0.0             63.152778   \n",
       "1                                    0.0             60.583333   \n",
       "2                                    0.0             56.708333   \n",
       "3                                    0.0             49.770833   \n",
       "4                                    0.0             49.863889   \n",
       "...                                  ...                   ...   \n",
       "1879584                              0.0             27.625000   \n",
       "1879585                              0.0             30.958333   \n",
       "1879586                              0.0             19.729167   \n",
       "1879587                              0.0             19.833333   \n",
       "1879588                              0.0             24.708333   \n",
       "\n",
       "         AverageDewPoint.missing  AverageRelativeHumidity.data  \\\n",
       "0                            0.0                     84.152778   \n",
       "1                            0.0                     68.708333   \n",
       "2                            0.0                     66.583333   \n",
       "3                            0.0                     86.395833   \n",
       "4                            0.0                    100.000000   \n",
       "...                          ...                           ...   \n",
       "1879584                      0.0                     60.708333   \n",
       "1879585                      0.0                     79.625000   \n",
       "1879586                      0.0                     83.562500   \n",
       "1879587                      0.0                     72.000000   \n",
       "1879588                      0.0                     70.750000   \n",
       "\n",
       "         AverageRelativeHumidity.missing  AverageSurfaceAirPressure.data  \\\n",
       "0                                    0.0                       29.590139   \n",
       "1                                    0.0                       29.537708   \n",
       "2                                    0.0                       29.520417   \n",
       "3                                    0.0                       29.642708   \n",
       "4                                    0.0                       29.548944   \n",
       "...                                  ...                             ...   \n",
       "1879584                              0.0                       27.302083   \n",
       "1879585                              0.0                       27.303125   \n",
       "1879586                              0.0                       27.498125   \n",
       "1879587                              0.0                       27.412500   \n",
       "1879588                              0.0                       27.265000   \n",
       "\n",
       "         AverageSurfaceAirPressure.missing  AveragePrecipitationTotal.data  \\\n",
       "0                                      0.0                        0.000833   \n",
       "1                                      0.0                        0.000000   \n",
       "2                                      0.0                        0.000000   \n",
       "3                                      0.0                        0.000000   \n",
       "4                                      0.0                        0.013750   \n",
       "...                                    ...                             ...   \n",
       "1879584                                0.0                        0.000000   \n",
       "1879585                                0.0                        0.000000   \n",
       "1879586                                0.0                        0.000000   \n",
       "1879587                                0.0                        0.000000   \n",
       "1879588                                0.0                        0.000000   \n",
       "\n",
       "         AveragePrecipitationTotal.missing  AveragePrecipitation.data  \\\n",
       "0                                      0.0                   0.000000   \n",
       "1                                      0.0                   0.000000   \n",
       "2                                      0.0                   0.000000   \n",
       "3                                      0.0                   0.011354   \n",
       "4                                      0.0                   0.008042   \n",
       "...                                    ...                        ...   \n",
       "1879584                                0.0                   0.000000   \n",
       "1879585                                0.0                   0.000000   \n",
       "1879586                                0.0                   0.000000   \n",
       "1879587                                0.0                   0.000000   \n",
       "1879588                                0.0                   0.000000   \n",
       "\n",
       "         AveragePrecipitation.missing  AverageWindDirection.data  \\\n",
       "0                                 0.0                 192.326389   \n",
       "1                                 0.0                 208.125000   \n",
       "2                                 0.0                 252.916667   \n",
       "3                                 0.0                  91.458333   \n",
       "4                                 0.0                  83.333333   \n",
       "...                               ...                        ...   \n",
       "1879584                           0.0                 283.750000   \n",
       "1879585                           0.0                 209.166667   \n",
       "1879586                           0.0                 270.416667   \n",
       "1879587                           0.0                 208.333333   \n",
       "1879588                           0.0                 295.000000   \n",
       "\n",
       "         AverageWindDirection.missing  AverageWindSpeed.data  \\\n",
       "0                                 0.0               6.732639   \n",
       "1                                 0.0              10.833333   \n",
       "2                                 0.0               8.125000   \n",
       "3                                 0.0               7.197917   \n",
       "4                                 0.0               3.705556   \n",
       "...                               ...                    ...   \n",
       "1879584                           0.0              14.791667   \n",
       "1879585                           0.0              12.291667   \n",
       "1879586                           0.0              17.875000   \n",
       "1879587                           0.0              15.416667   \n",
       "1879588                           0.0              16.333333   \n",
       "\n",
       "         AverageWindSpeed.missing  hospitalIcuBeds  hospitalStaffedBeds  \\\n",
       "0                             0.0              6.0                 25.0   \n",
       "1                             0.0              6.0                 25.0   \n",
       "2                             0.0              6.0                 25.0   \n",
       "3                             0.0              6.0                 25.0   \n",
       "4                             0.0              6.0                 25.0   \n",
       "...                           ...              ...                  ...   \n",
       "1879584                       0.0              1.0                  8.0   \n",
       "1879585                       0.0              1.0                  8.0   \n",
       "1879586                       0.0              1.0                  8.0   \n",
       "1879587                       0.0              1.0                  8.0   \n",
       "1879588                       0.0              1.0                  8.0   \n",
       "\n",
       "         hospitalLicensedBeds  latestTotalPopulation   fips  jhu_daily_death  \\\n",
       "0                        25.0                24527.0  45001              0.0   \n",
       "1                        25.0                24527.0  45001              0.0   \n",
       "2                        25.0                24527.0  45001              0.0   \n",
       "3                        25.0                24527.0  45001              0.0   \n",
       "4                        25.0                24527.0  45001              0.0   \n",
       "...                       ...                    ...    ...              ...   \n",
       "1879584                   8.0                 2756.0  46137              0.0   \n",
       "1879585                   8.0                 2756.0  46137              0.0   \n",
       "1879586                   8.0                 2756.0  46137              0.0   \n",
       "1879587                   8.0                 2756.0  46137              0.0   \n",
       "1879588                   8.0                 2756.0  46137              0.0   \n",
       "\n",
       "         jhu_daily_cases  jhu_daily_new_cases  jhu_daily_death_rolling_7  \\\n",
       "0                    0.0                  0.0                        0.0   \n",
       "1                    0.0                  0.0                        0.0   \n",
       "2                    0.0                  0.0                        0.0   \n",
       "3                    0.0                  0.0                        0.0   \n",
       "4                    0.0                  0.0                        0.0   \n",
       "...                  ...                  ...                        ...   \n",
       "1879584              3.0                  3.0                        0.0   \n",
       "1879585              0.0                  0.0                        0.0   \n",
       "1879586              0.0                  0.0                        0.0   \n",
       "1879587              0.0                  0.0                        0.0   \n",
       "1879588              0.0                  0.0                        0.0   \n",
       "\n",
       "         jhu_daily_cases_rolling_7  jhu_daily_new_cases_rolling_7  \\\n",
       "0                         0.000000                       0.000000   \n",
       "1                         0.000000                       0.000000   \n",
       "2                         0.000000                       0.000000   \n",
       "3                         0.000000                       0.000000   \n",
       "4                         0.000000                       0.000000   \n",
       "...                            ...                            ...   \n",
       "1879584                   1.571429                       1.571429   \n",
       "1879585                   1.285714                       1.285714   \n",
       "1879586                   1.000000                       1.000000   \n",
       "1879587                   1.000000                       1.000000   \n",
       "1879588                   1.000000                       1.000000   \n",
       "\n",
       "         jhu_daily_death_rolling_30  jhu_daily_cases_rolling_30  \\\n",
       "0                          0.000000                    0.000000   \n",
       "1                          0.000000                    0.000000   \n",
       "2                          0.000000                    0.000000   \n",
       "3                          0.000000                    0.000000   \n",
       "4                          0.000000                    0.000000   \n",
       "...                             ...                         ...   \n",
       "1879584                    0.033333                    0.700000   \n",
       "1879585                    0.033333                    0.700000   \n",
       "1879586                    0.033333                    0.700000   \n",
       "1879587                    0.033333                    0.666667   \n",
       "1879588                    0.033333                    0.666667   \n",
       "\n",
       "         jhu_daily_new_cases_rolling_30  jhu_death_rate  jhu_case_rate  \\\n",
       "0                              0.000000             0.0       0.000000   \n",
       "1                              0.000000             0.0       0.000000   \n",
       "2                              0.000000             0.0       0.000000   \n",
       "3                              0.000000             0.0       0.000000   \n",
       "4                              0.000000             0.0       0.000000   \n",
       "...                                 ...             ...            ...   \n",
       "1879584                        0.700000             0.0       0.000570   \n",
       "1879585                        0.700000             0.0       0.000467   \n",
       "1879586                        0.700000             0.0       0.000363   \n",
       "1879587                        0.666667             0.0       0.000363   \n",
       "1879588                        0.666667             0.0       0.000363   \n",
       "\n",
       "         jhu_new_case_rate    density  icu_beds_per_person  \\\n",
       "0                 0.000000  50.006116             0.000245   \n",
       "1                 0.000000  50.006116             0.000245   \n",
       "2                 0.000000  50.006116             0.000245   \n",
       "3                 0.000000  50.006116             0.000245   \n",
       "4                 0.000000  50.006116             0.000245   \n",
       "...                    ...        ...                  ...   \n",
       "1879584           0.000570   1.405212             0.000363   \n",
       "1879585           0.000467   1.405212             0.000363   \n",
       "1879586           0.000363   1.405212             0.000363   \n",
       "1879587           0.000363   1.405212             0.000363   \n",
       "1879588           0.000363   1.405212             0.000363   \n",
       "\n",
       "         staffed_beds_per_person  licensed_beds_per_person  cold_days  \\\n",
       "0                       0.001019                  0.001019          0   \n",
       "1                       0.001019                  0.001019          0   \n",
       "2                       0.001019                  0.001019          0   \n",
       "3                       0.001019                  0.001019          0   \n",
       "4                       0.001019                  0.001019          1   \n",
       "...                          ...                       ...        ...   \n",
       "1879584                 0.002903                  0.002903          1   \n",
       "1879585                 0.002903                  0.002903          1   \n",
       "1879586                 0.002903                  0.002903          1   \n",
       "1879587                 0.002903                  0.002903          1   \n",
       "1879588                 0.002903                  0.002903          1   \n",
       "\n",
       "         hot_days  moderate_days  gte_65_percent  lt_18_percent  \\\n",
       "0               0              1        0.217842       0.200758   \n",
       "1               0              1        0.217842       0.200758   \n",
       "2               0              1        0.217842       0.200758   \n",
       "3               0              1        0.217842       0.200758   \n",
       "4               0              0        0.217842       0.200758   \n",
       "...           ...            ...             ...            ...   \n",
       "1879584         0              0        0.095428       0.280842   \n",
       "1879585         0              0        0.095428       0.280842   \n",
       "1879586         0              0        0.095428       0.280842   \n",
       "1879587         0              0        0.095428       0.280842   \n",
       "1879588         0              0        0.095428       0.280842   \n",
       "\n",
       "         employed_percent  unemployed_percent  totalMoved  movedWithinState  \\\n",
       "0                0.961744            0.038256     24317.0            1118.0   \n",
       "1                0.961744            0.038256     24317.0            1118.0   \n",
       "2                0.961744            0.038256     24317.0            1118.0   \n",
       "3                0.961744            0.038256     24317.0            1118.0   \n",
       "4                0.961744            0.038256     24317.0            1118.0   \n",
       "...                   ...                 ...         ...               ...   \n",
       "1879584          0.962598            0.037402      2753.0              43.0   \n",
       "1879585          0.962598            0.037402      2753.0              43.0   \n",
       "1879586          0.962598            0.037402      2753.0              43.0   \n",
       "1879587          0.962598            0.037402      2753.0              43.0   \n",
       "1879588          0.962598            0.037402      2753.0              43.0   \n",
       "\n",
       "         movedWithoutState  movedFromAbroad  publicTrans  totalTrans  \\\n",
       "0                    329.0             12.0           44        9791   \n",
       "1                    329.0             12.0           44        9791   \n",
       "2                    329.0             12.0           44        9791   \n",
       "3                    329.0             12.0           44        9791   \n",
       "4                    329.0             12.0           44        9791   \n",
       "...                    ...              ...          ...         ...   \n",
       "1879584                8.0              0.0            1         951   \n",
       "1879585                8.0              0.0            1         951   \n",
       "1879586                8.0              0.0            1         951   \n",
       "1879587                8.0              0.0            1         951   \n",
       "1879588                8.0              0.0            1         951   \n",
       "\n",
       "         householdsTotal  houseWith65  house2+with65  houseFamily65  \\\n",
       "0                   9660         3645           2081           2042   \n",
       "1                   9660         3645           2081           2042   \n",
       "2                   9660         3645           2081           2042   \n",
       "3                   9660         3645           2081           2042   \n",
       "4                   9660         3645           2081           2042   \n",
       "...                  ...          ...            ...            ...   \n",
       "1879584              754          185            137            134   \n",
       "1879585              754          185            137            134   \n",
       "1879586              754          185            137            134   \n",
       "1879587              754          185            137            134   \n",
       "1879588              754          185            137            134   \n",
       "\n",
       "         houseNonfam65  houseNo65  house2+No65  houseFamilyNo65  \\\n",
       "0                   39       6015         4531             4206   \n",
       "1                   39       6015         4531             4206   \n",
       "2                   39       6015         4531             4206   \n",
       "3                   39       6015         4531             4206   \n",
       "4                   39       6015         4531             4206   \n",
       "...                ...        ...          ...              ...   \n",
       "1879584              3        569          482              474   \n",
       "1879585              3        569          482              474   \n",
       "1879586              3        569          482              474   \n",
       "1879587              3        569          482              474   \n",
       "1879588              3        569          482              474   \n",
       "\n",
       "         houseNonfamNo65  householdStructuresTotal  householdIncomeMedian  \\\n",
       "0                    325                      9660                  38741   \n",
       "1                    325                      9660                  38741   \n",
       "2                    325                      9660                  38741   \n",
       "3                    325                      9660                  38741   \n",
       "4                    325                      9660                  38741   \n",
       "...                  ...                       ...                    ...   \n",
       "1879584                8                       754                  37400   \n",
       "1879585                8                       754                  37400   \n",
       "1879586                8                       754                  37400   \n",
       "1879587                8                       754                  37400   \n",
       "1879588                8                       754                  37400   \n",
       "\n",
       "           gini  hoursWorkedMean  unitsInStructure  healthInsTotal  \\\n",
       "0        0.4790             38.1             12191           24397   \n",
       "1        0.4790             38.1             12191           24397   \n",
       "2        0.4790             38.1             12191           24397   \n",
       "3        0.4790             38.1             12191           24397   \n",
       "4        0.4790             38.1             12191           24397   \n",
       "...         ...              ...               ...             ...   \n",
       "1879584  0.4834             40.6              1009            2791   \n",
       "1879585  0.4834             40.6              1009            2791   \n",
       "1879586  0.4834             40.6              1009            2791   \n",
       "1879587  0.4834             40.6              1009            2791   \n",
       "1879588  0.4834             40.6              1009            2791   \n",
       "\n",
       "         healthInsNativeWith  healthInsForeignNatWith  \\\n",
       "0                      21415                      179   \n",
       "1                      21415                      179   \n",
       "2                      21415                      179   \n",
       "3                      21415                      179   \n",
       "4                      21415                      179   \n",
       "...                      ...                      ...   \n",
       "1879584                 2020                        4   \n",
       "1879585                 2020                        4   \n",
       "1879586                 2020                        4   \n",
       "1879587                 2020                        4   \n",
       "1879588                 2020                        4   \n",
       "\n",
       "         healthInsForeignNoncitWith  healthInsForeignNatNo  \\\n",
       "0                                87                     39   \n",
       "1                                87                     39   \n",
       "2                                87                     39   \n",
       "3                                87                     39   \n",
       "4                                87                     39   \n",
       "...                             ...                    ...   \n",
       "1879584                           0                      0   \n",
       "1879585                           0                      0   \n",
       "1879586                           0                      0   \n",
       "1879587                           0                      0   \n",
       "1879588                           0                      0   \n",
       "\n",
       "         healthInsForeignNoncitNo  healthInsNativeNo      pm25   latitude  \\\n",
       "0                             122               2555  9.618551  34.229041   \n",
       "1                             122               2555  9.618551  34.229041   \n",
       "2                             122               2555  9.618551  34.229041   \n",
       "3                             122               2555  9.618551  34.229041   \n",
       "4                             122               2555  9.618551  34.229041   \n",
       "...                           ...                ...       ...        ...   \n",
       "1879584                        16                751  4.322196  44.989764   \n",
       "1879585                        16                751  4.322196  44.989764   \n",
       "1879586                        16                751  4.322196  44.989764   \n",
       "1879587                        16                751  4.322196  44.989764   \n",
       "1879588                        16                751  4.322196  44.989764   \n",
       "\n",
       "          longitude  cyclical_sin  cyclical_cos  continuous_sin  \\\n",
       "0        -82.454058      0.137279      0.990532        0.013771   \n",
       "1        -82.454058      0.154309      0.988023        0.015492   \n",
       "2        -82.454058      0.171293      0.985220        0.017213   \n",
       "3        -82.454058      0.188227      0.982126        0.018934   \n",
       "4        -82.454058      0.205104      0.978740        0.020656   \n",
       "...             ...           ...           ...             ...   \n",
       "1879584 -101.660827     -0.840618      0.541628        0.915518   \n",
       "1879585 -101.660827     -0.831171      0.556017        0.916210   \n",
       "1879586 -101.660827     -0.821477      0.570242        0.916898   \n",
       "1879587 -101.660827     -0.811539      0.584298        0.917584   \n",
       "1879588 -101.660827     -0.801361      0.598181        0.918267   \n",
       "\n",
       "         continuous_cos      geo0      geo1      geo2      geo3      geo4  \\\n",
       "0              0.999905 -0.206256  0.121853 -0.059924 -0.341429 -0.240499   \n",
       "1              0.999880 -0.206256  0.121853 -0.059924 -0.341429 -0.240499   \n",
       "2              0.999852 -0.206256  0.121853 -0.059924 -0.341429 -0.240499   \n",
       "3              0.999821 -0.206256  0.121853 -0.059924 -0.341429 -0.240499   \n",
       "4              0.999787 -0.206256  0.121853 -0.059924 -0.341429 -0.240499   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "1879584        0.402276  0.071524  0.178312  0.439406  0.202796 -0.297441   \n",
       "1879585        0.400700  0.071524  0.178312  0.439406  0.202796 -0.297441   \n",
       "1879586        0.399122  0.071524  0.178312  0.439406  0.202796 -0.297441   \n",
       "1879587        0.397543  0.071524  0.178312  0.439406  0.202796 -0.297441   \n",
       "1879588        0.395963  0.071524  0.178312  0.439406  0.202796 -0.297441   \n",
       "\n",
       "             geo5      geo6      geo7      geo8  \n",
       "0        0.052870 -0.162839 -0.213781  0.165044  \n",
       "1        0.052870 -0.162839 -0.213781  0.165044  \n",
       "2        0.052870 -0.162839 -0.213781  0.165044  \n",
       "3        0.052870 -0.162839 -0.213781  0.165044  \n",
       "4        0.052870 -0.162839 -0.213781  0.165044  \n",
       "...           ...       ...       ...       ...  \n",
       "1879584  0.400439  0.367141 -0.038317  0.179132  \n",
       "1879585  0.400439  0.367141 -0.038317  0.179132  \n",
       "1879586  0.400439  0.367141 -0.038317  0.179132  \n",
       "1879587  0.400439  0.367141 -0.038317  0.179132  \n",
       "1879588  0.400439  0.367141 -0.038317  0.179132  \n",
       "\n",
       "[1879589 rows x 102 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ada429d-61e0-4c8f-9ec8-5e34d4547a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('/tmp/df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f842b-c049-404a-bb19-55f26e8efa25",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513da66f-15f7-4907-bf2d-f136a335f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_normalize = [\n",
    "       'TotalPopulation.data', 'MaleAndFemale_AtLeast65_Population.data',\n",
    "       'Male_Total_Population.data', 'Female_Total_Population.data',\n",
    "       'MaleAndFemale_Under18_Population.data', 'BLS_EmployedPopulation.data',\n",
    "       'BLS_EmployedPopulation.missing', 'BLS_UnemployedPopulation.data',\n",
    "       'BLS_UnemployedPopulation.missing', 'BLS_UnemploymentRate.data',\n",
    "       'BLS_UnemploymentRate.missing', 'BLS_LaborForcePopulation.data',\n",
    "       'BLS_LaborForcePopulation.missing', 'AverageDailyTemperature.data',\n",
    "       'AverageDailyTemperature.missing', 'AverageDewPoint.data',\n",
    "       'AverageDewPoint.missing', 'AverageRelativeHumidity.data',\n",
    "       'AverageRelativeHumidity.missing', 'AverageSurfaceAirPressure.data',\n",
    "       'AverageSurfaceAirPressure.missing', 'AveragePrecipitationTotal.data',\n",
    "       'AveragePrecipitationTotal.missing', 'AveragePrecipitation.data',\n",
    "       'AveragePrecipitation.missing', 'AverageWindDirection.data',\n",
    "       'AverageWindDirection.missing', 'AverageWindSpeed.data',\n",
    "       'AverageWindSpeed.missing', 'hospitalIcuBeds', 'hospitalStaffedBeds',\n",
    "       'hospitalLicensedBeds', 'latestTotalPopulation', 'jhu_daily_death',\n",
    "       'jhu_daily_cases', 'jhu_daily_new_cases', \n",
    "    'jhu_daily_death_rolling_7',\n",
    "       'jhu_daily_cases_rolling_7', 'jhu_daily_new_cases_rolling_7',\n",
    "       'jhu_daily_death_rolling_30', 'jhu_daily_cases_rolling_30',\n",
    "       'jhu_daily_new_cases_rolling_30', 'jhu_death_rate', 'jhu_case_rate',\n",
    "       'jhu_new_case_rate', 'density', 'icu_beds_per_person',\n",
    "       'staffed_beds_per_person', 'licensed_beds_per_person', 'cold_days',\n",
    "       'hot_days', 'moderate_days', 'gte_65_percent', 'lt_18_percent',\n",
    "       'employed_percent', 'unemployed_percent', 'totalMoved',\n",
    "       'movedWithinState', 'movedWithoutState', 'movedFromAbroad',\n",
    "       'publicTrans', 'totalTrans', 'householdsTotal', 'houseWith65',\n",
    "       'house2+with65', 'houseFamily65', 'houseNonfam65', 'houseNo65',\n",
    "       'house2+No65', 'houseFamilyNo65', 'houseNonfamNo65',\n",
    "       'householdStructuresTotal', 'householdIncomeMedian', 'gini',\n",
    "       'hoursWorkedMean', 'unitsInStructure', 'healthInsTotal',\n",
    "       'healthInsNativeWith', 'healthInsForeignNatWith',\n",
    "       'healthInsForeignNoncitWith', 'healthInsForeignNatNo',\n",
    "       'healthInsForeignNoncitNo', 'healthInsNativeNo', 'pm25', 'latitude',\n",
    "       'longitude']\n",
    "cols_raw = ['fips','JHU_ConfirmedCases.data', 'JHU_ConfirmedDeaths.data', 'cyclical_sin', 'cyclical_cos', 'continuous_sin',\n",
    "       'continuous_cos', 'geo0', 'geo1', 'geo2','geo3', 'geo4','geo5','geo6','geo7','geo8']\n",
    "df_normalized = df[cols_to_normalize]\n",
    "df_normalized = (df_normalized - df_normalized.mean())/df_normalized.std()\n",
    "df_raw = df[cols_raw]\n",
    "df = pd.concat([df_raw, df_normalized], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bf3ef-a6d5-4f22-b336-f46f2798a363",
   "metadata": {},
   "source": [
    "#### Prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7731216-cef5-4d0a-a9d2-51dfbe00172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_of_history = 30\n",
    "days_to_predict = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3c1f6-ec25-4420-8f70-b8d7f0cef1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = df.fips.unique()\n",
    "\n",
    "# def x_generator(data, days_of_history=30, days_to_predict=1):\n",
    "#     for j, fip in enumerate(fips):\n",
    "#         if not j % 100: print(j, end=' ')\n",
    "#         county = data[data.fips == fip]\n",
    "#         for i in range(days_of_history, len(county) - days_to_predict):\n",
    "#             data_matrix = data.iloc[i - days_of_history: i, 1:].to_numpy()\n",
    "#             yield data_matrix\n",
    "            \n",
    "# def y_generator(data, days_of_history=30, days_to_predict=1):\n",
    "#     for fip in fips:\n",
    "#         county = data[data.fips == fip]\n",
    "#         for i in range(days_of_history, len(county) - days_to_predict):\n",
    "#             data_matrix = data.iloc[i: i + days_to_predict, 1:3].to_numpy()\n",
    "#             yield data_matrix\n",
    "    \n",
    "def xy_generator(data, days=31):\n",
    "    for j, fip in enumerate(fips):\n",
    "        if not j % 100: print(j, end=' ')\n",
    "        county = data[data.fips == fip]\n",
    "        for i in range(days, len(county) + 1):\n",
    "            data_matrix = data.iloc[i - days: i, 1:].to_numpy()\n",
    "            yield data_matrix\n",
    "            \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac163c8-45f9-4a5c-972d-babb0d65816a",
   "metadata": {},
   "source": [
    "##### Save the raw X and Y to files of 50,000 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32add75b-d9ef-432b-a607-6fe85948c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xi = []\n",
    "j = 0\n",
    "\n",
    "N_SAMPLES = 200\n",
    "\n",
    "for i, x in enumerate(xy_generator(df)):\n",
    "    Xi.append(x)\n",
    "    if i and not i % (N_SAMPLES - 1):\n",
    "        X = np.asarray(Xi)\n",
    "        np.save(os.path.join('.','data', f'x_{j}.npy'), X)\n",
    "        j += 1\n",
    "        Xi = []\n",
    "if Xi:\n",
    "    X = np.asarray(Xi)\n",
    "    np.save(os.path.join('.','data', f'x_{j}.npy'), X)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac1bc8-469d-4412-a243-74e5113661c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('./data/x_0.npy')\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664610a-0086-4410-b237-4ae731bce0d7",
   "metadata": {},
   "source": [
    "##### Split into train, test, eval directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabce3f5-5188-408b-8bb1-711fdb8af856",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "def set_seed():\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.random.set_seed(RANDOM_SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee865b7d-1f2b-4c1e-b24b-ee56a59ca907",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_files = glob('./data/x_*.npy')\n",
    "random.shuffle(x_files)\n",
    "n_files = len(x_files)\n",
    "print(n_files)\n",
    "n_train = int(n_files * 0.70)\n",
    "print(n_train)\n",
    "n_eval = int(n_files * 0.15)\n",
    "print(n_eval)\n",
    "n_test = n_files - n_train - n_eval\n",
    "print(n_test)\n",
    "train_files = x_files[:n_train]\n",
    "# print(len(train_files))\n",
    "eval_files = x_files[n_train:n_train+n_test]\n",
    "# print(len(eval_files))\n",
    "test_files = x_files[n_train+n_test:]\n",
    "assert n_files == len(train_files) + len(eval_files) + len(test_files)\n",
    "for (subdir, lst) in [['train', train_files], ['eval', eval_files], ['test', test_files]]:\n",
    "    for file in lst:\n",
    "        shutil.move(file, os.path.join('.', 'data', subdir))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72038235-dbcf-47e3-8a70-7b8021b68f7f",
   "metadata": {},
   "source": [
    "##### Create the Tensorflow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c917f4-f604-48cf-a8dd-d07a1fffed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob('./data/train/x_*.npy')\n",
    "eval_files = glob('./data/eval/x_*.npy')\n",
    "test_files = glob('./data/test/x_*.npy')\n",
    "\n",
    "n_readers = 5\n",
    "n_parse_threads = 5\n",
    "len_array = 995\n",
    "\n",
    "def create_generator(files, cycle_length=5):\n",
    "    set_seed()\n",
    "    random.shuffle(files)\n",
    "    for i in range(0, len(files), cycle_length):\n",
    "        subset = files[i:i+cycle_length]\n",
    "        np_arrays = [np.load(s) for s in subset]\n",
    "        np_array = np.concatenate(np_arrays, axis=0)\n",
    "        np.random.shuffle(np_array)\n",
    "        yield np_array\n",
    "            \n",
    "\n",
    "def split_xy(np_array):\n",
    "    X = np_array[:,:-1,:]\n",
    "    y = np_array[:,-1:,:1]\n",
    "    return X,y\n",
    "        \n",
    "    \n",
    "train_ds = tf.data.Dataset.from_generator(lambda: create_generator(train_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "train_ds = train_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(lambda: create_generator(eval_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "val_ds = val_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(lambda: create_generator(test_files, cycle_length=n_readers), output_types=tf.float32 )\n",
    "test_ds = test_ds.map(split_xy, num_parallel_calls=n_parse_threads).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171e1b39-b6af-4662-9975-060c06c987ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in train_ds.take(3):\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaea1d7-a885-4ca5-9f52-83846c1bd2df",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351ce58-2517-4ec3-9a7f-36e819e7f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'recurrent_layer_type': [keras.layers.LSTM, keras.layers.GRU],\n",
    "    'n_recurrent': [1, 2, 3],\n",
    "    'unit': [32, 64, 128],\n",
    "    'dropout': [0.0, 0.10, 0.20],\n",
    "    'lr': [1e-2, 1e-3],   \n",
    "    'epochs': 256,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc01b2-659b-4d10-b0ba-a2ae967e58db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(param, shape=(30,101)):\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(param['n_recurrent']):\n",
    "        last = param['recurrent_layer_type'](param['unit'])(last)\n",
    "        if param['dropout']:\n",
    "            last = keras.layers.Dropout(param['dropout'])(last)\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=param['lr']),  loss='mae',  metrics=['mse', 'mae'])\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2195ed-28cc-4565-9d05-31d855c1ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=10, restore_best_weights=True)\n",
    "\n",
    "n_trial = 3\n",
    "\n",
    "name = f'trial_{n_trial}'\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(f'./data/model/{name}_{timestamp}.h5', save_best_only=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(name))\n",
    "\n",
    "\n",
    "hypermodel = lambda param: build_model(param=param)\n",
    "\n",
    "kgs = KerasGridSearch(hypermodel, param_grid, monitor='val_loss', greater_is_better=False, tuner_verbose=1)\n",
    "kgs.search(train_ds, validation_data=val_ds, callbacks=[early_stopping_cb, checkpoint_cb, tensorboard], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea660af-bf47-4c55-a9ae-989a7d972e45",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning \n",
    "\n",
    "Using technique from https://medium.com/ml-book/neural-networks-hyperparameter-tuning-in-tensorflow-2-0-a7b4e2b574a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1978f67-3294-44bf-b79e-007a6645b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([1, 2, 3]))\n",
    "# HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([3]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([32, 64, 128, 256]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.00, 0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-2, 1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning_full_3').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb81ad5-9d75-4725-ac82-5d514fcc5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=32\n",
    "\n",
    "def train_test_model(hparams, shape=(30,100)):\n",
    "    # set_seed()\n",
    "    train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "    \n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            batch_size=128,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcc69d-4d33-472c-a9b4-11eff17d74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.001}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                12864     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,897\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "      1/Unknown - 2s 2s/step - loss: 1525.8112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 15:23:06.882844: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 28s 22ms/step - loss: 7621.9395 - val_loss: 7626.8423\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7585.1494 - val_loss: 7592.2026\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7551.5376 - val_loss: 7559.0742\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7519.0703 - val_loss: 7526.8145\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7487.4551 - val_loss: 7495.3330\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7456.5352 - val_loss: 7464.4849\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7426.3125 - val_loss: 7434.2222\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7396.7466 - val_loss: 7404.5869\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7367.6997 - val_loss: 7375.6255\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7339.2134 - val_loss: 7346.9097\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7311.3188 - val_loss: 7319.2705\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7283.8530 - val_loss: 7291.3213\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7256.9058 - val_loss: 7264.3647\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7230.4663 - val_loss: 7237.9126\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7204.3340 - val_loss: 7212.0259\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7179.0312 - val_loss: 7186.3726\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7153.8848 - val_loss: 7162.6167\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7129.2036 - val_loss: 7137.3647\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7105.0830 - val_loss: 7112.8164\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7081.1128 - val_loss: 7089.7349\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7057.6240 - val_loss: 7066.4810\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7035.1919 - val_loss: 7041.9497\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7012.4878 - val_loss: 7020.4463\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6990.4321 - val_loss: 6998.2959\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6968.2339 - val_loss: 6974.3096\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6946.4194 - val_loss: 6953.2832\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6925.2144 - val_loss: 6934.8867\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6904.4727 - val_loss: 6909.8418\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6883.9282 - val_loss: 6888.2939\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6863.4463 - val_loss: 6871.3154\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6843.5254 - val_loss: 6847.8101\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6823.7891 - val_loss: 6832.4170\n",
      "258/258 [==============================] - 6s 21ms/step - loss: 6338.7925\n",
      "--- Starting trial: run-1\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.0, 'lr': 0.01}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 32)                12864     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,897\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7480.3579 - val_loss: 7360.9683\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7226.3721 - val_loss: 7135.7563\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7065.6890 - val_loss: 6986.9019\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6919.2793 - val_loss: 6982.8838\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6832.9922 - val_loss: 6733.3345\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6816.8638 - val_loss: 6859.6138\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6810.3574 - val_loss: 6758.6943\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6681.6294 - val_loss: 6636.6973\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6654.1143 - val_loss: 6635.6382\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6592.7578 - val_loss: 6559.8184\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6529.3633 - val_loss: 6428.9038\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6380.8374 - val_loss: 6321.5527\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6308.1938 - val_loss: 6331.4961\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6323.3730 - val_loss: 6329.6489\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6365.0747 - val_loss: 6307.9028\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6302.1621 - val_loss: 6257.6602\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6189.8257 - val_loss: 6122.9961\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6121.0879 - val_loss: 6128.5229\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6214.5576 - val_loss: 6218.8667\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6191.1724 - val_loss: 6172.9814\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6126.7153 - val_loss: 5975.5469\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6087.0854 - val_loss: 6121.5522\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6101.1846 - val_loss: 6061.0220\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6069.9258 - val_loss: 6000.6782\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5997.0518 - val_loss: 5981.1738\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5941.5049 - val_loss: 5949.6353\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5901.9150 - val_loss: 5855.9102\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5901.3657 - val_loss: 6069.5635\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6038.7793 - val_loss: 6043.3252\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5982.9468 - val_loss: 5830.4487\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5982.5459 - val_loss: 5952.0542\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5943.2510 - val_loss: 5951.5859\n",
      "258/258 [==============================] - 5s 17ms/step - loss: 5466.7852\n",
      "--- Starting trial: run-2\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.001}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 32)                12864     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,897\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7621.9331 - val_loss: 7626.7222\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7585.1636 - val_loss: 7592.0493\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7551.6221 - val_loss: 7558.8022\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7519.2417 - val_loss: 7526.5381\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7487.7021 - val_loss: 7494.9150\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7456.8662 - val_loss: 7463.9937\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7426.7021 - val_loss: 7433.9097\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7397.1362 - val_loss: 7403.7720\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7368.1909 - val_loss: 7374.4116\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7339.7188 - val_loss: 7345.5127\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7311.7632 - val_loss: 7317.5371\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7284.2466 - val_loss: 7289.9136\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7257.2388 - val_loss: 7261.9961\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7230.6270 - val_loss: 7235.0444\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7204.5679 - val_loss: 7208.6880\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7178.9819 - val_loss: 7182.4077\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7153.8672 - val_loss: 7157.0210\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7128.9946 - val_loss: 7132.6094\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7104.6982 - val_loss: 7107.1641\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7081.2598 - val_loss: 7082.7827\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7057.1064 - val_loss: 7058.5273\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7033.8130 - val_loss: 7034.7007\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7010.9956 - val_loss: 7012.3647\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7016.4956 - val_loss: 7023.4644\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7000.3359 - val_loss: 7001.6343\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6979.0078 - val_loss: 6979.3198\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6964.4741 - val_loss: 6957.2925\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6938.4414 - val_loss: 6938.6328\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6917.8921 - val_loss: 6915.5757\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6898.4478 - val_loss: 6895.1729\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6869.3501 - val_loss: 6858.3330\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6831.2251 - val_loss: 6826.5615\n",
      "258/258 [==============================] - 4s 17ms/step - loss: 6333.2007\n",
      "--- Starting trial: run-3\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 32, 'dropout': 0.2, 'lr': 0.01}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 32)                12864     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,897\n",
      "Trainable params: 12,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7484.7256 - val_loss: 7367.4858\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7273.4263 - val_loss: 7224.4043\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7129.2559 - val_loss: 7045.9585\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6997.1357 - val_loss: 6932.7319\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6892.5264 - val_loss: 6818.2480\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6820.8135 - val_loss: 6825.0464\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6713.0020 - val_loss: 6602.0078\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6582.8306 - val_loss: 6562.0005\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6549.0356 - val_loss: 6591.5898\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6559.9663 - val_loss: 6496.7739\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6558.9253 - val_loss: 6480.7388\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6454.9893 - val_loss: 6413.3057\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6373.9463 - val_loss: 6301.4556\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6343.2168 - val_loss: 6290.8682\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6353.2021 - val_loss: 6218.3379\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6259.6938 - val_loss: 6158.6445\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6185.1099 - val_loss: 6100.7705\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6118.8120 - val_loss: 6112.5942\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6096.7461 - val_loss: 6008.4414\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6027.7451 - val_loss: 6015.4316\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6033.7500 - val_loss: 5950.3481\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5985.9326 - val_loss: 5876.8389\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6003.4517 - val_loss: 5938.4136\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5968.6748 - val_loss: 5956.6270\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5999.8960 - val_loss: 5881.9155\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5908.1729 - val_loss: 5834.8413\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5907.0005 - val_loss: 5925.1138\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5961.9048 - val_loss: 5844.0186\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5908.4609 - val_loss: 5873.0337\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5865.9746 - val_loss: 5808.9131\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5872.3647 - val_loss: 5766.7397\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 25s 21ms/step - loss: 5806.8037 - val_loss: 5737.9800\n",
      "258/258 [==============================] - 5s 17ms/step - loss: 5259.2202\n",
      "--- Starting trial: run-4\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.001}\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 64)                31872     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,937\n",
      "Trainable params: 31,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 28s 22ms/step - loss: 7601.2183 - val_loss: 7589.4482\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7533.2031 - val_loss: 7525.4619\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7471.4536 - val_loss: 7464.7681\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7412.5605 - val_loss: 7406.3281\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7355.9033 - val_loss: 7350.0293\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7301.2451 - val_loss: 7295.8711\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7248.5728 - val_loss: 7243.1479\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7197.6543 - val_loss: 7192.9399\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7148.3970 - val_loss: 7143.7461\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7100.4199 - val_loss: 7096.5957\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7054.1699 - val_loss: 7051.3408\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7009.6665 - val_loss: 7006.2993\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6966.2949 - val_loss: 6962.5410\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6924.5820 - val_loss: 6919.6855\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6883.5391 - val_loss: 6880.2529\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6844.3882 - val_loss: 6839.9790\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6806.1748 - val_loss: 6801.3291\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6769.2559 - val_loss: 6763.3462\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6732.9395 - val_loss: 6727.6460\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6698.0874 - val_loss: 6692.4556\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6663.4746 - val_loss: 6658.2148\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6630.2808 - val_loss: 6623.6763\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6597.7002 - val_loss: 6596.5654\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6566.3154 - val_loss: 6564.8838\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6535.8564 - val_loss: 6530.7510\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6506.5518 - val_loss: 6498.6167\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6477.4912 - val_loss: 6469.9170\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6449.7500 - val_loss: 6449.2529\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6421.2520 - val_loss: 6415.6318\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6417.0127 - val_loss: 6417.8525\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6382.5698 - val_loss: 6373.4316\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6346.0786 - val_loss: 6333.8809\n",
      "258/258 [==============================] - 5s 17ms/step - loss: 5850.6230\n",
      "--- Starting trial: run-5\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.0, 'lr': 0.01}\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 64)                31872     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,937\n",
      "Trainable params: 31,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7354.8052 - val_loss: 7147.4443\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6956.4478 - val_loss: 6849.0942\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6785.6987 - val_loss: 6742.2759\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6647.2280 - val_loss: 6547.7988\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6517.4541 - val_loss: 6430.0518\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6479.5332 - val_loss: 6454.4644\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6436.4946 - val_loss: 6434.1250\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6365.3862 - val_loss: 6377.1162\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6381.6982 - val_loss: 6431.4438\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6389.2373 - val_loss: 6274.2432\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6208.9312 - val_loss: 6181.7461\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6131.6016 - val_loss: 6127.4844\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6097.6680 - val_loss: 6103.8145\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6065.7661 - val_loss: 6116.1260\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6109.3726 - val_loss: 6083.8750\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6089.2871 - val_loss: 6078.6470\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6027.3818 - val_loss: 5961.6426\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6012.6724 - val_loss: 6101.9136\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6093.1841 - val_loss: 6210.5889\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6040.4302 - val_loss: 5973.0254\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5958.1553 - val_loss: 5900.1118\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6152.5830 - val_loss: 6156.4365\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6081.1040 - val_loss: 5981.4985\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6060.3550 - val_loss: 5977.9126\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6000.0498 - val_loss: 5945.9482\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5865.3027 - val_loss: 5805.4409\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5941.5005 - val_loss: 5987.4199\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5959.9971 - val_loss: 5950.5825\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5842.0049 - val_loss: 5855.1494\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5743.4668 - val_loss: 5723.3892\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5707.1294 - val_loss: 5645.9111\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5584.8149 - val_loss: 5680.5435\n",
      "258/258 [==============================] - 5s 17ms/step - loss: 5203.9443\n",
      "--- Starting trial: run-6\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.001}\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 64)                31872     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,937\n",
      "Trainable params: 31,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 28s 22ms/step - loss: 7601.3018 - val_loss: 7589.4521\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7533.4175 - val_loss: 7525.3643\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7471.7358 - val_loss: 7464.5293\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7412.9180 - val_loss: 7405.8379\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7356.3037 - val_loss: 7349.1055\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7301.5850 - val_loss: 7294.5059\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7248.7788 - val_loss: 7241.4111\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7197.6782 - val_loss: 7190.2402\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7148.2749 - val_loss: 7140.4556\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7100.1978 - val_loss: 7092.0093\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7053.6567 - val_loss: 7045.7437\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 7008.3931 - val_loss: 6999.4189\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6964.7217 - val_loss: 6954.8359\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6922.0781 - val_loss: 6914.0791\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6880.4624 - val_loss: 6868.0723\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6841.0938 - val_loss: 6827.8457\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6801.6499 - val_loss: 6787.0376\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6763.1289 - val_loss: 6752.5498\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6725.8911 - val_loss: 6709.5464\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6689.5923 - val_loss: 6673.8608\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6653.8931 - val_loss: 6639.0044\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6622.3003 - val_loss: 6613.0688\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6598.0547 - val_loss: 6581.9775\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6565.3936 - val_loss: 6543.6660\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6533.1211 - val_loss: 6511.4976\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6508.4263 - val_loss: 6494.6641\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6481.2051 - val_loss: 6450.9600\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6434.9312 - val_loss: 6405.2158\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6416.9009 - val_loss: 6446.9336\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6420.2524 - val_loss: 6403.9951\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6411.8467 - val_loss: 6396.7642\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6389.1479 - val_loss: 6364.7715\n",
      "258/258 [==============================] - 5s 17ms/step - loss: 5880.9243\n",
      "--- Starting trial: run-7\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 64, 'dropout': 0.2, 'lr': 0.01}\n",
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 64)                31872     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,937\n",
      "Trainable params: 31,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 28s 22ms/step - loss: 7358.2158 - val_loss: 7153.7852\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6994.5210 - val_loss: 6879.9570\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6787.3125 - val_loss: 6691.1187\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6721.8115 - val_loss: 6601.0518\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6551.7026 - val_loss: 6455.0620\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6411.1118 - val_loss: 6307.2510\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6377.9863 - val_loss: 6323.0244\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6327.4272 - val_loss: 6305.2368\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6315.4565 - val_loss: 6312.1094\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6249.4229 - val_loss: 6139.2397\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6188.7168 - val_loss: 6106.8135\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6192.4648 - val_loss: 6193.2090\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6254.3149 - val_loss: 6193.6768\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6170.2173 - val_loss: 6131.8140\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6185.4824 - val_loss: 6126.3784\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6150.1279 - val_loss: 6055.2051\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6087.0513 - val_loss: 6079.5400\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6123.0479 - val_loss: 5946.1602\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 27s 23ms/step - loss: 5996.4565 - val_loss: 5913.2666\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5975.7212 - val_loss: 5971.7876\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6070.2158 - val_loss: 6053.8960\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5966.5645 - val_loss: 5825.2363\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5864.5449 - val_loss: 5696.9976\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5891.9067 - val_loss: 5935.9731\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5906.2036 - val_loss: 5808.9971\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5897.2451 - val_loss: 5915.0654\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5873.7979 - val_loss: 5745.1846\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5845.4395 - val_loss: 5733.1606\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5751.5488 - val_loss: 5651.4839\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5726.6045 - val_loss: 5736.9600\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5790.9648 - val_loss: 5686.2056\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5761.1938 - val_loss: 5617.9854\n",
      "258/258 [==============================] - 5s 17ms/step - loss: 5143.2915\n",
      "--- Starting trial: run-8\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.001}\n",
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 128)               88320     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,449\n",
      "Trainable params: 88,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 28s 22ms/step - loss: 7566.6035 - val_loss: 7524.3447\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7441.8560 - val_loss: 7407.4678\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7330.8506 - val_loss: 7300.0195\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7227.7002 - val_loss: 7198.4111\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7130.8149 - val_loss: 7103.4399\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7039.7939 - val_loss: 7014.1494\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6954.4590 - val_loss: 6930.4243\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6874.0840 - val_loss: 6850.3271\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6800.0005 - val_loss: 6780.1055\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6728.7881 - val_loss: 6702.6333\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6657.9062 - val_loss: 6635.6836\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6592.3687 - val_loss: 6571.6084\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6531.2607 - val_loss: 6509.1841\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6473.7505 - val_loss: 6457.0488\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6417.3110 - val_loss: 6397.3496\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6363.4644 - val_loss: 6342.2500\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6312.7061 - val_loss: 6292.3477\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6277.6855 - val_loss: 6260.9087\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6232.2959 - val_loss: 6217.6855\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6205.3115 - val_loss: 6207.3735\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6169.3149 - val_loss: 6137.2593\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6123.4795 - val_loss: 6109.4380\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 28s 23ms/step - loss: 6089.6895 - val_loss: 6082.6885\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6061.0981 - val_loss: 6055.9565\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6039.5405 - val_loss: 6038.0991\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6022.9375 - val_loss: 5996.8325\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5998.7559 - val_loss: 5990.9473\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5979.5522 - val_loss: 5961.4116\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5963.5723 - val_loss: 5966.6401\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5951.8647 - val_loss: 5934.1934\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5942.6025 - val_loss: 5925.3423\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5916.7183 - val_loss: 5896.3154\n",
      "258/258 [==============================] - 5s 17ms/step - loss: 5417.3359\n",
      "--- Starting trial: run-9\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.0, 'lr': 0.01}\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_9 (GRU)                 (None, 128)               88320     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,449\n",
      "Trainable params: 88,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7170.1050 - val_loss: 6853.5171\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6670.7402 - val_loss: 6596.7344\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6504.7734 - val_loss: 6373.1426\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6405.8462 - val_loss: 6443.6294\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6400.5400 - val_loss: 6327.3745\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6311.2886 - val_loss: 6335.7349\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6270.7505 - val_loss: 6300.3330\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 6261.7388 - val_loss: 6253.4604\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6186.4888 - val_loss: 6126.5596\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6112.3989 - val_loss: 6027.4399\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6037.2446 - val_loss: 6088.1821\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6143.5557 - val_loss: 6072.2402\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6002.2720 - val_loss: 5954.3442\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5912.8481 - val_loss: 5876.1611\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5840.6255 - val_loss: 5862.0078\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5892.7871 - val_loss: 5850.5464\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5824.5532 - val_loss: 5743.6929\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5770.4624 - val_loss: 5737.2568\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5935.9287 - val_loss: 5959.5293\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 26s 21ms/step - loss: 5897.5342 - val_loss: 5808.9634\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5771.3779 - val_loss: 5723.9951\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5837.2256 - val_loss: 5859.5483\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5930.8164 - val_loss: 6079.9097\n",
      "Epoch 24/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5982.0732 - val_loss: 5945.8574\n",
      "Epoch 25/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5837.2690 - val_loss: 5746.1382\n",
      "Epoch 26/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5883.5234 - val_loss: 5971.1113\n",
      "Epoch 27/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5891.7876 - val_loss: 5711.3257\n",
      "Epoch 28/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 5749.4648 - val_loss: 5737.9565\n",
      "Epoch 29/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5747.4077 - val_loss: 5648.9854\n",
      "Epoch 30/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5689.9072 - val_loss: 5661.8804\n",
      "Epoch 31/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5603.0278 - val_loss: 5571.8486\n",
      "Epoch 32/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 5556.9653 - val_loss: 5518.7266\n",
      "258/258 [==============================] - 4s 17ms/step - loss: 5044.6611\n",
      "--- Starting trial: run-10\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 1, 'n_unit': 128, 'dropout': 0.2, 'lr': 0.001}\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_10 (GRU)                (None, 128)               88320     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,449\n",
      "Trainable params: 88,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1201/1201 [==============================] - 28s 22ms/step - loss: 7566.6948 - val_loss: 7524.2202\n",
      "Epoch 2/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7442.1089 - val_loss: 7407.0049\n",
      "Epoch 3/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7331.1079 - val_loss: 7298.9131\n",
      "Epoch 4/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7227.8647 - val_loss: 7200.6328\n",
      "Epoch 5/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 7130.7837 - val_loss: 7100.8467\n",
      "Epoch 6/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 7039.6094 - val_loss: 7012.6094\n",
      "Epoch 7/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6953.6230 - val_loss: 6924.1641\n",
      "Epoch 8/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6872.4917 - val_loss: 6841.7222\n",
      "Epoch 9/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6794.9351 - val_loss: 6765.2100\n",
      "Epoch 10/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6721.1484 - val_loss: 6690.6099\n",
      "Epoch 11/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6650.8120 - val_loss: 6618.2852\n",
      "Epoch 12/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6584.2109 - val_loss: 6568.2944\n",
      "Epoch 13/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6531.7026 - val_loss: 6499.2622\n",
      "Epoch 14/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6472.7212 - val_loss: 6436.8472\n",
      "Epoch 15/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6418.4839 - val_loss: 6381.0205\n",
      "Epoch 16/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6361.9082 - val_loss: 6324.8335\n",
      "Epoch 17/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6297.4731 - val_loss: 6265.1738\n",
      "Epoch 18/32\n",
      "1201/1201 [==============================] - 26s 22ms/step - loss: 6243.5181 - val_loss: 6200.3296\n",
      "Epoch 19/32\n",
      "1201/1201 [==============================] - 27s 23ms/step - loss: 6231.4268 - val_loss: 6212.6841\n",
      "Epoch 20/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6206.4106 - val_loss: 6161.9175\n",
      "Epoch 21/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6154.7666 - val_loss: 6110.5771\n",
      "Epoch 22/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6114.6689 - val_loss: 6073.5874\n",
      "Epoch 23/32\n",
      "1201/1201 [==============================] - 27s 22ms/step - loss: 6068.5503 - val_loss: 6022.1113\n",
      "Epoch 24/32\n",
      " 510/1201 [===========>..................] - ETA: 12s - loss: 5615.8179"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning_full_3/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd7ef0-b079-4654-844d-c9c1958e07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96591bc2-d3ce-4f32-a0e5-b2a8323c2b35",
   "metadata": {},
   "source": [
    "### Testing bigger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35663ab1-9789-40d4-b701-5cb51b8daccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 20:51:49.413666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.414323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.454870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.455112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.455330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.455542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.456121: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-23 20:51:49.576521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.576865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.577059: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.577234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.577406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:49.577584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:50.101057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:50.101303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:50.101488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:50.101670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:50.101850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:50.102020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2704 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2022-02-23 20:51:50.102224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-23 20:51:50.102382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 823 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "# HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([1, 2, 3]))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([3, 4, 5]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([256, 512]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-2]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning_big').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03be26e6-7f4c-4bd7-897d-086c77371fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 256, 'dropout': 0.2, 'lr': 0.01}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 30, 256)           274944    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 256)           0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 30, 256)           394752    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 256)               394752    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,064,705\n",
      "Trainable params: 1,064,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-23 20:51:59.934940: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258/1258 [==============================] - 120s 93ms/step - loss: 6328.7192 - val_loss: 5944.5210\n",
      "Epoch 2/32\n",
      "1258/1258 [==============================] - 119s 94ms/step - loss: 5791.5454 - val_loss: 5588.0327\n",
      "Epoch 3/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5415.9507 - val_loss: 5313.6079\n",
      "Epoch 4/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5395.4253 - val_loss: 5279.4102\n",
      "Epoch 5/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5247.3804 - val_loss: 5267.7173\n",
      "Epoch 6/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5093.1504 - val_loss: 6702.6533\n",
      "Epoch 7/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5284.3530 - val_loss: 6202.3418\n",
      "Epoch 8/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5614.5205 - val_loss: 5418.5156\n",
      "Epoch 9/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5133.6953 - val_loss: 5235.1494\n",
      "Epoch 10/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5007.6792 - val_loss: 4908.4419\n",
      "Epoch 11/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5142.8398 - val_loss: 5475.4829\n",
      "Epoch 12/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5387.1196 - val_loss: 5509.9043\n",
      "Epoch 13/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4943.5322 - val_loss: 5051.4731\n",
      "Epoch 14/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5064.0571 - val_loss: 5152.4487\n",
      "Epoch 15/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5123.6729 - val_loss: 6409.2451\n",
      "Epoch 16/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5235.2119 - val_loss: 5167.2104\n",
      "Epoch 17/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4942.9629 - val_loss: 5372.6938\n",
      "Epoch 18/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5194.0791 - val_loss: 5428.8579\n",
      "Epoch 19/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5095.6309 - val_loss: 5179.6719\n",
      "Epoch 20/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5095.2202 - val_loss: 4959.3452\n",
      "Epoch 21/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 5183.3477 - val_loss: 5098.0815\n",
      "Epoch 22/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4852.1201 - val_loss: 5652.8799\n",
      "Epoch 23/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4769.8257 - val_loss: 5047.9307\n",
      "Epoch 24/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4778.8857 - val_loss: 5421.7905\n",
      "Epoch 25/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4693.4912 - val_loss: 4923.2124\n",
      "Epoch 26/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4790.1167 - val_loss: 5043.7178\n",
      "Epoch 27/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4582.6592 - val_loss: 4662.4180\n",
      "Epoch 28/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4791.4463 - val_loss: 5506.7173\n",
      "Epoch 29/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4568.3047 - val_loss: 5090.5166\n",
      "Epoch 30/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4646.2837 - val_loss: 4798.8081\n",
      "Epoch 31/32\n",
      "1258/1258 [==============================] - 120s 95ms/step - loss: 4901.3091 - val_loss: 5120.5420\n",
      "Epoch 32/32\n",
      "1258/1258 [==============================] - 119s 95ms/step - loss: 5050.8022 - val_loss: 5253.1147\n",
      "270/270 [==============================] - 9s 32ms/step - loss: 6392.5742\n",
      "--- Starting trial: run-1\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 3, 'n_unit': 512, 'dropout': 0.2, 'lr': 0.01}\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 30, 512)           943104    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 30, 512)           0         \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 30, 512)           1575936   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 30, 512)           0         \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 512)               1575936   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,095,489\n",
      "Trainable params: 4,095,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1258/1258 [==============================] - 430s 340ms/step - loss: 6053.0684 - val_loss: 5497.1372\n",
      "Epoch 2/32\n",
      "1258/1258 [==============================] - 427s 339ms/step - loss: 5643.7549 - val_loss: 5420.0918\n",
      "Epoch 3/32\n",
      "1258/1258 [==============================] - 427s 339ms/step - loss: 5485.2300 - val_loss: 5139.5845\n",
      "Epoch 4/32\n",
      "1258/1258 [==============================] - 430s 341ms/step - loss: 5413.9580 - val_loss: 5544.5127\n",
      "Epoch 5/32\n",
      "1258/1258 [==============================] - 427s 339ms/step - loss: 5436.0151 - val_loss: 5547.8003\n",
      "Epoch 6/32\n",
      "1258/1258 [==============================] - 427s 339ms/step - loss: 5419.8311 - val_loss: 5226.7119\n",
      "Epoch 7/32\n",
      "1258/1258 [==============================] - 427s 339ms/step - loss: 5189.4658 - val_loss: 4960.1953\n",
      "Epoch 8/32\n",
      "1258/1258 [==============================] - 426s 338ms/step - loss: 5182.3730 - val_loss: 5054.2749\n",
      "Epoch 9/32\n",
      "1258/1258 [==============================] - 425s 338ms/step - loss: 5069.4160 - val_loss: 4914.6802\n",
      "Epoch 10/32\n",
      "1258/1258 [==============================] - 427s 339ms/step - loss: 5309.4487 - val_loss: 5187.2407\n",
      "Epoch 11/32\n",
      "1258/1258 [==============================] - 426s 338ms/step - loss: 5161.5371 - val_loss: 5859.1812\n",
      "Epoch 12/32\n",
      "1258/1258 [==============================] - 425s 338ms/step - loss: 5132.2358 - val_loss: 5418.9688\n",
      "Epoch 13/32\n",
      "1258/1258 [==============================] - 426s 338ms/step - loss: 5097.8672 - val_loss: 5120.4160\n",
      "Epoch 14/32\n",
      "1258/1258 [==============================] - 425s 338ms/step - loss: 5040.3262 - val_loss: 5182.7275\n",
      "Epoch 15/32\n",
      "1258/1258 [==============================] - 426s 338ms/step - loss: 4878.0981 - val_loss: 5321.6533\n",
      "Epoch 16/32\n",
      "1258/1258 [==============================] - 426s 338ms/step - loss: 4910.5732 - val_loss: 4842.8042\n",
      "Epoch 17/32\n",
      "1258/1258 [==============================] - 425s 337ms/step - loss: 5150.8364 - val_loss: 5406.9492\n",
      "Epoch 18/32\n",
      "1258/1258 [==============================] - 424s 337ms/step - loss: 4808.3838 - val_loss: 4796.5923\n",
      "Epoch 19/32\n",
      "1258/1258 [==============================] - 425s 337ms/step - loss: 4879.3306 - val_loss: 4955.9624\n",
      "Epoch 20/32\n",
      "1258/1258 [==============================] - 424s 336ms/step - loss: 4681.1914 - val_loss: 5360.8135\n",
      "Epoch 21/32\n",
      "1258/1258 [==============================] - 425s 338ms/step - loss: 4908.7271 - val_loss: 5997.6357\n",
      "Epoch 22/32\n",
      "1258/1258 [==============================] - 425s 337ms/step - loss: 4827.4634 - val_loss: 4627.7988\n",
      "Epoch 23/32\n",
      "1258/1258 [==============================] - 424s 337ms/step - loss: 4720.5142 - val_loss: 4700.5269\n",
      "Epoch 24/32\n",
      "1258/1258 [==============================] - 425s 338ms/step - loss: 4793.3730 - val_loss: 4996.0752\n",
      "Epoch 25/32\n",
      "1258/1258 [==============================] - 424s 337ms/step - loss: 4839.7310 - val_loss: 5514.5991\n",
      "Epoch 26/32\n",
      "1258/1258 [==============================] - 427s 339ms/step - loss: 4847.4561 - val_loss: 4744.1509\n",
      "Epoch 27/32\n",
      "1258/1258 [==============================] - 424s 336ms/step - loss: 4939.0332 - val_loss: 4641.2559\n",
      "Epoch 28/32\n",
      "1258/1258 [==============================] - 424s 337ms/step - loss: 4692.9897 - val_loss: 5529.2036\n",
      "Epoch 29/32\n",
      "1258/1258 [==============================] - 424s 337ms/step - loss: 4767.0420 - val_loss: 5320.2256\n",
      "Epoch 30/32\n",
      "1258/1258 [==============================] - 424s 337ms/step - loss: 4706.8169 - val_loss: 5053.1748\n",
      "Epoch 31/32\n",
      "1258/1258 [==============================] - 424s 336ms/step - loss: 4635.4907 - val_loss: 5399.4805\n",
      "Epoch 32/32\n",
      "1258/1258 [==============================] - 425s 337ms/step - loss: 4639.4272 - val_loss: 5381.4858\n",
      "270/270 [==============================] - 28s 105ms/step - loss: 6551.2773\n",
      "--- Starting trial: run-2\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 4, 'n_unit': 256, 'dropout': 0.2, 'lr': 0.01}\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 30, 256)           274944    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 30, 256)           394752    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 30, 256)           394752    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " gru_9 (GRU)                 (None, 256)               394752    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,459,457\n",
      "Trainable params: 1,459,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "1258/1258 [==============================] - 165s 129ms/step - loss: 6327.7749 - val_loss: 5834.6133\n",
      "Epoch 2/32\n",
      "1258/1258 [==============================] - 162s 128ms/step - loss: 5780.8525 - val_loss: 5592.5322\n",
      "Epoch 3/32\n",
      "1258/1258 [==============================] - 162s 128ms/step - loss: 5639.8198 - val_loss: 5772.7153\n",
      "Epoch 4/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5613.7002 - val_loss: 5689.5137\n",
      "Epoch 5/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5541.3774 - val_loss: 5484.3755\n",
      "Epoch 6/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5753.2446 - val_loss: 6229.5171\n",
      "Epoch 7/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5709.0513 - val_loss: 5812.9170\n",
      "Epoch 8/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5594.0552 - val_loss: 5820.7095\n",
      "Epoch 9/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5959.8555 - val_loss: 5851.2529\n",
      "Epoch 10/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5633.8560 - val_loss: 6293.1357\n",
      "Epoch 11/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5665.8403 - val_loss: 5572.2129\n",
      "Epoch 12/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5665.5332 - val_loss: 5770.0889\n",
      "Epoch 13/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5516.0781 - val_loss: 5692.8169\n",
      "Epoch 14/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5455.9253 - val_loss: 5922.7983\n",
      "Epoch 15/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5468.9297 - val_loss: 6094.3706\n",
      "Epoch 16/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5455.2876 - val_loss: 5836.6870\n",
      "Epoch 17/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5429.2725 - val_loss: 5672.0220\n",
      "Epoch 18/32\n",
      "1258/1258 [==============================] - 161s 128ms/step - loss: 5631.1309 - val_loss: 5763.3477\n",
      "Epoch 19/32\n",
      "1258/1258 [==============================] - 161s 128ms/step - loss: 5788.1904 - val_loss: 5739.5103\n",
      "Epoch 20/32\n",
      "1258/1258 [==============================] - 161s 128ms/step - loss: 5503.6221 - val_loss: 5953.2026\n",
      "Epoch 21/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5524.1704 - val_loss: 5533.8198\n",
      "Epoch 22/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5500.0962 - val_loss: 5344.6445\n",
      "Epoch 23/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5644.1743 - val_loss: 6346.0752\n",
      "Epoch 24/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5369.1943 - val_loss: 5680.7544\n",
      "Epoch 25/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5767.6689 - val_loss: 5609.8564\n",
      "Epoch 26/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5665.4375 - val_loss: 6545.1348\n",
      "Epoch 27/32\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 5760.3813 - val_loss: 5466.8320\n",
      "Epoch 28/32\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 5661.9951 - val_loss: 6513.0186\n",
      "Epoch 29/32\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 5865.8657 - val_loss: 6961.7158\n",
      "Epoch 30/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5680.1436 - val_loss: 6666.1504\n",
      "Epoch 31/32\n",
      "1258/1258 [==============================] - 161s 127ms/step - loss: 5544.4702 - val_loss: 5606.7876\n",
      "Epoch 32/32\n",
      "1258/1258 [==============================] - 160s 127ms/step - loss: 5556.2437 - val_loss: 5500.5771\n",
      "270/270 [==============================] - 11s 41ms/step - loss: 6632.6177\n",
      "--- Starting trial: run-3\n",
      "{'layer_type': 'keras.layers.GRU', 'n_recurrent': 4, 'n_unit': 512, 'dropout': 0.2, 'lr': 0.01}\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " gru_10 (GRU)                (None, 30, 512)           943104    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 30, 512)           0         \n",
      "                                                                 \n",
      " gru_11 (GRU)                (None, 30, 512)           1575936   \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 30, 512)           0         \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 30, 512)           1575936   \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 30, 512)           0         \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 512)               1575936   \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,671,425\n",
      "Trainable params: 5,671,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/32\n",
      "    620/Unknown - 271s 431ms/step - loss: 6241.2783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-24 03:14:21.922130: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 194.73MiB (rounded to 204191744)requested by op gradients/CudnnRNN_grad/CudnnRNNBackprop\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-02-24 03:14:21.922215: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2022-02-24 03:14:21.922247: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 60, Chunks in use: 60. 15.0KiB allocated for chunks. 15.0KiB in use in bin. 273B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922272: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922294: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922314: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 3, Chunks in use: 3. 7.5KiB allocated for chunks. 7.5KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922358: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 1, Chunks in use: 0. 5.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922401: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 12, Chunks in use: 12. 144.0KiB allocated for chunks. 144.0KiB in use in bin. 144.0KiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922436: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922465: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922492: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922522: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922554: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 1, Chunks in use: 0. 441.8KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922588: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 3, Chunks in use: 3. 2.15MiB allocated for chunks. 2.15MiB in use in bin. 1.76MiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922626: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 17, Chunks in use: 13. 29.26MiB allocated for chunks. 25.26MiB in use in bin. 25.26MiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922664: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 25, Chunks in use: 25. 75.46MiB allocated for chunks. 75.46MiB in use in bin. 71.37MiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922703: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 6, Chunks in use: 5. 37.01MiB allocated for chunks. 29.31MiB in use in bin. 27.05MiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922742: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 11.39MiB allocated for chunks. 11.39MiB in use in bin. 11.39MiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922774: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922813: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 8, Chunks in use: 7. 446.87MiB allocated for chunks. 408.11MiB in use in bin. 408.11MiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922852: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 6, Chunks in use: 6. 566.48MiB allocated for chunks. 566.48MiB in use in bin. 349.80MiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922896: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 136.43MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922949: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 4, Chunks in use: 4. 1.37GiB allocated for chunks. 1.37GiB in use in bin. 1.37GiB client-requested in use in bin.\n",
      "2022-02-24 03:14:21.922992: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 194.73MiB was 128.00MiB, Chunk State: \n",
      "2022-02-24 03:14:21.923058: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 136.43MiB | Requested Size: 11.77MiB | in_use: 0 | bin_num: 19, prev:   Size: 58.30MiB | Requested Size: 58.30MiB | in_use: 1 | bin_num: -1, next:   Size: 349.80MiB | Requested Size: 349.80MiB | in_use: 1 | bin_num: -1\n",
      "2022-02-24 03:14:21.923090: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2836267008\n",
      "2022-02-24 03:14:21.930435: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000000 of size 1280 next 1\n",
      "2022-02-24 03:14:21.930496: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000500 of size 256 next 2\n",
      "2022-02-24 03:14:21.930516: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000600 of size 256 next 3\n",
      "2022-02-24 03:14:21.930530: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000700 of size 256 next 4\n",
      "2022-02-24 03:14:21.930543: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000800 of size 256 next 5\n",
      "2022-02-24 03:14:21.930557: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000900 of size 256 next 6\n",
      "2022-02-24 03:14:21.930569: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000a00 of size 256 next 7\n",
      "2022-02-24 03:14:21.930583: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000b00 of size 256 next 10\n",
      "2022-02-24 03:14:21.930596: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000c00 of size 256 next 113\n",
      "2022-02-24 03:14:21.930609: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000d00 of size 256 next 161\n",
      "2022-02-24 03:14:21.930622: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000e00 of size 256 next 150\n",
      "2022-02-24 03:14:21.930645: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa000f00 of size 2048 next 20\n",
      "2022-02-24 03:14:21.930669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa001700 of size 3328 next 13\n",
      "2022-02-24 03:14:21.930694: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002400 of size 256 next 12\n",
      "2022-02-24 03:14:21.930718: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002500 of size 256 next 14\n",
      "2022-02-24 03:14:21.930744: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002600 of size 256 next 78\n",
      "2022-02-24 03:14:21.930769: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002700 of size 256 next 52\n",
      "2022-02-24 03:14:21.930796: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002800 of size 256 next 61\n",
      "2022-02-24 03:14:21.930821: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002900 of size 256 next 17\n",
      "2022-02-24 03:14:21.930847: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002a00 of size 256 next 152\n",
      "2022-02-24 03:14:21.930873: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002b00 of size 256 next 154\n",
      "2022-02-24 03:14:21.930899: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002c00 of size 256 next 37\n",
      "2022-02-24 03:14:21.930925: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002d00 of size 256 next 129\n",
      "2022-02-24 03:14:21.930953: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa002e00 of size 2304 next 141\n",
      "2022-02-24 03:14:21.930978: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003700 of size 256 next 88\n",
      "2022-02-24 03:14:21.931004: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003800 of size 256 next 128\n",
      "2022-02-24 03:14:21.931027: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003900 of size 256 next 110\n",
      "2022-02-24 03:14:21.931042: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003a00 of size 256 next 46\n",
      "2022-02-24 03:14:21.931055: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003b00 of size 256 next 148\n",
      "2022-02-24 03:14:21.931068: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003c00 of size 256 next 151\n",
      "2022-02-24 03:14:21.931081: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003d00 of size 256 next 75\n",
      "2022-02-24 03:14:21.931094: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003e00 of size 256 next 77\n",
      "2022-02-24 03:14:21.931107: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa003f00 of size 256 next 137\n",
      "2022-02-24 03:14:21.931121: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa004000 of size 256 next 56\n",
      "2022-02-24 03:14:21.931134: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4baa004100 of size 5376 next 22\n",
      "2022-02-24 03:14:21.931156: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005600 of size 256 next 23\n",
      "2022-02-24 03:14:21.931180: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005700 of size 256 next 21\n",
      "2022-02-24 03:14:21.931203: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005800 of size 256 next 24\n",
      "2022-02-24 03:14:21.931228: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005900 of size 256 next 25\n",
      "2022-02-24 03:14:21.931254: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005a00 of size 256 next 126\n",
      "2022-02-24 03:14:21.931281: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005b00 of size 256 next 29\n",
      "2022-02-24 03:14:21.931306: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005c00 of size 256 next 30\n",
      "2022-02-24 03:14:21.931332: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005d00 of size 256 next 31\n",
      "2022-02-24 03:14:21.931359: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005e00 of size 256 next 32\n",
      "2022-02-24 03:14:21.931383: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa005f00 of size 256 next 33\n",
      "2022-02-24 03:14:21.931408: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006000 of size 256 next 26\n",
      "2022-02-24 03:14:21.931435: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006100 of size 256 next 87\n",
      "2022-02-24 03:14:21.931460: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006200 of size 256 next 135\n",
      "2022-02-24 03:14:21.931486: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006300 of size 256 next 36\n",
      "2022-02-24 03:14:21.931512: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006400 of size 256 next 27\n",
      "2022-02-24 03:14:21.931527: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006500 of size 256 next 34\n",
      "2022-02-24 03:14:21.931540: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006600 of size 256 next 35\n",
      "2022-02-24 03:14:21.931555: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa006700 of size 614400 next 58\n",
      "2022-02-24 03:14:21.931568: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa09c700 of size 614400 next 11\n",
      "2022-02-24 03:14:21.931582: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa132700 of size 3145728 next 138\n",
      "2022-02-24 03:14:21.931604: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa432700 of size 3145728 next 155\n",
      "2022-02-24 03:14:21.931630: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baa732700 of size 5522432 next 80\n",
      "2022-02-24 03:14:21.931655: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baac76b00 of size 256 next 104\n",
      "2022-02-24 03:14:21.931681: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baac76c00 of size 256 next 94\n",
      "2022-02-24 03:14:21.931708: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baac76d00 of size 256 next 93\n",
      "2022-02-24 03:14:21.931734: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baac76e00 of size 1030656 next 98\n",
      "2022-02-24 03:14:21.931761: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad72800 of size 256 next 125\n",
      "2022-02-24 03:14:21.931786: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad72900 of size 256 next 111\n",
      "2022-02-24 03:14:21.931816: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad72a00 of size 12288 next 99\n",
      "2022-02-24 03:14:21.931842: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad75a00 of size 12288 next 96\n",
      "2022-02-24 03:14:21.931869: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad78a00 of size 256 next 127\n",
      "2022-02-24 03:14:21.931893: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad78b00 of size 256 next 109\n",
      "2022-02-24 03:14:21.931912: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad78c00 of size 12288 next 49\n",
      "2022-02-24 03:14:21.931927: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad7bc00 of size 12288 next 156\n",
      "2022-02-24 03:14:21.931950: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad7ec00 of size 12288 next 45\n",
      "2022-02-24 03:14:21.931974: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad81c00 of size 12288 next 139\n",
      "2022-02-24 03:14:21.931999: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad84c00 of size 12288 next 89\n",
      "2022-02-24 03:14:21.932024: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad87c00 of size 12288 next 18\n",
      "2022-02-24 03:14:21.932051: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad8ac00 of size 12288 next 51\n",
      "2022-02-24 03:14:21.932076: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad8dc00 of size 12288 next 42\n",
      "2022-02-24 03:14:21.932102: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad90c00 of size 12288 next 57\n",
      "2022-02-24 03:14:21.932128: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad93c00 of size 12288 next 117\n",
      "2022-02-24 03:14:21.932154: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad96c00 of size 256 next 68\n",
      "2022-02-24 03:14:21.932180: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad96d00 of size 256 next 134\n",
      "2022-02-24 03:14:21.932206: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad96e00 of size 256 next 112\n",
      "2022-02-24 03:14:21.932227: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad96f00 of size 256 next 103\n",
      "2022-02-24 03:14:21.932241: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad97000 of size 256 next 121\n",
      "2022-02-24 03:14:21.932254: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baad97100 of size 256 next 147\n",
      "2022-02-24 03:14:21.932267: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4baad97200 of size 452352 next 69\n",
      "2022-02-24 03:14:21.932289: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baae05900 of size 3145728 next 84\n",
      "2022-02-24 03:14:21.932315: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bab105900 of size 3145728 next 19\n",
      "2022-02-24 03:14:21.932341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bab405900 of size 3145728 next 114\n",
      "2022-02-24 03:14:21.932365: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bab705900 of size 3145728 next 91\n",
      "2022-02-24 03:14:21.932390: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baba05900 of size 3145728 next 144\n",
      "2022-02-24 03:14:21.932415: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4babd05900 of size 3145728 next 100\n",
      "2022-02-24 03:14:21.932441: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bac005900 of size 3145728 next 8\n",
      "2022-02-24 03:14:21.932467: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bac305900 of size 3145728 next 95\n",
      "2022-02-24 03:14:21.932493: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bac605900 of size 3145728 next 149\n",
      "2022-02-24 03:14:21.932519: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bac905900 of size 3145728 next 28\n",
      "2022-02-24 03:14:21.932546: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bacc05900 of size 3145728 next 39\n",
      "2022-02-24 03:14:21.932571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bacf05900 of size 3145728 next 105\n",
      "2022-02-24 03:14:21.932597: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bad205900 of size 3145728 next 108\n",
      "2022-02-24 03:14:21.932618: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bad505900 of size 3145728 next 9\n",
      "2022-02-24 03:14:21.932633: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bad805900 of size 3145728 next 136\n",
      "2022-02-24 03:14:21.932646: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4badb05900 of size 3145728 next 101\n",
      "2022-02-24 03:14:21.932663: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bade05900 of size 3145728 next 83\n",
      "2022-02-24 03:14:21.932685: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bae105900 of size 3145728 next 50\n",
      "2022-02-24 03:14:21.932712: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bae405900 of size 2037760 next 115\n",
      "2022-02-24 03:14:21.932737: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bae5f7100 of size 2037760 next 43\n",
      "2022-02-24 03:14:21.932765: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bae7e8900 of size 2037760 next 146\n",
      "2022-02-24 03:14:21.932790: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bae9da100 of size 2037760 next 60\n",
      "2022-02-24 03:14:21.932818: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baebcb900 of size 3789056 next 65\n",
      "2022-02-24 03:14:21.932844: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baef68a00 of size 2037760 next 130\n",
      "2022-02-24 03:14:21.932871: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baf15a200 of size 3350528 next 47\n",
      "2022-02-24 03:14:21.932898: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4baf48c200 of size 11940096 next 92\n",
      "2022-02-24 03:14:21.932925: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4baffef300 of size 1048576 next 59\n",
      "2022-02-24 03:14:21.932947: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bb00ef300 of size 3772416 next 71\n",
      "2022-02-24 03:14:21.932961: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bb0488300 of size 61132800 next 133\n",
      "2022-02-24 03:14:21.932980: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bb3ed5300 of size 2037760 next 102\n",
      "2022-02-24 03:14:21.933004: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bb40c6b00 of size 2037760 next 116\n",
      "2022-02-24 03:14:21.933057: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bb42b8300 of size 6303744 next 143\n",
      "2022-02-24 03:14:21.933087: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4bb48bb300 of size 8072192 next 131\n",
      "2022-02-24 03:14:21.933114: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bb506df00 of size 2037760 next 41\n",
      "2022-02-24 03:14:21.933141: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4bb525f700 of size 40643584 next 15\n",
      "2022-02-24 03:14:21.933168: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bb7922300 of size 61132800 next 140\n",
      "2022-02-24 03:14:21.933199: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bbb36f300 of size 79394816 next 122\n",
      "2022-02-24 03:14:21.933227: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bbff26b00 of size 366797056 next 70\n",
      "2022-02-24 03:14:21.933254: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4bd5cf4c00 of size 1048576 next 106\n",
      "2022-02-24 03:14:21.933272: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bd5df4c00 of size 6303744 next 55\n",
      "2022-02-24 03:14:21.933286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bd63f7c00 of size 61132800 next 40\n",
      "2022-02-24 03:14:21.933299: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bd9e44c00 of size 2037760 next 38\n",
      "2022-02-24 03:14:21.933313: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bda036400 of size 2037760 next 64\n",
      "2022-02-24 03:14:21.933327: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bda227c00 of size 2156544 next 157\n",
      "2022-02-24 03:14:21.933341: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bda436400 of size 116033536 next 76\n",
      "2022-02-24 03:14:21.933358: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4be12dec00 of size 2037760 next 86\n",
      "2022-02-24 03:14:21.933372: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4be14d0400 of size 79888384 next 107\n",
      "2022-02-24 03:14:21.933386: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4be6100400 of size 366797056 next 79\n",
      "2022-02-24 03:14:21.933409: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4bfbece500 of size 1048576 next 44\n",
      "2022-02-24 03:14:21.933435: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bfbfce500 of size 6303744 next 81\n",
      "2022-02-24 03:14:21.933461: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4bfc5d1500 of size 61132800 next 82\n",
      "2022-02-24 03:14:21.933487: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c0001e500 of size 2037760 next 90\n",
      "2022-02-24 03:14:21.933515: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c0020fd00 of size 3145728 next 74\n",
      "2022-02-24 03:14:21.933543: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c0050fd00 of size 117082112 next 159\n",
      "2022-02-24 03:14:21.933571: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c074b8500 of size 2037760 next 66\n",
      "2022-02-24 03:14:21.933598: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c076a9d00 of size 79888384 next 54\n",
      "2022-02-24 03:14:21.933626: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c0c2d9d00 of size 366797056 next 145\n",
      "2022-02-24 03:14:21.933653: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4c220a7e00 of size 1048576 next 62\n",
      "2022-02-24 03:14:21.933676: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c221a7e00 of size 6303744 next 85\n",
      "2022-02-24 03:14:21.933692: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c227aae00 of size 61132800 next 16\n",
      "2022-02-24 03:14:21.933705: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c261f7e00 of size 61132800 next 53\n",
      "2022-02-24 03:14:21.933718: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7f4c29c44e00 of size 143058944 next 120\n",
      "2022-02-24 03:14:21.933732: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c324b3600 of size 366797056 next 97\n",
      "2022-02-24 03:14:21.933745: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c48281700 of size 61132800 next 73\n",
      "2022-02-24 03:14:21.933760: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 7f4c4bcce700 of size 121706752 next 18446744073709551615\n",
      "2022-02-24 03:14:21.933773: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2022-02-24 03:14:21.933799: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 60 Chunks of size 256 totalling 15.0KiB\n",
      "2022-02-24 03:14:21.933822: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-02-24 03:14:21.933852: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2048 totalling 2.0KiB\n",
      "2022-02-24 03:14:21.933881: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2304 totalling 2.2KiB\n",
      "2022-02-24 03:14:21.933911: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3328 totalling 3.2KiB\n",
      "2022-02-24 03:14:21.933946: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 12 Chunks of size 12288 totalling 144.0KiB\n",
      "2022-02-24 03:14:21.933978: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 614400 totalling 1.17MiB\n",
      "2022-02-24 03:14:21.934012: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1030656 totalling 1006.5KiB\n",
      "2022-02-24 03:14:21.934047: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 13 Chunks of size 2037760 totalling 25.26MiB\n",
      "2022-02-24 03:14:21.934079: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2156544 totalling 2.06MiB\n",
      "2022-02-24 03:14:21.934110: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 21 Chunks of size 3145728 totalling 63.00MiB\n",
      "2022-02-24 03:14:21.934128: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3350528 totalling 3.20MiB\n",
      "2022-02-24 03:14:21.934144: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3772416 totalling 3.60MiB\n",
      "2022-02-24 03:14:21.934159: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3789056 totalling 3.61MiB\n",
      "2022-02-24 03:14:21.934175: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 5522432 totalling 5.27MiB\n",
      "2022-02-24 03:14:21.934201: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 6303744 totalling 24.05MiB\n",
      "2022-02-24 03:14:21.934233: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 11940096 totalling 11.39MiB\n",
      "2022-02-24 03:14:21.934265: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 7 Chunks of size 61132800 totalling 408.11MiB\n",
      "2022-02-24 03:14:21.934299: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 79394816 totalling 75.72MiB\n",
      "2022-02-24 03:14:21.934333: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 79888384 totalling 152.38MiB\n",
      "2022-02-24 03:14:21.934366: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 116033536 totalling 110.66MiB\n",
      "2022-02-24 03:14:21.934398: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 117082112 totalling 111.66MiB\n",
      "2022-02-24 03:14:21.934433: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 121706752 totalling 116.07MiB\n",
      "2022-02-24 03:14:21.934464: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 366797056 totalling 1.37GiB\n",
      "2022-02-24 03:14:21.934491: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 2.46GiB\n",
      "2022-02-24 03:14:21.934509: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 2836267008 memory_limit_: 2836267008 available bytes: 0 curr_region_allocation_bytes_: 5672534016\n",
      "2022-02-24 03:14:21.934549: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                      2836267008\n",
      "InUse:                      2639840256\n",
      "MaxInUse:                   2795237376\n",
      "NumAllocs:                    26483102\n",
      "MaxAllocSize:                367165696\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-02-24 03:14:21.934619: W tensorflow/core/common_runtime/bfc_allocator.cc:474] *******_***********************x**********************x*********************____******************xx\n",
      "2022-02-24 03:14:21.935058: E tensorflow/stream_executor/dnn.cc:764] OOM when allocating tensor with shape[204191744] and type uint8 on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2022-02-24 03:14:21.936085: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at cudnn_rnn_ops.cc:1968 : INTERNAL: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 512, 512, 1, 30, 995, 0] \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "   Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 512, 512, 1, 30, 995, 0] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall]] [Op:__inference_train_function_930585]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--- Starting trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m({h\u001b[38;5;241m.\u001b[39mname: hparams[h] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hparams})\n\u001b[0;32m---> 17\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs/hparam_tuning_big/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m session_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_dir, hparams)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mcreate_file_writer(run_dir)\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m     34\u001b[0m     hp\u001b[38;5;241m.\u001b[39mhparams(hparams)  \u001b[38;5;66;03m# record the values used in this trial\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mscalar(METRIC_MAE, val_loss, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mtrain_test_model\u001b[0;34m(hparams, shape)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mhparams[HP_LR]),  loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_ds)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m:    Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 512, 512, 1, 30, 995, 0] \n\t [[{{node gradients/CudnnRNN_grad/CudnnRNNBackprop}}]]\n\t [[Adam/gradients/PartitionedCall]] [Op:__inference_train_function_930585]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning_big/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f3c8b-f7f2-443d-994c-12348b93af8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b18c15-146e-4206-9aea-56ea77ae8532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281eeb3-f816-466e-8ee3-6a101f0ed11c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cbdc3-fc87-4f23-8f1b-f51d6aa9805e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf09c66-8a82-4800-b8f0-a9b6cd9774e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ffee6-9cc7-4dae-b81a-40491c65f8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38795671-b9c2-453a-92b0-bec2444c07fd",
   "metadata": {},
   "source": [
    "#### Testing 4, 5, and 6 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c00c2e5-4dfa-4637-aaab-35a7ecbe5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([4, 5, 6]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([32, 64, 128]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([256]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning6').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e722ee-b3d4-46dc-bedf-8a4616789a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,101)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6e570-cc34-4cbd-9071-f4a17e802e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning6/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250522c-de3c-426f-a9ce-a8b4c75363d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2152bffd-bde8-4e80-a2d9-f2dea66d3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM', 'keras.layers.GRU']))\n",
    "HP_LAYER_TYPE=hp.HParam('layer_type', hp.Discrete(['keras.layers.LSTM']))\n",
    "HP_N_RECURRENT=hp.HParam('n_recurrent', hp.Discrete([3]))\n",
    "# HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128, 256, 512]))\n",
    "HP_N_UNIT=hp.HParam('n_unit', hp.Discrete([128, 256, 512]))\n",
    "HP_DROPOUT=hp.HParam('dropout', hp.Discrete([0.20]))\n",
    "HP_LR=hp.HParam('lr', hp.Discrete([1e-3]))\n",
    "METRIC_MAE = 'mae'\n",
    "\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning5').as_default():\n",
    "    hp.hparams_config(\n",
    "    hparams=[HP_LAYER_TYPE, HP_N_RECURRENT, HP_N_UNIT, HP_DROPOUT, HP_LR],\n",
    "    metrics=[hp.Metric(METRIC_MAE, display_name='Mean Avg Error')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b57339-bdb2-44b6-96b3-5ac4f0e3d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=64\n",
    "\n",
    "def train_test_model(hparams, shape=(30,101)):\n",
    "    set_seed()\n",
    "    input = keras.layers.Input(shape=shape)\n",
    "    last = input\n",
    "    for i in range(hparams[HP_N_RECURRENT]):\n",
    "        if i < hparams[HP_N_RECURRENT] - 1:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT], return_sequences=True)(last)\n",
    "        else:\n",
    "            last = eval(hparams[HP_LAYER_TYPE])(hparams[HP_N_UNIT])(last)\n",
    "        \n",
    "        if hparams[HP_DROPOUT]:\n",
    "            last = keras.layers.Dropout(hparams[HP_DROPOUT])(last)\n",
    "\n",
    "    output = keras.layers.Dense(1)(last)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input, outputs=output)\n",
    "    model.compile(optimizer = Adam(learning_rate=hparams[HP_LR]),  loss='mae')\n",
    "\n",
    "    model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=EPOCHS)\n",
    " \n",
    "    val_loss = model.evaluate(test_ds)\n",
    "    return val_loss\n",
    "        \n",
    "\n",
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        val_loss = train_test_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MAE, val_loss, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deaee76-e048-404a-9f86-10038aa964b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_num = 0\n",
    "for layer_type in HP_LAYER_TYPE.domain.values:\n",
    "    for n_recurrent in HP_N_RECURRENT.domain.values:\n",
    "        for n_unit in HP_N_UNIT.domain.values:\n",
    "            for dropout in HP_DROPOUT.domain.values:\n",
    "                for lr in HP_LR.domain.values:\n",
    "                    hparams = {\n",
    "                      HP_LAYER_TYPE: layer_type,\n",
    "                      HP_N_RECURRENT: n_recurrent,\n",
    "                      HP_N_UNIT: n_unit,\n",
    "                      HP_DROPOUT: dropout,\n",
    "                      HP_LR: lr\n",
    "                    }\n",
    "                    run_name = f'run-{session_num}'\n",
    "                    print(f'--- Starting trial: {run_name}')\n",
    "                    print({h.name: hparams[h] for h in hparams})\n",
    "                    run('./logs/hparam_tuning5/' + run_name, hparams)\n",
    "                    session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3619c57-c71e-4046-a136-9c695001ca73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeabaac-292d-4b7e-b16b-9865e57f7ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6563f7-1956-45a4-9d1a-48b1d0123d40",
   "metadata": {},
   "source": [
    "# IGNORE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f591b3b-a116-4223-9a66-b549b75a1030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 07:46:48.420863: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.421242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.487403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.487658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.487866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.488192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.489039: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-26 07:46:48.618142: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.618362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.618543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.618715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.618884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:48.619065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:49.326563: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:49.326805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:49.326984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:49.327164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:49.327335: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:49.330501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2768 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2022-02-26 07:46:49.331266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 07:46:49.331425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 823 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Complex model\n",
    "\n",
    "input = keras.layers.Input(shape=(30,100), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-3')(dropout2)\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(1, name='Output'))(lstm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a747ed-ff54-40f4-85bc-96b35165648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model multi dense\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-3')(dropout2)\n",
    "dense = keras.layers.Dense(128, name='Dense')(lstm3)\n",
    "output = keras.layers.Dense(1, name='Output')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996daba-5af0-40cc-a2b2-faa36f00f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model for quick testing\n",
    "\n",
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(32, return_sequences=True)(input)\n",
    "output = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm1)\n",
    "model = keras.models.Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd42f8d-55a5-4ef6-962f-bf2cdfe25af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Covid-Prediction-30-1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " LSTM-1 (LSTM)               (None, 30, 256)           365568    \n",
      "                                                                 \n",
      " Dropout-1 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " LSTM-2 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " Dropout-2 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " LSTM-3 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 30, 1)            257       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,416,449\n",
      "Trainable params: 1,416,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3280bb06-dae2-4115-9eaa-2c24336ea76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAysAAAAsCAYAAABluid5AAAABmJLR0QA/wD/AP+gvaeTAAAZQklEQVR4nO3deVRTZ/4/8PcNWxZBNoECAlqUuIHb1LVjWzuuuFTFKo60znEBt1HcalErdSlVS9XiGWw7ouNRrLbiiHM6ou3Y0VZb7dhxq0un0w6iICKg7AQ+vz/85X4JhOQmJtyon9c5nOPN8txPnnvfuc9zk1wFIiIwxhhjjDHGmGO5rJC7AsYYY4wxxhgzhicrjDHGGGOMMYfk3HChpKQEZWVlctXyVFIqlfD19ZW7DNTU1ODOnTtyl9EiAgMDoVDIP0/Pz8+HTqeTuwyHFxwcLHcJAIBbt26hvr5e7jLsztvbG2q1Wu4y+Hgkka+vL5RKpdxloKioCJWVlXKXYXdqtRre3t5yl4GqqircvXtX7jIYswtPT0+0atVKXBYa/mYlPj4eWVlZcHd3l6W4p01VVRW0Wi2OHz8udyk4deoUhgwZgsDAQLlLsatff/0Vubm5CAgIkLsUhIWFQRAEODk5yV2Kw/r5559RW1vrEH2k0WjwzDPPyF2GXRUWFmLLli14/fXX5S6Fj0cS5OfnY//+/RgxYoTcpWDcuHE4c+aMQ0x07aW8vBwvvPACMjMz5S4FWVlZiIuLg7+/v9ylMGZTRUVFSE5Oxvz58/U3XXZu/KB3333XIQ5UT4NTp05h9erVcpch6t+/v0NMnOwpLCxM7hIMnD592iEmTo5Ko9HIXYKBa9euOcTEyV7i4+PlLsEAH49MGzdunNwlGPj4448dYuJkL1lZWdi/f7/cZYiio6MdYuLEmC0tXbq0yW3yfxeGMcYYY4wxxozgyQpjjDHGGGPMIfFkhTHGGGOMMeaQeLLCGGOMMcYYc0g8WWGMMcYYY4w5JJ6sMMYYY4wxxhwST1YYY4wxxhhjDoknK4wxxhhjjDGHxJMVxhhjjLGn1Pvvv4/Ro0fLXYbI3vU0bj8+Ph6CIDjEfxqdn58PQRAgCAL27dvX5P7MzEx88sknMlRmvfr6esyYMQN37961ug2LJyunTp0SO1IQBAwcONDqlVtj7dq16Nu3r0PV9LSQ2s86nQ7Jycno2LEjVCoVAgMDMW3aNJw5cwYAEB4ebtCOsb/Vq1eL/w4ODkZVVZXRdW3dulV83JP2P1037m9BEODu7o6oqCgkJSU9UvDlYCy7TxJb5QOQlpGnPR8AZ+RxIyUjtsoHH0Ok9dO5c+dARCAiu9VhaU4trcfS3DRuPz09HR999JHk59uqDmMCAgJARPD3929y344dO5CVlYWYmJjH6r1PoVBg2rRpGD58OKqrq61rw9InDBw4EEQEHx8fbNy4EadOnbJqxbbkiDU9iaT2c3JyMj788ENkZGSguLgY58+fR1BQEPr164eysjI4OzuLbxa1tbUAgN27d4u3TZkyBdHR0WJg8/LykJ6e3mQ9lZWV2LBhAwDg2LFj2Llzp91euxwa9zcRITc3FykpKcjJyUFkZCR+/PFHuctk/5+t8gFAUkbOnj37VOcD4Iw8bqRkxFb54GOItH4CgMTERGRnZ9utDktzau967N2+PeTn5yMxMRGpqalQKBSP3Xtf//790alTJ6xfv96q59vka2D6MxhKpRJffPEFwsPD4enpiYSEBHH22vBjtl27dqFt27ZQq9WIjY1FWVkZXn/9dQiCIM5Ky8rKxNnioUOHxDZWrlyJb7/9Vrzvp59+klzn1atXMXbsWHh5eaFt27ZYunQpamtrDdYlCAKOHDkC4P/OSug/GqypqUFiYiL8/PygUqnw4osv4sKFC0b7QKvVQqFQYOzYsbboYqvV1taivLy8Rdd5/PhxxMTEYMCAAVAqlfD398fatWvRsWNHi9vy9fXFiBEjkJKS0uR1pKWlYdiwYbYq22ZKSkrs1ranpyeGDx+Of/7zn/Dx8cGrr76K+vp6s/ufft/19/dvsu+ay6aeqTbM5fdRs2tP9txexnA+7Nvf1mRE7nzo1+OIGXmc8wE8fhmpr6/H/fv3W2x9DXPRePn48eNo164dvL29sWHDBty7dw/Dhg1Dq1at8Lvf/c7g7L2p8ZExUnLa0JEjR9CtWzeoVCr06NEDBw8eBNB8bprLu/4xjdvX+/TTTxEWFmbT8am5vikpKcGECROgVqvRvn17oxOp/fv3o1u3bggODra4T83VIGUMb2obSHmNADBhwgTs3LnTuk/xqIFZs2ZRRkYGSeHj40MbN24Ul7Ozs8nZ2ZkWLVpExcXFdOLECRIEgXJycsTHfPTRR6RWq2n27NlUVFRE58+fp7Zt29LMmTOJiGjNmjXUp08fg/VoNBrKysoSl409prmaGhsyZAgdOXKEqqqq6Pr169SrVy9KSUkhIqJ79+6Rm5sbHThwQHx8VVUV9erVi+rr64mIaMGCBdSzZ0+6ceMGlZSUUGJiIgUHB1NlZaXYB05OTjRr1iwqLCykrKwsGjNmTLP1nDx5kgYPHtzs/bZw8+ZNUiqVNHr0aMrOzqbq6upHrsVcP0+cOJG6dOlCt27dMttWbW0tAaDdu3cbvb9Lly7073//mxQKhbitiIgePHhAXbt2pUuXLhEAOnbsmKTaQ0ND6fbt25Iea60ePXpQZGQkbdu2jQoLCx+5lub6OyMjgwDQmTNniMj0/jdv3jwKCwujf/3rX1RUVERz586lNm3aUElJCRGZz6aUNszl11R2m6NWq0mn01n0HEtptVrq0aMHpaenU1FR0SPXYst8EJnOiK3zYckxwFqrVq2igIAAeuONN+jKlSs2qeVRM+II+WjuMaa88sor9Le//U3y462xcOFCCgwMpBUrVtD169dtUoupjNgyH0S2zcjBgwdp0qRJkuqyVnl5Obm6utKQIUPos88+E8cXj1qLqX7Kzs4mNzc3g2VnZ2datmwZlZSU0IEDBwgATZo0iS5fvkx5eXnUsWNHWr58ufgcc+MjIsty2rCe0tJSUqlUlJOTQ9XV1XT16lUKDw8Xj6/N5aa5vDdun+hhxj08PCg+Pp7u3r1r0/Gpub6ZMmUKdejQgS5dukSFhYU0Y8YM0mg0lJmZKbYxatQomjFjRpO2pfaplPGrqTG8uW0gZfvfuHGDAJh83yciWrJkCW3ZsqXhTZds+gN7nU6HBQsWwNPTE4MGDYJWq8XFixcNHlNTU4N3330X3t7e6N69OxITE5GRkdEiZxKOHj2KkSNHws3NDR06dMCUKVPw97//HQDg5eWF6Oho7NixQ3z8oUOHEB0dDUEQUFxcjG3btuHtt99GeHg4WrdujZSUFNy9exeffvqp+Jy6ujokJSXB19cXY8eOFWfdcnJycsLhw4cxefJk+Pj4YOrUqfjqq6/EGbetrVu3DvX19QgLC0N0dDS2bNmC//73v1a3FxkZiSlTpmDDhg3ifrJ582bMnj0bGo3GVmXbTEVFBS5cuIAlS5YgJCQE/fv3x+7du/HgwQObricqKgoAcOXKFfE2Y/tfcXEx/vSnP+HNN99Ejx49xLNk1dXVBl+NMJVNqW08jioqKnD+/HkkJiYiODgYAwcORGZmpt0+kXza80FEyM/Px6ZNm9C7d2+0a9cOKSkpyM3Ntfm6pGQkIyOD82FCXV0dbt26hZSUFHTv3h0dOnTApk2bkJeXZ5f12TofwOOXEUEQkJOTg9deew0+Pj6YMGECcnJyUFdX12I16HQ6zJs3D61bt8aECRPg4eEBrVaLzp07IzAwECNHjsQPP/wAAJLHR80xltOGbt++jcrKStTX18PV1RURERG4ceMGfH19zbZtyZisrKwM77zzDnx8fGw2PjXXN4WFhcjMzMTy5cvRpUsX+Pr6Yu3atU2OP3l5eZJer17DPpW6fUyN4U1tA6nt6+u/deuWxf1o08mKs7OzwUdUXl5eTTayv78/WrVqJS537doVtbW1LfJx97Fjx/Dcc89Bo9FAEAQkJiYafIwZFxeHnJwc8U04IyMDcXFxAB5+hay2tlacvAiCAFdXV1RVVRl8L9DFxQVt27a1+2uRiojg5OQE4GEQy8rKsGfPHowePRo+Pj6YOXMmTp06ZdMf14WHh+PixYs4fPgw2rdvj+3bt+PZZ5/FlClTUFNTY1Wba9asQXl5OTZv3ozS0lIcPnwY06dPt1nN9lBRUYHKykqcPn0as2fPhr+/P1588UX85S9/QUVFxSO3r99mgiCItxnb/65evQqdTodu3bqJt6lUKoSHhxt8TGsqm1LbeJzpt9fXX3+NhIQEtGnTBi+99BIOHDhg9X5rDOfjIZ1Oh4qKCvzyyy9ITk5GREQEOnTogPfffx8FBQU2WYeUjHA+pNFvr59++gkrV65Ehw4d0LVrV2zevBmFhYU2W4898gE8XhnR769lZWWoqKjAZ599hokTJ8LT0xNxcXE2P2Yb4+zsjKCgIHHZ3d1d/Eo8AHh4eIjjO6njo+YYy2lDHTt2xMiRIzFs2DBotVps3LgRxcXFkl6HJWMyf39/eHp6isu2GJ+a65vr16+jvr4enTt3Fp/j5+dnUAfwcNLj4uIieb0N+1Tq9jE1hje1DaS27+rqCgC4d++e5Nch1mbxM0zQD4r1jO14jc/mNw5c42WdTmeT2u7cuYOxY8di3rx5OH78ODw8PJCWlmZw1mv48OHw8vLCrl27MHXqVFRXV6N9+/YGdZ05cwZ9+vRpdj0KhWXzv7t372LWrFlWvCJpysvLm5yNISJxB8zIyMCePXugVqvh4eGBW7duITAw8JHX6+TkhKFDh2Lo0KEAgK+++gpDhw7F888/j/j4eIvbCw0NRUJCAlJTU1FYWIjFixdbFFw9nU6HJUuWQK1WW/xcqYydhdF/t/3EiRP4/vvvER8fD4VCgbNnz2LUqFFWrUd/Vqvhm5yx/c/UQa1hRk1lU2obts4vESEhIaHZg5gtGLs6SWlpKQDgH//4B86dO4dp06ahpqYGZ8+etcnVmhw1HwCwd+9enD592qrnSnHp0qUmt+mv1KQfCC9btkw8Czd16tQmxxappGTkcc4H8PBKRn/9618fuZ3mXLt2rclt+u11+fJlrFixAkuXLoWnpyciIiIwfPjwR86rrfMB2C4jly9ftusxW6fTGd2f9O9Je/fuxcGDB8XJxC+//GIwibAVY5lrLodSx0fNMZbThgRBQHZ2Nj7//HOkpaUhKSkJ7733Hr777juEhISYbNuSMZm59wZr8muub5q7EEvjvvby8hIvkiBFwz7V12lu+5gaw5vaBlK3v/5kg7e3t+TXoWfTyYoUd+7cQUlJiThrvHLlClxcXBAeHg53d3cxkMDDj50aDySseRPUarVIT09HRUUFZsyYAQ8PDwBocpbGxcUFkyZNws6dO1FXV4ff//734n2dOnWCi4sLLly4YFUYm6NSqfDyyy/brL3GioqKTB7I3NzcIAgCevXqhcLCQgQEBFi9Lq1Wi6tXryI6Ohrr169HZGSkeN+gQYMQEhKC/Px8q9tPSkrCjh07cPr0aWzdutWqNhQKBX772982OWthS0ePHm32PrVaDZ1Oh+7du+PGjRvo1KmTVeuorKxEamoqIiMj0bt3b5OP7dSpE5ydnXHp0iVxoF1VVYX//Oc/mDhxovg4U9msq6sz24a5/Fo7gBk8eLDFJwEsob+ghjH67dWzZ0+cO3eu2YOpFI9DPvR1Pv/881Y/35z79+/jm2++MXqf/upFQUFB8Pf3R8eOHa2eqEjNiKPkA7AuI1FRUQb7kq2ZOguq316hoaFQKpXo3LnzI01UtFotwsPD7ZIPwDYZ8fHxsesxu7q6Gnv27Gn2fpVKJZ6Jd3Nzc4hvcjzK+EhqTgVBwIgRIzBixAjk5uaie/fuyM7Oxpw5c2x2MqugoABlZWXiJ6i2GJ+a65uIiAgoFApcuXJFvL+4uBhFRUUGjwsKCpJ8OeLGfVpaWmqT8Wtz2yA2NlZS+/r6rTkh3uKTFY1Gg6SkJKxZswa5ublITU3FtGnT4OHhgcjISFy/fh0nT55Et27dkJqaKn5spNemTRvk5+ejrKwMCxcuRFRUFObOnWt2vREREXB1dUVaWhrefvttFBQUYNeuXU0eN3XqVKSlpSEtLQ03btwQb/fy8sKcOXOwbt06REZGonv37rh48SJiY2Oxa9cu9OvXz+r+iImJseq5Uty8eRPLli0zuK1169aoqalB3759kZCQgDFjxuC7777D6tWrbTYonD9/PrZu3YqIiAjcv38fu3fvxs8//4whQ4ZY3aavry++/PJLeHl5Wf3mpFAoMGrUqEealJmzcuVKg2WlUglBEBAUFITZs2cjNjYW/v7+CAsLM/haiRSlpaX45ptvsGrVKhQVFeHLL780u828vLyQkJCAd955B7/5zW8QEhKC1atXw9XV1eAspalsAjDbhrn8WpNdQRAwYcIEqwesUixevNhgWb+92rdvj+nTpyM2NhZ+fn7QaDQ2+367o+YDAHr27GnX96TGv2NUKBTw8PCAs7MzYmJiMGPGDPTo0QPx8fFWnYGzNCOOkg/Auoz069cPI0aMsLifpGp85tfZ2RkajQYuLi6Ii4vDzJkzERERgXHjxsHHx8cm67RHPgDbZCQgIMCu+dCfVG1Io9Ggvr4evXv3xvTp0zF+/Hjk5ORg//79dn1vlMqa8ZElOT1z5gwWL16MzMxMBAQEoKCgADU1NdBqtQCsHxc25uLigmXLlmHt2rX43//+Z7Pxqbm+mTx5MlJSUtCnTx8EBATgjTfeaHK1spdfftns73+a61NbjF9NbQOp7V+5cgUhISHidrNIw5/bS7n6ysmTJwmA+DdgwAD64IMPDG4rLi6miIgIcfm1114joodXWwgNDaUDBw5QaGgoqVQqmjRpEj148EBsf+7cueTp6Unt2rWjY8eOkUajIQD01ltvERFRYWEh9enTh5RKJfXv35+Kioqa1NT4T3/Vh08++YTCw8PJ3d2dBg8eTAsWLCAApNFoDF6jVqul2NjYJq+9urqaFi1aRH5+fqRUKikyMpL27dtHRNSkDz744AOT/ajvS3tfDSw3N5eUSiWp1WpSqVQ0YMAA2rt3L5WVlVlci9R+vnnzJq1YsYKioqJIo9GQt7c3DRgwgA4dOmTQnv4KI/q/8ePHN7uu06dPN6mncZ//8Y9/NNsfLXE1sIiICHJyciK1Wk1hYWG0fv16+vXXXy2uxVh/q9Vq6tatGy1fvtzgSmPm9r+qqipauHAhtWnThtzc3GjQoEH0ww8/iPdLyaa5NohM59dYds1piauBhYSEkJOTE2k0GgoPD6cNGzbQzZs3La7F1vkgaj4j9spHS1wNbOXKlSQIAnl4eFDr1q1p1qxZ9O2331pVi60y4gj5IDJ+fDOlJa4GNn/+fBIEgdzd3cnT05PmzJlD33//vVW1SMmILfJhbF22yEhLXQ3MxcVFPGY/99xzlJGRQaWlpVbXYqqf3nrrLYP75syZ02Q85+TkJC5v3LiRZs2aJS4HBQURkenxkSU5bVzPjz/+SDqdjtLT0ykqKopUKhW1b9+eNm/eLD7HWG6ay7ux9vWvJzQ0lHbu3EnBwcE2G5+a6xsiouLiYho/fjwplUoKCQmhffv2kb+/PwGgRYsWERHR7du3ydPTUzw2WdKn5mqQMoY3tw3MvUYiori4OFq1apXZ/dXY1cCsvnSxNfRv+I4uPj6ejh49avf1tMRkJS8vj/r160fbt283eeBriVocQUtMVmJiYmjZsmVmL8/XErVI5ajZbInJyujRoykpKYmuXbsmey1ya4nJSmpqKk2ePJm++OILqqurk7UWqRw1Hy0xWVm3bh1NnTqVTpw4YXJ7tUQtcmuJyUpFRQX17t2btm7dSgUFBbLWwhzLn//8Z5o4caLJHDqqr7/+mnr37k1VVVVmH2tsstLiXwNzdKWlpbh48SK2bdsmdyk2ERgY2Oz3w5l97N+/X+4SmAXs+eNk1tTChQvlLoFZ4M0335S7hKeKSqXC2bNn5S6DOaA//OEPUCqVOHDgAF599VW5y5Gsvr4eO3bswOeffw43Nzer2mixyUp8fDy2b98OAOL/W2LPHzlbavHixXjvvffg5+eHDz/80K4/6GXMkTh6NhmTE+eDMeYoYmNj5S7BYgqFAh9//PGjtWGjWsxKT08HEYl/jvZmv2nTJhARCgoKMGbMGLnLYazFOHo2GZMT54MxxuTFHx8wxhhjjDHGHBJPVhhjjDHGGGMOiScrjDHGGGOMMYfEkxXGGGOMMcaYQ+LJCmOMMcYYY8wh8WSFMcYYY4wx5pB4ssIYY4wxxhhzSDxZYYwxxhhjjDkknqwwxhhjjDHGHJJz4xuKi4tx8+ZNOWp56hQWFspdgoHq6uonftvX1dXJXYKB27dvQ6fTyV2GwyIiuUswkJeXB4XiyT3HU15eLncJBvh4ZFpVVZXcJRgoKip6ordXUVGR3CUYqKysfKL7mz2dysrKmtxmMFnx8/PDtm3bsG3bthYr6mk3YMAAuUsAAKhUKty+fRsvvPCC3KXYlZubG5ydm8zRZREWFoaYmBi5y3BogYGBEARB7jIAAM8++yxeeuklucuwu1deeUXuEgDw8UgqjUYjdwkAgGeeeQbJyclITk6WuxS7GjVqlNwlAABatWqFS5cuPfHHbPZ06tu3r8GyQI526pIxxhhjjDHGgMtP7vcZGGOMMcYYY481nqwwxhhjjDHGHJIzgONyF8EYY4wxxhhjjfz6/wD+eEiSVTFQ4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keras.utils.plot_model(model, to_file='multilayer_perceptron_graph.png')\n",
    "keras.utils.plot_model(model, to_file='model_3_256.png', show_shapes=False, show_dtype=False, \n",
    "                       show_layer_activations=False, rankdir='LR', dpi=70, show_layer_names=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23a23bb-19f0-472c-8f52-963f931b9cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, to_file='model_3_256_detailed.png', show_shapes=True, show_dtype=True, \n",
    "                       show_layer_activations=False, rankdir='TB', dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692656d-ff4e-43ce-b6b5-93301c71c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME='selected_td'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_selected_td.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=50, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252ff31-ff7e-4e3e-a832-d24ae83dcf17",
   "metadata": {},
   "source": [
    "### Selected model trained to convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a52820-5929-47a6-b787-9ab60dd994e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Covid-Prediction-30-1-Densex2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, 30, 100)]         0         \n",
      "                                                                 \n",
      " LSTM-1 (LSTM)               (None, 30, 256)           365568    \n",
      "                                                                 \n",
      " Dropout-1 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " LSTM-2 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " Dropout-2 (Dropout)         (None, 30, 256)           0         \n",
      "                                                                 \n",
      " LSTM-3 (LSTM)               (None, 30, 256)           525312    \n",
      "                                                                 \n",
      " Output (Dense)              (None, 30, 1)             257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,416,449\n",
      "Trainable params: 1,416,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 07:50:25.112717: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201/1201 [==============================] - 142s 115ms/step - loss: 6904.7598 - mse: 1051213120.0000 - mae: 6904.7852 - val_loss: 6453.9409 - val_mse: 1105392896.0000 - val_mae: 6453.9409\n",
      "Epoch 2/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 6179.2319 - mse: 1029391616.0000 - mae: 6179.2554 - val_loss: 5955.1670 - val_mse: 1085113216.0000 - val_mae: 5955.1665\n",
      "Epoch 3/1024\n",
      "1201/1201 [==============================] - 143s 119ms/step - loss: 5847.4043 - mse: 1011740480.0000 - mae: 5847.4312 - val_loss: 5748.2661 - val_mse: 1071805120.0000 - val_mae: 5748.2671\n",
      "Epoch 4/1024\n",
      "1201/1201 [==============================] - 143s 119ms/step - loss: 5646.3711 - mse: 1000044032.0000 - mae: 5646.3960 - val_loss: 5466.9531 - val_mse: 1061515072.0000 - val_mae: 5466.9492\n",
      "Epoch 5/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5394.6538 - mse: 986333952.0000 - mae: 5394.6738 - val_loss: 5147.0137 - val_mse: 1044074432.0000 - val_mae: 5147.0142\n",
      "Epoch 6/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5297.9697 - mse: 976247040.0000 - mae: 5297.9873 - val_loss: 5295.1768 - val_mse: 1042308800.0000 - val_mae: 5295.1758\n",
      "Epoch 7/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5590.6421 - mse: 985663424.0000 - mae: 5590.6670 - val_loss: 5306.3604 - val_mse: 1039338496.0000 - val_mae: 5306.3608\n",
      "Epoch 8/1024\n",
      "1201/1201 [==============================] - 143s 119ms/step - loss: 5299.1416 - mse: 971721600.0000 - mae: 5299.1611 - val_loss: 5510.0132 - val_mse: 1050808640.0000 - val_mae: 5510.0156\n",
      "Epoch 9/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5210.6064 - mse: 963843904.0000 - mae: 5210.6255 - val_loss: 4874.4995 - val_mse: 1020125632.0000 - val_mae: 4874.4971\n",
      "Epoch 10/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5027.5566 - mse: 952152320.0000 - mae: 5027.5820 - val_loss: 4948.7598 - val_mse: 1011984320.0000 - val_mae: 4948.7622\n",
      "Epoch 11/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 4985.8208 - mse: 948257856.0000 - mae: 4985.8408 - val_loss: 5128.7749 - val_mse: 1007378048.0000 - val_mae: 5128.7749\n",
      "Epoch 12/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5729.2451 - mse: 986532800.0000 - mae: 5729.2661 - val_loss: 6000.6235 - val_mse: 1038325760.0000 - val_mae: 6000.6245\n",
      "Epoch 13/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5164.6816 - mse: 954298880.0000 - mae: 5164.6997 - val_loss: 4776.3638 - val_mse: 1004117760.0000 - val_mae: 4776.3652\n",
      "Epoch 14/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 4850.2437 - mse: 938788864.0000 - mae: 4850.2656 - val_loss: 5759.8140 - val_mse: 1069888448.0000 - val_mae: 5759.8130\n",
      "Epoch 15/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 4945.5581 - mse: 937145344.0000 - mae: 4945.5835 - val_loss: 4970.1113 - val_mse: 997650496.0000 - val_mae: 4970.1094\n",
      "Epoch 16/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5081.9629 - mse: 940728768.0000 - mae: 5081.9834 - val_loss: 4822.0366 - val_mse: 1011425472.0000 - val_mae: 4822.0376\n",
      "Epoch 17/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 5138.2500 - mse: 948508672.0000 - mae: 5138.2734 - val_loss: 4758.1885 - val_mse: 993327296.0000 - val_mae: 4758.1885\n",
      "Epoch 18/1024\n",
      "1201/1201 [==============================] - 143s 118ms/step - loss: 4814.1660 - mse: 928798976.0000 - mae: 4814.1929 - val_loss: 4587.3120 - val_mse: 983988032.0000 - val_mae: 4587.3130\n",
      "Epoch 19/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5226.0508 - mse: 943581632.0000 - mae: 5226.0718 - val_loss: 4828.1836 - val_mse: 988632192.0000 - val_mae: 4828.1831\n",
      "Epoch 20/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5037.4946 - mse: 936775424.0000 - mae: 5037.5190 - val_loss: 4761.0381 - val_mse: 983442944.0000 - val_mae: 4761.0371\n",
      "Epoch 21/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4830.3970 - mse: 925222464.0000 - mae: 4830.4170 - val_loss: 4996.7593 - val_mse: 980775296.0000 - val_mae: 4996.7588\n",
      "Epoch 22/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5223.2715 - mse: 946583616.0000 - mae: 5223.2915 - val_loss: 4737.3955 - val_mse: 979316928.0000 - val_mae: 4737.3965\n",
      "Epoch 23/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4763.1016 - mse: 917156224.0000 - mae: 4763.1235 - val_loss: 4589.9370 - val_mse: 970753152.0000 - val_mae: 4589.9360\n",
      "Epoch 24/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5289.1411 - mse: 947125184.0000 - mae: 5289.1641 - val_loss: 4383.1265 - val_mse: 968440576.0000 - val_mae: 4383.1255\n",
      "Epoch 25/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4560.0342 - mse: 904305152.0000 - mae: 4560.0537 - val_loss: 4156.5288 - val_mse: 959064128.0000 - val_mae: 4156.5273\n",
      "Epoch 26/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4694.0015 - mse: 911044608.0000 - mae: 4694.0220 - val_loss: 4604.9785 - val_mse: 959332992.0000 - val_mae: 4604.9775\n",
      "Epoch 27/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4432.5938 - mse: 895670720.0000 - mae: 4432.6128 - val_loss: 5242.4302 - val_mse: 997705344.0000 - val_mae: 5242.4307\n",
      "Epoch 28/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4625.6514 - mse: 901955840.0000 - mae: 4625.6826 - val_loss: 4337.4951 - val_mse: 952373952.0000 - val_mae: 4337.4951\n",
      "Epoch 29/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4387.8896 - mse: 888022528.0000 - mae: 4387.9102 - val_loss: 4100.7817 - val_mse: 943380928.0000 - val_mae: 4100.7827\n",
      "Epoch 30/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4564.3838 - mse: 894688832.0000 - mae: 4564.4043 - val_loss: 4810.4751 - val_mse: 1008457408.0000 - val_mae: 4810.4736\n",
      "Epoch 31/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4565.1182 - mse: 896735168.0000 - mae: 4565.1348 - val_loss: 4508.1191 - val_mse: 946946752.0000 - val_mae: 4508.1177\n",
      "Epoch 32/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4617.0542 - mse: 900160960.0000 - mae: 4617.0728 - val_loss: 4469.6597 - val_mse: 986853056.0000 - val_mae: 4469.6611\n",
      "Epoch 33/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4481.6162 - mse: 889365312.0000 - mae: 4481.6304 - val_loss: 4795.7998 - val_mse: 1005474048.0000 - val_mae: 4795.7988\n",
      "Epoch 34/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4888.3125 - mse: 913497280.0000 - mae: 4888.3276 - val_loss: 5178.6719 - val_mse: 957589312.0000 - val_mae: 5178.6719\n",
      "Epoch 35/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5044.7163 - mse: 931748608.0000 - mae: 5044.7393 - val_loss: 4411.3779 - val_mse: 952674112.0000 - val_mae: 4411.3760\n",
      "Epoch 36/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4755.2725 - mse: 908580480.0000 - mae: 4755.2861 - val_loss: 5421.3022 - val_mse: 1043392704.0000 - val_mae: 5421.3032\n",
      "Epoch 37/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5022.7427 - mse: 924620800.0000 - mae: 5022.7598 - val_loss: 5912.2607 - val_mse: 1079738752.0000 - val_mae: 5912.2588\n",
      "Epoch 38/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4998.9717 - mse: 922923520.0000 - mae: 4998.9976 - val_loss: 4595.5322 - val_mse: 990955328.0000 - val_mae: 4595.5347\n",
      "Epoch 39/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4742.9722 - mse: 899351040.0000 - mae: 4742.9932 - val_loss: 4265.1201 - val_mse: 938026432.0000 - val_mae: 4265.1187\n",
      "Epoch 40/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4620.9380 - mse: 892670144.0000 - mae: 4620.9570 - val_loss: 4230.8052 - val_mse: 938268352.0000 - val_mae: 4230.8052\n",
      "Epoch 41/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4471.4438 - mse: 881096960.0000 - mae: 4471.4624 - val_loss: 4330.6704 - val_mse: 934279104.0000 - val_mae: 4330.6704\n",
      "Epoch 42/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4505.0972 - mse: 896756224.0000 - mae: 4505.1226 - val_loss: 4291.2510 - val_mse: 935234048.0000 - val_mae: 4291.2529\n",
      "Epoch 43/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5267.9492 - mse: 945180608.0000 - mae: 5267.9712 - val_loss: 5873.3447 - val_mse: 1074497536.0000 - val_mae: 5873.3467\n",
      "Epoch 44/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 5089.4272 - mse: 928604224.0000 - mae: 5089.4453 - val_loss: 4292.2979 - val_mse: 949932544.0000 - val_mae: 4292.2983\n",
      "Epoch 45/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4816.0430 - mse: 910829952.0000 - mae: 4816.0586 - val_loss: 4231.8784 - val_mse: 934125760.0000 - val_mae: 4231.8779\n",
      "Epoch 46/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4863.5718 - mse: 915263232.0000 - mae: 4863.5894 - val_loss: 5016.7197 - val_mse: 1013351744.0000 - val_mae: 5016.7207\n",
      "Epoch 47/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4926.9429 - mse: 913372736.0000 - mae: 4926.9604 - val_loss: 4394.4331 - val_mse: 937738368.0000 - val_mae: 4394.4331\n",
      "Epoch 48/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4657.6362 - mse: 899340800.0000 - mae: 4657.6519 - val_loss: 5451.4688 - val_mse: 1052472320.0000 - val_mae: 5451.4692\n",
      "Epoch 49/1024\n",
      "1201/1201 [==============================] - 142s 118ms/step - loss: 4862.0708 - mse: 918955392.0000 - mae: 4862.0957 - val_loss: 5711.2808 - val_mse: 1066962176.0000 - val_mae: 5711.2817\n"
     ]
    }
   ],
   "source": [
    "# Selected model trained to convergence\n",
    "\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "\n",
    "input = keras.layers.Input(shape=(30,100), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-3')(dropout2)\n",
    "output = keras.layers.Dense(1, name='Output')(lstm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')\n",
    "print(model.summary())\n",
    "\n",
    "NAME='selected_model'\n",
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate=0.01),  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_selected.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03cbb93a-133a-4770-8573-bd4d6d30d9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "    973/Unknown - 113s 113ms/step - loss: nan - mse: nan - mae: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(NAME))\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/engine/training.py:1221\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1221\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1223\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:436\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 436\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    314\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 316\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    319\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:354\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    353\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 354\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1032\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1032\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/callbacks.py:1104\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1103\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:554\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/keras/utils/tf_utils.py:550\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    549\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 550\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1149\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1149\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/projects/drexel/dsci592/DS-capstone-pt1/env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1115\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1114\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1116\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Complex model multi dense\n",
    "\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-2')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True, activity_regularizer=tf.keras.regularizers.L2(0.01), name='LSTM-3')(dropout2)\n",
    "dense = keras.layers.Dense(128, name='Dense', activation='relu')(lstm3)\n",
    "output = keras.layers.Dense(1, name='Output')(dense)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-Densex2')\n",
    "\n",
    "\n",
    "NAME='densex2c'\n",
    "model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),  loss='mse',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_densex2c.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mse', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, batch_size=128,\n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebcbd3-8213-4e55-8ecd-90f7ffdad381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex model one dense, batch normalization\n",
    "\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "\n",
    "input = keras.layers.Input(shape=(30,101), name='Input')\n",
    "lstm1 = keras.layers.LSTM(256, return_sequences=True, name='LSTM-1')(input)\n",
    "dropout1 = keras.layers.Dropout(0.20, name='Dropout-1')(lstm1)\n",
    "batchnorm1 = keras.layers.BatchNormalization(name='Batch-Normalization-1')(dropout1)\n",
    "lstm2 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-2')(batchnorm1)\n",
    "dropout2 = keras.layers.Dropout(0.20, name='Dropout-2')(lstm2)\n",
    "batchnorm2 = keras.layers.BatchNormalization(name='Batch-Normalization-2')(dropout2)\n",
    "lstm3 = keras.layers.LSTM(256, return_sequences=True,  name='LSTM-3')(batchnorm2)\n",
    "dropout3 = keras.layers.Dropout(0.20, name='Dropout-3')(lstm3)\n",
    "batchnorm3 = keras.layers.BatchNormalization(name='Batch-Normalization-3')(dropout3)\n",
    "output = keras.layers.Dense(1, name='Output')(batchnorm3)\n",
    "model = keras.models.Model(inputs=input, outputs=output, name='Covid-Prediction-30-1-BatchNorm')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "NAME='batchnorm'\n",
    "model.compile(optimizer = 'adam',  loss='mae',  metrics=['mse', 'mae'])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('./data/model/covid_lstm_batchnorm.h5', save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='mae', patience=20, restore_best_weights=True)\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/{}\".format(NAME))\n",
    "history = model.fit(train_ds, epochs=1024, \n",
    "                    validation_data=val_ds, \n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard]\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d3f08-9917-4700-aa20-875fcedfa275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added this to the data preprocessing, so not required\n",
    "\n",
    "class TimeEncoding(keras.layers.Layer):\n",
    "    \"\"\" Layer to encode cyclical and continuous time.  \n",
    "    Input should an n x 1 array or vector of integers.  \n",
    "    Integers represent number of time units (i.e., days) from the starting point\"\"\"\n",
    "\n",
    "    def __init__(self, cyclical_interval=365, continuous_interval=3650 , **kwargs):\n",
    "        self.cyclical_interval = cyclical_interval\n",
    "        self.continuous_interval = continuous_interval\n",
    "        super(TimeEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        cyclical_sin = np.sin((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        cyclical_cos = np.cos((x * 2 * np.pi)/self.cyclical_interval)\n",
    "        continuous_sin = np.sin((x * 2 * np.pi)/self.continuous_interval)\n",
    "        continuous_cos = np.cos((x * 2 * np.pi)/self.continuous_interval)\n",
    "        \n",
    "        return keras.layers.concatenate([cyclical_sin, cyclical_cos, continuous_sin, continuous_cos], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42a910-0e7c-4bad-b21a-fc9b8a939924",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_encoding = TimeEncoding()\n",
    "x = np.arange(30)/1.0\n",
    "x = x[:, tf.newaxis]\n",
    "time_encoding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fb679-5415-4550-ad4c-e80deaa94f1f",
   "metadata": {},
   "source": [
    "input = keras.layers.Input(shape=(30,92))\n",
    "lstm1 = keras.layers.LSTM(512, return_sequences=True)(input)\n",
    "dropout1 = keras.layers.Dropout(0.20)(lstm1)\n",
    "lstm2 = keras.layers.LSTM(512, return_sequences=True)(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.20)(lstm2)\n",
    "lstm3 = keras.layers.LSTM(512, return_sequences=True)(dropout2)\n",
    "dropout3 = keras.layers.Dropout(0.20)(lstm3)\n",
    "lstm4 = keras.layers.LSTM(512, return_sequences=True)(dropout3)\n",
    "dropout4 = keras.layers.Dropout(0.20)(lstm4)\n",
    "lstm5 = keras.layers.LSTM(512)(dropout4)\n",
    "output = keras.layers.Dense(1)(lstm5)\n",
    "model = keras.models.Model(inputs=input, outputs=output)\n",
    "\n",
    "\n",
    "Output with 5 LSTM @512, one dense layer\n",
    "\n",
    "Epoch 1/32\n",
    "1256/1256 [==============================] - 854s 677ms/step - loss: 1525.3854 - mse: 3823870.7500 - mae: 1525.3854 - val_loss: 1395.9611 - val_mse: 3222036.5000 - val_mae: 1395.9611\n",
    "Epoch 2/32\n",
    "1256/1256 [==============================] - 860s 685ms/step - loss: 1306.3831 - mse: 2720509.2500 - mae: 1306.3831 - val_loss: 1268.0597 - val_mse: 2458543.2500 - val_mae: 1268.0597\n",
    "Epoch 3/32\n",
    "1256/1256 [==============================] - 861s 686ms/step - loss: 1224.3491 - mse: 2193218.7500 - mae: 1224.3491 - val_loss: 1221.3195 - val_mse: 2095028.1250 - val_mae: 1221.3195\n",
    "Epoch 4/32\n",
    "1256/1256 [==============================] - 861s 685ms/step - loss: 1190.3538 - mse: 1940674.8750 - mae: 1190.3538 - val_loss: 1205.6909 - val_mse: 1935832.7500 - val_mae: 1205.6909\n",
    "Epoch 5/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1179.1588 - mse: 1829032.5000 - mae: 1179.1588 - val_loss: 1197.5588 - val_mse: 1859381.2500 - val_mae: 1197.5588\n",
    "Epoch 6/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1173.8285 - mse: 1778276.3750 - mae: 1173.8285 - val_loss: 1195.9708 - val_mse: 1827763.8750 - val_mae: 1195.9708\n",
    "Epoch 7/32\n",
    "1256/1256 [==============================] - 858s 683ms/step - loss: 1170.9568 - mse: 1751288.7500 - mae: 1170.9568 - val_loss: 1193.4899 - val_mse: 1804784.8750 - val_mae: 1193.4899\n",
    "Epoch 8/32\n",
    "1256/1256 [==============================] - 859s 684ms/step - loss: 1168.4807 - mse: 1736100.6250 - mae: 1168.4807 - val_loss: 1190.9672 - val_mse: 1791784.7500 - val_mae: 1190.9672\n",
    "Epoch 9/32\n",
    "1256/1256 [==============================] - 856s 682ms/step - loss: 1166.9685 - mse: 1730238.2500 - mae: 1166.9685 - val_loss: 1191.2921 - val_mse: 1792275.2500 - val_mae: 1191.2921\n",
    "Epoch 10/32\n",
    "1256/1256 [==============================] - 855s 681ms/step - loss: 1166.2148 - mse: 1729396.2500 - mae: 1166.2148 - val_loss: 1189.8693 - val_mse: 1789450.0000 - val_mae: 1189.8693\n",
    "Epoch 11/32\n",
    "1256/1256 [==============================] - 856s 681ms/step - loss: 1165.9210 - mse: 1729189.6250 - mae: 1165.9210 - val_loss: 1190.8143 - val_mse: 1792432.6250 - val_mae: 1190.8143\n",
    "Epoch 12/32\n",
    "1256/1256 [==============================] - 856s 682ms/step - loss: 1165.8933 - mse: 1730290.7500 - mae: 1165.8933 - val_loss: 1190.2284 - val_mse: 1791079.6250 - val_mae: 1190.2284\n",
    "Epoch 13/32\n",
    "1256/1256 [==============================] - 857s 682ms/step - loss: 1165.8029 - mse: 1731005.8750 - mae: 1165.8029 - val_loss: 1189.7284 - val_mse: 1790104.0000 - val_mae: 1189.7284\n",
    "Epoch 14/32\n",
    "1256/1256 [==============================] - 854s 680ms/step - loss: 1165.6666 - mse: 1729771.0000 - mae: 1165.6666 - val_loss: 1190.9232 - val_mse: 1793442.3750 - val_mae: 1190.9232\n",
    "Epoch 15/32\n",
    "1256/1256 [==============================] - 853s 680ms/step - loss: 1166.0372 - mse: 1731329.2500 - mae: 1166.0372 - val_loss: 1190.1742 - val_mse: 1791922.6250 - val_mae: 1190.1742\n",
    "Epoch 16/32\n",
    " 532/1256 [===========>..................] - ETA: 7:34 - loss: 1170.4701 - mse: 1743321.3750 - mae: 1170.4701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2d016b9-bb92-4212-b14f-2ff53eb074a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1688712"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds()\n",
    "train_ds, val_ds, test_ds = dsci.get_train_test_eval_ds(train='./data/train7/x_*.npy', test='./data/test7/x_*.npy', eval_='./data/eval7/x_*.npy', days_to_predict=7)\n",
    "count = 0\n",
    "for ds in [train_ds, val_ds, test_ds]:\n",
    "    for x, _ in ds:\n",
    "        count += x.shape[0]\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c569ce-c43b-4165-b3f4-9c6fceba7818",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
